[
  {
    "objectID": "reading/index.html",
    "href": "reading/index.html",
    "title": "Reading a Technical Paper",
    "section": "",
    "text": "There may never be a point in your academic life where you will read a paper and understand everything in it (or if there is, I haven‚Äôt gotten there). Instead, you have to develop methods for getting whatever information you can out of a paper itself, and then draw up a list of terms and concepts to do further background research on.\nLet‚Äôs work through a sample paragraph from Bender et al. (2021) to see some of these strategies in action."
  },
  {
    "objectID": "reading/index.html#a-tricky-paragraph",
    "href": "reading/index.html#a-tricky-paragraph",
    "title": "Reading a Technical Paper",
    "section": "A Tricky Paragraph",
    "text": "A Tricky Paragraph\nBender et al. (2021) is an important paper about ethics and safety concerns in natural language processing. However, it could be hard to follow some of the discussion without a background in the NLP literature. Here‚Äôs a sample paragraph where they discuss specific advancements made in NLP models.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.¬†82].\n\nMy own approach to trying to understand paragraphs (and whole papers) like this, where I might not be familiar with all of the concepts involved, is ask the following questions:\n\nCan I figure out the upshot? What message is this paragraph trying to communicate?\nCan I pick out any themes? What keeps getting repeated?\nWhat search terms can I pull out of the paper to do more background research."
  },
  {
    "objectID": "reading/index.html#can-we-figure-out-the-upshot",
    "href": "reading/index.html#can-we-figure-out-the-upshot",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out the upshot?",
    "text": "Can we figure out the upshot?\nThis paragraph is trying to tell us something. I‚Äôve color coded the pieces of the paragraph which I think are most useful for figuring out the point of this paragraph even if you don‚Äôt know what all the specifics mean.\n\nThe next big step1 was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art2 performance on question answering, textual entailment, semaantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well3. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary4 for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed5 to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score6 in 10 epochs as opposed to 4867 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data8. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.¬†82].1¬†‚ÄúBig steps‚Äù, improvement2¬†‚Äústate of the art‚Äù, (SOTA) means the best it can be3¬†‚Äúat first ‚Ä¶ and later‚Äù from limited applications, it expanded4¬†less ‚Äúlabeled data‚Äù needed.5¬†Less ‚Äútraining data‚Äù needed.6¬†Whatever the ‚ÄúF1 score‚Äù is, this got the best one7¬†Whatever an ‚Äúepoch‚Äù is, this needed less of them8¬†The same score, but less data.\n\n\nThe upshot\n~Things~ got better with less."
  },
  {
    "objectID": "reading/index.html#can-we-figure-out-themes-from-repetition",
    "href": "reading/index.html#can-we-figure-out-themes-from-repetition",
    "title": "Reading a Technical Paper",
    "section": "Can we figure out themes from repetition?",
    "text": "Can we figure out themes from repetition?\nI‚Äôve highlighted the words whose repetition struck me as being important to the theme of this paragraph.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.¬†82].\n\nIt seems clear ‚Äútraining‚Äù is a big deal, and it involves data."
  },
  {
    "objectID": "reading/index.html#any-search-terms-for-background-research",
    "href": "reading/index.html#any-search-terms-for-background-research",
    "title": "Reading a Technical Paper",
    "section": "Any search terms for background research?",
    "text": "Any search terms for background research?\nThere are a lot of technical terms in this paragraph, but it seems like they can be classified under a few main categories.\n\nThe next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddingsrequired a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g.¬†82].\n\nThere‚Äôs at least three kinds of things we could try doing some background research on here:\n\n‚ÄúModels‚Äù\nThere are a few models named here. Some seem like generic names, and others seem more specific. The most generic ‚Äúword embeddings‚Äù is actually defined in the paragraph\n\nrepresentations of the distribution of words\n\nThere‚Äôs probably a lot more to investigate here.\nAfter defining ‚Äúword embeddings,‚Äù they say ‚Äúthese word vectors‚Äù, which seems to suggest that these are synonymous or at least highly interchangeable concepts.\nSpecific ‚Äúsystems‚Äù that generate ‚Äúword vectors‚Äù are\n\nword2vec\nGloVe\n\nSome good search terms to find information about these would probably be ‚Äúword2vec word vectors‚Äù or ‚ÄúGloVe word vectors‚Äù.\nNext they name another class of model, ‚ÄúLSTM models,‚Äù that generate ‚Äúword vectors‚Äù, and name some specific LSTM modelsYou can also get an idea of naming conventions in this field. The name formats seem to either be thing2thing or an acronym that is a pronounceable word‚Ä¶ and hard to search for all on its own.\n\ncontext2vec\nELMo\n\nSome good search terms here would probably be ‚ÄúLSTM context2vec‚Äù or ‚ÄúLSTM ELMo‚Äù.\n\n\n‚ÄúNLP Tasks‚Äù\nAfter saying that these models are used on ‚ÄúNLP tasks‚Äù, they name a few specific ones. Some of them just have names, while others also have an acronym associated with them.\n\nquestion answering\ntextual entailment\nsemantic role labeling (SRL)\ncoreference resolution\nnamed entity recognition (NER)\nsentiment analysis\n\nSome of these tasks wouldn‚Äôt make for great search terms on their own, like ‚Äúquestion answering,‚Äù but appending ‚ÄúNLP‚Äù to the beginning for ‚ÄúNLP question answering‚Äù would probably work.\n\n\nScores\nIn this paragraph, only one kind of score, the ‚ÄúF1 score‚Äù is mentioned. But in the context of the rest of the paper, there are a number of other ‚ÄúScores‚Äù that could be important to investigate."
  },
  {
    "objectID": "reading/index.html#wrapping-up",
    "href": "reading/index.html#wrapping-up",
    "title": "Reading a Technical Paper",
    "section": "Wrapping up",
    "text": "Wrapping up\nNormally, the process isn‚Äôt as elaborate as it appears in this demo. I don‚Äôt usually color code all of the words in a paragraph, much less a whole paper, like this. But I do often try to mentally summarize paragraphs and sections with what the upshot is. Some authors are better than others in getting across their point in among the technical aspects, but ideally they are always trying to communicate some message that you can at least approximate even if you don‚Äôt understand everything in detail."
  },
  {
    "objectID": "data_sparsity/data_sparsity.html",
    "href": "data_sparsity/data_sparsity.html",
    "title": "Data Sparsity",
    "section": "",
    "text": "Let‚Äôs say we‚Äôre biologists, working in a rain forest, and put out a bug net to survey the biodiversity of the forest. We catch 10 bugs, and each species is a different color:\n[\\(_1\\), \\(_2\\), \\(_3\\), \\(_4\\), \\(_5\\), \\(_6\\), \\(_7\\), \\(_8\\), \\(_9\\), \\(_{10}\\)]\nWe have 10 bugs in total, so we‚Äôll say \\(N=10\\). This is our ‚Äútoken count.‚Äù We‚Äôll use the \\(i\\) subscript to refer to each individual bug (or token).\nIf we made a table of each bug species, it would look like:\n\n\n\nspecies\nindex \\(j\\)\ncount\n\n\n\n\n\n1\n5\n\n\n\n2\n2\n\n\n\n3\n1\n\n\n\n4\n1\n\n\n\n5\n1\n\n\n\nLet‚Äôs use \\(M\\) to represent the total number of species, so \\(M=5\\) here. This is our type count, and we‚Äôll the subscript \\(j\\) to represent the index of specific types.\nWe can mathematically represent the count of each species like so.\n\\[\nc_j = C(\\class{fa fa-bug}{}_j)\n\\]\nHere, the function \\(C()\\) takes a specific species representation \\(\\class{fa fa-bug}{}_j\\) as input, and returns the specific count \\(c_j\\) for how many times that species showed up in our net. So when \\(j = {\\color{#785EF0}{1}}\\), \\(\\color{#785EF0}{c_1}=5\\), and when \\(j = {\\color{#FFB000}{4}}\\), \\(\\color{#FFB000}{c_4}=1\\).\nHere‚Äôs a plot, with the species id \\(j\\) on the x-axis, and the number of times that species appeared in the net \\(c_j\\) on the y-axis.\n\n\n\n\n\n\n\nWhat is the probability that tomorrow, when we put the net out again, that the first bug we catch will be from species ? Usually in these cases, we‚Äôll use past experience to predict the future. Today, of the \\(N=10\\) bugs we caught, \\(\\color{#785EF0}{c_1}=5\\) of them were species . We can represent this as a fraction like so:\n\\[\n\\frac{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5}\n{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_6,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_7,\n      {\\color{#FE6100}{\\class{fa fa-bug}{}}}_8,\n      {\\color{#FFB000}{\\class{fa fa-bug}{}}}_9,\n      {\\color{#4C8C05}{\\class{fa fa-bug}{}}}_{10}}\n\\]\nOr, we can simplify it a little bit. The top part (the numerator) is equal to \\(\\color{#785EF0}{c_1}=5\\), and the bottom part (the denominator) is equal to the total number of bugs, \\(N\\). Simplifying then:\n\\[\n\\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe‚Äôll use this as our guesstimate of the probability that the very next bug we catch will be from species . Let‚Äôs use the function \\(\\hat{P}()\\) to mean ‚Äúour method for guessing the probability‚Äù, and \\(\\hat{p}\\) to represent the guess we came to. We could express ‚Äúour guess that the first bug we catch will be ‚Äù like so.\n\\[\n{\\color{#785EF0}{\\hat{p}_1}} = \\hat{P}({\\color{#785EF0}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n\\]\nWe can then generalize our method to any bug like so:\n\\[\n\\hat{p}_j = \\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\n\\]\n\n\n\nLet‚Äôs say we set out the net again, and the first bug we catch is actually . This is a new species of bug that wasn‚Äôt in the net the first time. Makes enough sense, the forest is very large. However, what probability would we have given catching this new species?\nWell, \\(\\color{#35F448}{c_6} = C({\\color{#35F448}{\\class{fa fa-bug}{}}}) = 0\\). So our estimate of the probability would have been \\({\\color{#35F448}{\\hat{p}_6}} = \\hat{P}({\\color{#35F448}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#35F448}{c_6}}{N} = \\frac{0}{10} = 0\\).\nWell obviously, the probability that we would catch a bug from species  wasn‚Äôt 0, because events with 0 probability don‚Äôt happen, and we did catch the bug. Admittedly, \\(N=10\\) is a small sample to try and base a probability estimate on, so how large would we need the sample to be before we could make probabity estimates for all possible bug species, assuming we stick with the probability estimating function \\(\\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\\)?"
  },
  {
    "objectID": "data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "href": "data_sparsity/data_sparsity.html#youd-need-fa-infinity",
    "title": "Data Sparsity",
    "section": "You‚Äôd need ",
    "text": "You‚Äôd need \nThis kind of data problem does arise for counting species, but this is really a tortured analogy for language data.1 For example, let‚Äôs take all of the words from Chapter 1 of Mary Shelly‚Äôs Frankenstein, downloaded from Project Gutenberg. I‚Äôll count how often each word occurred, and assign it a rank, with 1 being given to the word that occurred the most.1¬†For me, I used this analogy to include colorful images of bugs in the lecture notes. For Good (1953), they had to use a tortured analogy since the methods for fixing probability estimates were still classified after being used to crack the Nazi Enigma Code in WWII.\n\n\n\n\n\n\n\n\n\nJust to draw the parallels between the two analogies:\n\n\n\n\n\n\n\n\nvariable\nin the analogy\nin Frankenstein Chapter 1\n\n\n\n\n\\(N\\)\nThe total number of bugs caught in the net. (\\(N=10\\))\nThe total number of words in the first chapter. (\\(N=1,780\\)).\n\n\n\\(x_i\\)\nAn individual bug. e.g.¬†\\(_1\\)\nAn individual word token. In chapter 1, \\(x_1\\) = ‚Äúi‚Äù\n\n\n\\(w_j\\)\nA bug species. \nA word type. The indices are frequency ordered, so for chapter 1 \\(w_1\\) = ‚Äúof‚Äù\n\n\n\\(c_j\\)\nThe count of how many individuals there are of a species.\nThe count of how many tokens there are of a type.\n\n\n\nHere‚Äôs a table of the top 10 most frequent word types.\n\n\n\n\n\n\\(w_j\\)\n\\(c_j\\)\n\\(j\\)\n\n\n\n\nof\n75\n1\n\n\nthe\n75\n2\n\n\nand\n70\n3\n\n\nto\n61\n4\n\n\na\n52\n5\n\n\nher\n52\n6\n\n\nwas\n40\n7\n\n\nmy\n33\n8\n\n\nin\n32\n9\n\n\nhis\n29\n10\n\n\n\n\n\nIf we plot out all of the word types with the rank (\\(j\\)) on the x-axis and the count of each word type (\\(c_j\\)) on the y-axis, we get a pattern that if you‚Äôre not already familiar with it, you will be.\n\n\n\n\n\nThis is a ‚ÄúZipfian Distribution‚Äù a.k.a. a ‚ÄúPareto Distribution‚Äù a.k.a. a ‚ÄúPower law,‚Äù and it has a few features which make it ~problematic~ for all sorts of analyses.\nFor example, let‚Äôs come back to the issue of predicting the probability of the next word we‚Äôre going to see. Language Models are ‚Äústring prediction models,‚Äù after all, and in order to get a prediction for a specific string, you need to have seen the string in the training data. Remember how our bug prediction method had no way of predicting that we‚Äôd see a  because it had never seen one before?\nThere are a lot of possible string types of ‚ÄúEnglish‚Äù that we have not observed in Chapter 1 of Frankenstein. Good & Turing proposed that you could guesstimate that the probability of seeing a never before seen ‚Äúspecies‚Äù was about equal to the proportion of ‚Äúspecies‚Äù you‚Äôd only seen once. With just Chapter 1, that‚Äôs a pretty high probability that there are words you haven‚Äôt seen yet.\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n1216\n0.683\n\n\nyes\n564\n0.317\n\n\n\n\n\nSo, let‚Äôs increase our sample size. Here‚Äôs the same plot of rank by count for chapters 1 through 5.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n9928\n0.858\n\n\nyes\n1649\n0.142\n\n\n\n\n\nWe increased the size of the whole corpus by a factor of 10, but we‚Äôve still got a pretty high probability of encountering an unseen word.\nLet‚Äôs expand it out to the whole book now.\n\n\n\n\n\n\n\n\n\n\nseen once?\ntotal\nproportion\n\n\n\n\nno\n72122\n0.96\n\n\nyes\n3021\n0.04\n\n\n\n\n\n\nüéµ Ain‚Äôt no corpus large enough üéµ\nAs it turns out, there‚Äôs no corpus large enough to guarantee observing every possible word at least once, for a few reasons.\n\nThe infinite generative capacity of language! The set of all possible words is, in principle infinitely large.\nThese power law distributions will always have the a lot of tokens with a frequency of 1, and even just those tokens are going to have their probabilities poorly estimated.\n\nTo illustrate this, I downloaded the 1-grams of just words beginning with [Aa] from the Google Ngrams data set. This is an ngram dataset based on all of the books scanned by the Google Books project. It‚Äôs 4 columns wide, 86,618,505 rows long, and 1.8G large, and even then I think it‚Äôs a truncated version of the data set, because the fewest number of years any given word appears is exactly 40.\nIf we take just all of the words that start with [Aa] published in the year 2000, the most common frequency for a word to be is still just 1, even if it is a small proportion of all tokens.\n\n\n\nFrequencies of frequencies in words starting with [Aa] from the year 2000 in google ngrams \n\n\n\n\n\n\n\nword frequency\nnumber of types with frequency\nproportion of all tokens\n\n\n\n\n1\n205141\n4.77e-10\n\n\n2\n152142\n9.55e-10\n\n\n3\n107350\n1.43e-09\n\n\n4\n80215\n1.91e-09\n\n\n5\n60634\n2.39e-09\n\n\n6\n47862\n2.86e-09\n\n\n\n\n\n\n\nAn aside\nI‚Äôll be plotting the rank vs the frequency with logarithmic axes from here on. Linear axes give equal visual space for every incremental change in the x and y values, while lograrithmic axes put more space between smaller numbers than larger numbers.\n\n\n\n\n\n\nrank by frequency on linear scales\n\n\n\n\n\n\n\nrank by frequency on logarithmic scales\n\n\n\n\n\n\n\nIt gets worse\nWe can maybe get very far with our data sparsity for how often we‚Äôll see each individual word by increasing the size of our corpus size, but 1gram word counts are rarely as far as we‚Äôll want to go.\nTo come back to our bugs example, let‚Äôs say that bug species  actually hunts bug species . If we just caught a  in our net, it‚Äôs a lot more likely that we‚Äôll catch a  next, coming after the helpless  than it would be if we hadn‚Äôt just caught a . To know what exactly the probability catching  and then a  is, we‚Äôd need to count up every 2 bug sequence we‚Äôve seen.\nBringing this back to words, 2 word sequences are called ‚Äúbigrams‚Äù and 3 word sequences are called ‚Äútrigrams,‚Äù and they are also distributed according to a Power Law, and each larger string of words has a worse data sparsity one than the one before. But each larger string of words means more context, which makes for better predictions."
  },
  {
    "objectID": "data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "href": "data_sparsity/data_sparsity.html#some-notes-on-power-laws",
    "title": "Data Sparsity",
    "section": "Some Notes on Power Laws",
    "text": "Some Notes on Power Laws\nThe power law distribution is pervasive in linguistic data, in almost every domain where we might count how often something happens or is observed. This is absolutely a fact that must be taken into account when we develop our theories or build our models. Some people also think it is an important fact to be explained about language, but I‚Äôm deeply skeptical.\nA lot of things follow power law distributions. The general property of these distributions is that the second most frequent thing will have a frequency about as half as the most frequent thing, the third most frequent thing will have a frequency about a third of the most frequent thing, etc. We could put that mathematically as:\n\\[\nc_j = \\frac{c_1}{j}\n\\]\nFor example, here‚Äôs the log-log plot of baby name rank by baby name frequency in the US between 1880 and 2017.22¬†Data from the babynames R package, which in turn got the data from the Social Security Administration.\n\n\n\n\n\nrank by frequency of baby names\n\n\n\n\nThe log-log plot isn‚Äôt perfectly straight (it‚Äôs common enough for data like this to have two ‚Äúregimes‚Äù).\nHere‚Äôs the number of ratings each movie on IMDB has received.\n\n\n\n\n\nIf we break down the movies by their genre, we get the same kind of result.\n\n\n\n\n\nOther things that have been shown to exhibit power law distributions (Newman 2005; Jiang and Jia 2011) are\n\nUS city populations\nnumber of citations academic papers get\nwebsite traffic\nnumber of copies books sell\nearthquake magnitudes\n\nThese are all possibly examples of ‚Äúpreferential attachment‚Äù, but we can also create an example that doesn‚Äôt involve preferential attachment, and still wind up with a power-law. Let‚Äôs take the first 12 words from Frankenstein:\n\n\n\n\"to\"\"mrs\"\"saville\"\"england\"\"st\"\"petersburgh\"\"dec\"\"11th\"\"17\"\"you\"\"will\"\"rejoice\"\n\n\n\nNow, let‚Äôs paste them all together into one long string with spaces.\n\n\n\n\"to mrs saville england st petersburgh dec 11th 17 you will rejoice\"\n\n\nAnd now, let‚Äôs choose another arbitrary symbol to split up words besides \" \". I‚Äôll go with e.\n\n\n\n\"to mrs savill\"\" \"\"ngland st p\"\"t\"\"rsburgh d\"\"c 11th 17 you will r\"\"joic\"\"\"\n\n\n\nThe results aren‚Äôt words. They‚Äôre hardly useful substrings. But, if we do this to the entire novel and plot out the rank and count of thes substrings like they were words, we still get a power law distribution.\n\n\n\n\n\nIn fact, if I take the top 4 most frequent letters, besides spaces, that occur in the text and use them as substring delimiters, the resulting substring distributions are all power-law distributed.\n\n\n\n\n\nThey even have other similar properties often associated with power law distributions in language. For example, it‚Äôs often been noted that more frequent words tend to be shorter. These weird substrings exhibit that pattern even more strongly than actual words do!\n\n\n\n\n\nThis is all to say, be cautious about explanations for power-law distributions that are"
  },
  {
    "objectID": "data_sparsity/data_sparsity.html#extra",
    "href": "data_sparsity/data_sparsity.html#extra",
    "title": "Data Sparsity",
    "section": "Extra",
    "text": "Extra\nTo work out just how accurate the Good-Turing estimate is, I did the following experiment.\nStarting from the beginning of the book, I coded each word \\(w_i\\) for whether or not it had already appeared in the book, 1 if yes, 0 if no. This is my best shot at writing that out in mathematical notation.\n\\[\na_i = \\left\\{\\begin{array}{ll}1,& x_i\\in x_{1:i-1}\\\\\n                             0,& x_1 \\notin x_{1:i-1}\\end{array}\\right\\}\n\\]\nThen for every position in the book, I made a table of counts of all the words up to that point in the book so far, and got the proportion of word tokens that had appeared only once. Again, here‚Äôs my best stab at writing that out mathematically.\n\\[\nc_{ji} = C(w_j), w_j \\in x_{i:i-1}\n\\]\n\\[\nr_i = \\sum_{j=1}\\left\\{\\begin{array}{ll}1,&c_{ji}=1\\\\0,& c_{ji} >1 \\end{array}\\right\\}\n\\]\n\\[\ng_i = \\frac{r_i}{i-1}\n\\]\n\nfrank_words$first_appearance <- NA\nfrank_words$first_appearance[1] <- 1\n\nfrank_words$gt_est <- NA\nfrank_words$gt_est[1] <- 1\nfor(i in 2:nrow(frank_words)){\n  i_minus <- i-1\n  prev_corp <- frank_words$word[1:i_minus]\n  this_word <- frank_words$word[i]\n  \n  frank_words$first_appearance[i] <- ifelse(this_word %in% prev_corp, 0, 1)\n  frank_words$gt_est[i] <- sum(table(prev_corp) == 1)/i_minus\n}\n\n\n\n\nThen, I plotted the Good-Turing estimate for every position as well as a non-linear logistic regression smooth."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lin517: Natural Language Processing",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "what_is_nlp/index.html#nlp-in-computational-linguistics",
    "href": "what_is_nlp/index.html#nlp-in-computational-linguistics",
    "title": "What is NLP?(for this course)",
    "section": "NLP \\(\\in\\) Computational Linguistics",
    "text": "NLP \\(\\in\\) Computational Linguistics\nIn set notation, \\(\\in\\) means ‚Äúis an element of‚Äù. That is, there‚Äôs a large set of things called ‚ÄúComputational Linguistics‚Äù, and NLP is a part of that larger set.\n‚ÄúComputational Linguistics‚Äù covers a very broad range of topics. Natural Language Processing is currently an area of research and application that receives a lot of attention & money, but Computational Linguistics is a much broader umbrella term. The Association for Computational Linguistics defines it as\n\nComputational linguistics is the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. These models may be ‚Äúknowledge-based‚Äù (‚Äúhand-crafted‚Äù) or ‚Äúdata-driven‚Äù (‚Äústatistical‚Äù or ‚Äúempirical‚Äù). Work in computational linguistics is in some cases motivated from a scientific perspective in that one is trying to provide a computational explanation for a particular linguistic or psycholinguistic phenomenon; and in other cases the motivation may be more purely technological in that one wants to provide a working component of a speech or natural language system. Indeed, the work of computational linguists is incorporated into many working systems today, including speech recognition systems, text-to-speech synthesizers, automated voice response systems, web search engines, text editors, language instruction materials, to name just a few."
  },
  {
    "objectID": "what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "href": "what_is_nlp/index.html#some-examples-of-x-x-in-textcltextandx-ne-textnlp",
    "title": "What is NLP?(for this course)",
    "section": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)",
    "text": "Some examples of \\(\\{x | x \\in \\text{CL}~\\text{and}~x \\ne \\text{NLP}\\}\\)\nThis is set notation that means ‚Äúthe set of things, such that each thing is in Computational Linguistics, and the thing is not Natural Language Processing\n\nFormalizing Theory\nOne use of computational linguistics is to formalize linguistic theories into a computational framework. This might seem weird, since a lot of linguistic theory already looks very formal. But giving mathy looking definitions in a verbal description of a theory is a very different thing from implementing that theory in code that will run.\nSome examples are\n\nMinimalist Parsers (Berwick and Stabler 2019) implementing parsers for Minimalist Syntax\nThe Gradual Learning Algorithm (Boersma and Hayes 2001) implementing constraint re-ranking in Optimality Theory\nThe Tolerance Principle (Charles. Yang 2016), formalizing how learners might acquire rules that have exceptions.\n\nThe interesting thing with formalizing verbal theories, computationally, is that things that might seem like big differences in the verbal theories could turn out to be computationally identical, and some things that might not seem like a big difference can turn out to be massively different computationally.\n\n\nConceptual Experiments\nYou can use general computational principles to flesh out what you would expect to happen given under specific theories, or to use specific computational implementations of linguistic theory to explore their consequences.\nHere‚Äôs a little example from dialectology. We have two proposed principles:\n\nGarde‚Äôs Principle: Mergers are irreversible by linguistic means\n\nonce a community gets merger, like the cot/caught merger, it cannot get back the distinction\n\nHerzog‚Äôs Corollary: Mergers expand at the expense of distinctions.\n\nonce a community develops a merger, like the cot/caught merger, it will inevitably spread geographically to other communities\n\n\nWe can translate these two principles into a ‚Äúfinite state automaton‚Äù below.\n\n\n\n\n\n\n\nfinite_state_machine\n\n  \n\nd\n\n …î/…ë distinction   \n\nd->d\n\n  0.90   \n\nm\n\n …î/…ë merger   \n\nd->m\n\n  0.10   \n\nm->d\n\n  0.01   \n\nm->m\n\n  0.99   \n\ninit\n\n   \n\ninit->d\n\n   \n\n\n\n\n\nA verbal translation of this diagram would be\n\nWe start out in a state of distinguishing between /…î/ and /…ë/. With each step in time (‚Äúgeneration‚Äù), we probably keep distinguishing between /…î/ and /…ë/ with a 0.9 probability, but there‚Äôs some chance we become a merged community. Once we become a merged community, we are overwhelmingly likely to remain a merged community with a 0.99 probability. But there is a very little probability that we might go back to being merged at 0.01 probability.\n\nUnder these circumstances, are we inevitably going to become a merged community? How long until we reach the maximum probability of becoming a merged community? We can answer these questions with a conceptual experiment, converting the description and diagram above into a transition probability matrix, and then just doing a bunch of matrix multiplications.\n\n# python\nimport numpy as np\n\nd_change = np.array([0.90, 0.10])\nm_change = np.array([0.01, 0.99])\n\nchange_mat = np.row_stack((d_change, m_change))\nprint(change_mat)\n\n[[0.9  0.1 ]\n [0.01 0.99]]\n\n\n\n# python\ninitial_state = np.array((1,0))\nn_generations = 100\ncollector = [initial_state]\n\ncurrent_state = initial_state\nfor i in range(n_generations):\n  new_state = current_state @ change_mat\n  collector.append(new_state)\n  current_state = new_state\n  \nresults_mat = np.row_stack(collector)\n\n\n\n\n\n\nLooks like with the probabilities set up this way, we‚Äôre not guaranteed to become a merged community. The probability is very high (about 0.91), but not for certain. We might say, seeing this, that unless the Garde‚Äôs Principle is absolute (it‚Äôs impossible to undo a merger by any means) then Herzog‚Äôs Corollary won‚Äôt necessarily hold.\nOther examples of conceptual experiments are\n\nC. D. Yang (2000) used a model of variable grammar learning to see if he could predict which grammars (e.g.¬†V2 vs no-V2) would win over time.\nSneller, Fruehwald, and Yang (2019) used the tolerance principle to see if a specific phonological change in Philadelphia could plausibly develop on its own, or if it had to be due to dialect contact.\nLinzen and Baroni (2021) used RNNs (a kind of neural network) to see if ‚Äúgarden path‚Äù sentences (e.g.¬†‚ÄúThe horse raced past the barn fell.‚Äù1) were difficult just because the word at the pivot point was especially unlikely.\n\n\n\nAgent Based Modelling\nThis doesn‚Äôt always fall under the rubric of ‚Äúcomputational linguistics,‚Äù but agent-based modelling involves programming virtual ‚Äúagents‚Äù that then ‚Äúinteract‚Äù with each other. Part of what you program into the simulation is rules for how agents interact with each other, and what information they exchange or adopt when they do. It‚Äôs often used to model the effect of social network structure.\n\nDe Boer (2001) models vowel system acquisition and development over time.\nStanford and Kenny (2013) explore models of geographic spread of variation.\nKauhanen (2017) explores whether any linguistic variant needs to have an advantage over another in order to become the dominant form.\n\n\n\nBuilding and using computational tools and data\nOf course, there is a massive amount of effort that goes into constructing linguistic corpora, and developing computational tools to analyze those corpora."
  },
  {
    "objectID": "what_is_nlp/index.html#nlp",
    "href": "what_is_nlp/index.html#nlp",
    "title": "What is NLP?(for this course)",
    "section": "NLP",
    "text": "NLP\nFor this class, we‚Äôll be mostly focusing on the ‚ÄúLanguage Modeling‚Äù component of NLP, and we‚Äôll be following the definition of ‚ÄúLanguage Model‚Äù from Bender and Koller (2020) as a model trained to predict what string or word is most likely in the context of other words. For example, from the following sentence, can you guess the missing word?\n\nI could tell he was mad from the tone of his [____]\n\n\nUsing the predictions\nLanguage model predictions are really useful for many applications. For example, let‚Äôs say you built an autocaptioning system that took audio and processed it into a transcription. You might have a situation where the following sentence gets transcribed.\n\nPoker and blackjack are both  games people play at casinos.\n\nThe digital signal, , in this sentence is consistent with two possible words here\n\ncar\ncard\n\nUs humans here know that in the context of ‚Äúpoker‚Äù, ‚Äúblackjack‚Äù, ‚Äúgames‚Äù and ‚Äúcasinos‚Äù, the more likely word is ‚Äúcard‚Äù, not ‚Äúcar.‚Äù But a simple model that‚Äôs just processing acoustics doesn‚Äôt know that. So to improve your captioning, you‚Äôd probably want to incorporate a language model that takes the context into account and boosts the probability of ‚Äúcard‚Äù.\nThis is just one example, but there are many other kinds of string prediction tasks, such as:\n\nGiven a string in language A, predict the string in language B (a.k.a. machine translation).\nGiven a whole paragraph, predict a summary of the paragraph (summarization).\nGiven a question, predict an answer (question answering).\nGiven a prompt, continue the text in the same style (text generation).\n\n\n\nUsing the representations\nIn the process of training models to do text generation, they develop internal representations of strings of text that can be useful for other purposes. For example, a common NLP task is ‚Äúsentiment analysis,‚Äù that could be used to analyze, say, reviews of products online.2\nOne really very simplistic approach would be to get a dictionary of words that have been scored for their ‚Äúpositivity‚Äù and ‚Äúnegativity.‚Äù Then, every one of those words or a tweet or what ever has one of those words in it, you add its score as a total sentiment score.\n\n# R\nlibrary(tidytext)\nset.seed(101)\nget_sentiments(\"afinn\") %>%\n  sample_n(10) %>%\n  kable()\n\n\n\n\nword\nvalue\n\n\n\n\nlobby\n-2\n\n\nstricken\n-2\n\n\nloser\n-3\n\n\njealous\n-2\n\n\nbreakthrough\n3\n\n\ninability\n-2\n\n\nharshest\n-2\n\n\nranter\n-3\n\n\ncried\n-2\n\n\nwarfare\n-2\n\n\n\n\n\nHere‚Äôs an example with a notable tweet.\n\n# R\ntweet <- \"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"\ntweet_df <- tibble(word = tweet %>%\n                     tolower() %>%\n                     str_split(\" \") %>%\n                     simplify()) %>%\n  left_join(get_sentiments(\"afinn\")) %>%\n  replace_na(replace = list(value = 0))\n\n\nfull tweetsum\n\n\n\n# R\ntweet_df\n\n# A tibble: 19 √ó 2\n   word      value\n   <chr>     <dbl>\n 1 if            0\n 2 the           0\n 3 zoo           0\n 4 bans          0\n 5 me            0\n 6 for           0\n 7 hollering     0\n 8 at            0\n 9 the           0\n10 animals       0\n11 i             0\n12 will          0\n13 face          0\n14 god           1\n15 and           0\n16 walk          0\n17 backwards     0\n18 into          0\n19 hell         -4\n\n\n\n\n\n# R\ntweet_df %>%\n  summarise(sentiment = sum(value))\n\n# A tibble: 1 √ó 1\n  sentiment\n      <dbl>\n1        -3\n\n\n\n\n\nHowever, this is a kind of lackluster approach to sentiment analysis nowadays. Many language models now now, as a by product of their string prediction training, have more complex representations of words than just a score between -5 and 5, and have representations of whole strings that can be used (so it won‚Äôt give the same score to ‚Äúgood‚Äù and ‚Äúnot good‚Äù).\n\n# python\nfrom transformers import pipeline\n\n# warning, this will download approx\n# 1.3G of data.\nsentiment_analysis = pipeline(\"sentiment-analysis\",\n                              model=\"siebert/sentiment-roberta-large-english\")\n\n\n# python\nprint(sentiment_analysis(\"IF THE ZOO BANS ME FOR HOLLERING AT THE ANIMALS I WILL FACE GOD AND WALK BACKWARDS INTO HELL\"))\nprint(sentiment_analysis(\"This ain't bad!\"))\n\n[{'label': 'NEGATIVE', 'score': 0.9990140199661255}]\n[{'label': 'POSITIVE', 'score': 0.9944340586662292}]"
  }
]