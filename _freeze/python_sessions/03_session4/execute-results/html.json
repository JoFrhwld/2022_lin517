{
  "hash": "6b7d4d3ca470f92c2b6b94f425f092bd",
  "result": {
    "markdown": "---\ntitle: \"Comprehensions and Useful Things\"\neditor: visual\nauthor: Josef Fruehwald\ndate: 2022-09-23\ncategories: \n  - \"python\"\n---\n\n# Instructions\n\n## Setup\n\nWe're going to be exploring the way the `spaCy` package does tokenization.\n\nIf you get an error at the very beginning when hitting Run, run this code to download the `spaCy` model in the shell.\n\n``` bash\n# bash\npython -m spacy download en_core_web_sm\n```\n\nCurrently, the code in `main.py`\n\n1.  loads the `spaCy` English model\n2.  reads in *Frankenstein*\n3.  Strips leading whitespace from the beginning and end of each line\n4.  Concatenates all of the lines into one megastring\n5.  Uses the `spaCy` analyzer to (among other things) tokenize the book.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport spacy\nfrom collections import Counter\nfrom collections import defaultdict\n\n# Load the spaCy english model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# open and read in Frankenstein\nwith open(\"gen/texts/frank.txt\", 'r') as f:\n  lines = f.readlines()\n\n# Remove leading and trailing whitespaces\nlines = [line.strip() for line in lines]\n\n# concatenate frankenstein into one huge string\nfrank_one_string = \" \".join(lines)\n\n# Tokenize all of frankenstein\nfrank_doc = nlp(frank_one_string)\n\nprint(frank_doc[500:600])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nriver. But supposing all these conjectures to be false, you cannot contest the inestimable benefit which I shall confer on all mankind, to the last generation, by discovering a passage near the pole to those countries, to reach which at present so many months are requisite; or by ascertaining the secret of the magnet, which, if at all possible, can only be effected by an undertaking such as mine.  These reflections have dispelled the agitation with which I began my letter, and I feel my heart glow\n```\n:::\n:::\n\n\n## Spacy token structure\n\nWe can treat `frank_doc` like a list, but it's actually a special data structure. The same goes for each token inside `frank_doc`. If you just say\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nprint(frank_doc[506])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nconjectures\n```\n:::\n:::\n\n\nIt will print `conjectures`. But if you say\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(\n  dir(frank_doc[506])\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n```\n:::\n:::\n\n\nYou'll see a *lot* more values and methods associated with the token than you normally would for a string. For example, `frank_doc[506].text` will give us the text of the token, and `frank_doc[506].lemma_` will give us the lemma.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nprint(\n  f\"The word '{frank_doc[506].text}' is lemmatized as '{frank_doc[506].lemma_}'\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe word 'conjectures' is lemmatized as 'conjecture'\n```\n:::\n:::\n\n\nOr we can get the guessed part of speech with `frank_doc[506].pos_`\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(\n  f\"The word '{frank_doc[506].text}' is given the part of speech '{frank_doc[506].pos_}'\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe word 'conjectures' is given the part of speech 'VERB'\n```\n:::\n:::\n\n\nOr we can pull out the guessed morphological information:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(\n  f\"spacy guesses '{frank_doc[506].text}' is '{frank_doc[506].morph}'\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspacy guesses 'conjectures' is 'Number=Sing|Person=3|Tense=Pres|VerbForm=Fin'\n```\n:::\n:::\n\n\n## if-statements to control code (like loops)\n\nWe can use `if` statements to control how our code runs. An `if` statement checks to see if its logical comparison is true, and if it is, it executes its code.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n## This is not true, so it dosn't print\nif frank_doc[506].pos_ == \"NOUN\":\n  print(\"it's a verb!\")\n\n## This is true, so it prints\nif frank_doc[506].pos_ == \"VERB\":\n  print(\"it's a verb!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nit's a verb!\n```\n:::\n:::\n\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 1\n> Print the `.text` of every word whose `.lemma_` is `\"monster\"`\n:::\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 2\n> With a for loop, create a list called `five_letter` which contains every 5 letter word from the book (a.k.a. `.text` is 5 characters long.)\n:::\n\n## Comprehensions\n\n\"Comprehensions\" are a great shortcut around writing out a whole for loop. Let's take the following list:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nrain_list = \"The rain in Spain stays mainly on the plain\".split(\" \")\nprint(rain_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'rain', 'in', 'Spain', 'stays', 'mainly', 'on', 'the', 'plain']\n```\n:::\n:::\n\n\nIf I wanted to capitalize all of those words, one way I could do it is with a for loop\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nupper_rain = []\nfor word in rain_list:\n  upper_rain.append(word.upper())\n\nprint(upper_rain)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['THE', 'RAIN', 'IN', 'SPAIN', 'STAYS', 'MAINLY', 'ON', 'THE', 'PLAIN']\n```\n:::\n:::\n\n\nAlternatively, I could do it with a \"list comprehension\":\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nupper_rain2 = [word.upper() for word in rain_list]\n\nprint(upper_rain2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['THE', 'RAIN', 'IN', 'SPAIN', 'STAYS', 'MAINLY', 'ON', 'THE', 'PLAIN']\n```\n:::\n:::\n\n\nList comprehensions keep the `for word in rain_list` part the same, but instead of needing to initialize a whole empty list, we wrap the whole thing inside `[ ]`, which tells python we're going to capture the results inside a list. The variable (& whatever we do to it) at the beginning of the command is what gets captured.\n\nWe can use `if` statements too.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nai_words = [word for word in rain_list if \"ai\" in word]\n\nprint(ai_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['rain', 'Spain', 'mainly', 'plain']\n```\n:::\n:::\n\n\nWe can even have *nested* for statements\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nrain_list = \"The rain in Spain stays mainly on the plain\".split(\" \")\nletters = [letter\n            for word in rain_list\n              for letter in word]\nprint(letters)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['T', 'h', 'e', 'r', 'a', 'i', 'n', 'i', 'n', 'S', 'p', 'a', 'i', 'n', 's', 't', 'a', 'y', 's', 'm', 'a', 'i', 'n', 'l', 'y', 'o', 'n', 't', 'h', 'e', 'p', 'l', 'a', 'i', 'n']\n```\n:::\n:::\n\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 3\n> With a *list comprehension*, create a list called `five_letter2` which contains every 5 letter word from the book (a.k.a. `.text` is 5 characters long.)\n:::\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 4\n> By whatever means necessary (but I recommend using a list comprehension), create a list containing all of the words with a `VERB` as `.pos`\n:::\n\n## `set()`\n\nA `set` is another special python data structure that, among other things, will \"uniquify\" a list.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nbman_list = \"na na na na na na na na na na na na na na na na Batman\".split(\" \")\nbman_set = set(bman_list)\nprint(bman_set)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'Batman', 'na'}\n```\n:::\n:::\n\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 5\n> Find out how many total words there are in *Frankenstein*, excluding tokens with `.pos` of `PUNCT` and `SPACE`\n:::\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 6\n> Find out how many total *unique* words (`.text`) there are in *Frankenstein*, excluding tokens with `.pos` of `PUNCT` and `SPACE`\n:::\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 7\n> Find out how many total *unique* lemmas (`.lemma_`) there are in *Frankenstein*, excluding tokens with `.pos` of `PUNCT` and `SPACE`\n:::\n\n## `Counter()`\n\nThere is a handy dandy function called `Counter` that we can import from the `collections` module like so\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom collections import Counter\n```\n:::\n\n\nWhen we pass `Counter()` a list, it will return a dictionary of counts of items in that list.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nbman_list = \"na na na na na na na na na na na na na na na na Batman\".split(\" \")\nbman_count = Counter(bman_list)\nprint(bman_count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCounter({'na': 16, 'Batman': 1})\n```\n:::\n:::\n\n\n:::{.callout-note}\n> ##### ðŸ’¡ TASK 8\n> Create a counter dictionary of all of the forms of \"be\" (`.lemma == \"be\"`) in *Frankenstein*\n:::\n\n",
    "supporting": [
      "03_session4_files"
    ],
    "filters": [],
    "includes": {}
  }
}