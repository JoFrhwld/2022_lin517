{
  "hash": "fb4e9c11d584a6b085e0baaeaff4a17c",
  "result": {
    "markdown": "---\ntitle: \"Making and Counting Bigrams\"\neditor: visual\nauthor: Josef Fruehwald\ndate: 2022-09-30\ncategories: \n  - \"python\"\n---\n\n# Instructions\n\nThe goal is, for a given book, find\n\n-   The token *most likely* to follow \"the\". What is its conditional probability?\n-   What is that token's *overall* probability in the book?\n-   How much does knowing the preceding word is \"the\" boost that token's probability vs not know what the preceding word is?\n\n## How to download a book\n\nI've written the [`getbook.py`](assets/getbook.py) script for you to be able to quickly download a book from project gutenberg with the header and license info stripped out. You can use it like this, in the shell:\n\n``` bash\n# bash\npython getbook.py 84 frankenstein.txt\n```\n\n## Some pythonic strategies\n\n### Megastrings\n\nAfter reading in a book (and potentially `.strip()`ing off leading and trailing whitespace), you'll need to glue all of the lines together into one big megastring for tokenizing. You can do that like so:\n\n``` {{python}}\nmegastring = \" \".join(book_lines)\n```\n\n### Counting things\n\nThere's a convenient function called `collections.Counter()` that counts how many things are in a list, and returns a dictionary keyed by the values it counted, with its values as the dictionary values.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#python\nfrom collections import Counter\n\nletters = [\"a\", \"a\", \"b\", \"b\", \"b\"]\nletters_c = Counter(letters)\n\nprint(letters_c)\n\nprint(letters_c[\"b\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCounter({'b': 3, 'a': 2})\n3\n```\n:::\n:::\n\n\nYou can also get the most common value from the counting dictionary with `.most_common(1)`. This returns a list of \"tuples\"\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# python\nprint(letters_c.most_common(1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('b', 3)]\n```\n:::\n:::\n\n\n## Some `nltk` strategies\n\n`nltk` has a few functions that will make this go easier.\n\n#### Side note\n\nYou *might* need to run `nltk.download('punkt')`\n\n### Sentence \"tokenizing\"\n\nIn a long paragraph or a \"megastring\", if we want bigram counts that are sensitive to sentence boundaries, that means we need to first split it up into sentences. We can do that with `ntlk.sent_tokenize()`\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pprint\npp = pprint.PrettyPrinter(indent = 2)\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# python\nfrom nltk import sent_tokenize\n\n\npara = \"This is a sentence. This is a sentence too. Is this?\"\nsentences = sent_tokenize(para)\npp.pprint(sentences)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['This is a sentence.', 'This is a sentence too.', 'Is this?']\n```\n:::\n:::\n\n\n### Word tokenizing\n\nDon't forget to tokenize sentences into words\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# python\nfrom nltk import word_tokenize\n\nsentence_words = [word_tokenize(sent) for sent in sentences]\npp.pprint(sentence_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ ['This', 'is', 'a', 'sentence', '.'],\n  ['This', 'is', 'a', 'sentence', 'too', '.'],\n  ['Is', 'this', '?']]\n```\n:::\n:::\n\n\n### Sentence *padding*\n\nWe'll also want to put start-of-sentence and end-of-sentence padding on each sentence, which we can do with `nltk.lm.preprocessing.pad_both_ends()`\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# python\n\nfrom nltk.lm.preprocessing import pad_both_ends\n\n# n = 2 because we're *going* to do bigrams\n# pad_both_ends returns a special object we're\n# converting to a list, just to see what's happening\nsentence_padded = [list(pad_both_ends(sent, n = 2)) \n                     for sent in sentence_words]\npp.pprint(sentence_padded)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ ['<s>', 'This', 'is', 'a', 'sentence', '.', '</s>'],\n  ['<s>', 'This', 'is', 'a', 'sentence', 'too', '.', '</s>'],\n  ['<s>', 'Is', 'this', '?', '</s>']]\n```\n:::\n:::\n\n\n### Bigrams!!\n\nWe (finally!) get the bigrams in each sentence `nltk.bigrams()`.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# python\nfrom nltk import bigrams\n\n# Again, bigrams() returns a special object we're\n# converting to a list\nsent_bg = [list(bigrams(sent)) \n             for sent in sentence_padded]\npp.pprint(sent_bg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ [ ('<s>', 'This'),\n    ('This', 'is'),\n    ('is', 'a'),\n    ('a', 'sentence'),\n    ('sentence', '.'),\n    ('.', '</s>')],\n  [ ('<s>', 'This'),\n    ('This', 'is'),\n    ('is', 'a'),\n    ('a', 'sentence'),\n    ('sentence', 'too'),\n    ('too', '.'),\n    ('.', '</s>')],\n  [('<s>', 'Is'), ('Is', 'this'), ('this', '?'), ('?', '</s>')]]\n```\n:::\n:::\n\n\n### One big list\n\nBefore you try counting anything, you're going to need to \"flatten\" this list of lists into just one flat list of all of the bigrams.\n\n    left as an exercise to the reader.\n\n## Conditional Probability\n\nWhen I find the \"conditional probability\" of the most common word following \"the\", what I mean is \"What is the probability of the word w, given that we just had 'the'?\". Or, to put it in math terms $P(w | \\text{the})$.\n\nThe conditional probability $P(w | \\text{the})$ is equal to the *joint* probability of P(the, w) (a.k.a. the probability of that bigram out of all bigrams) divided by the probability of just \"the\", $P(\\text{the})$.\n\n$$\nP(w|\\text{the}) = \\frac{P(\\text{the}~w)}{P(\\text{the})}\n$$\n\nTo get the probablity of $P(\\text{the}~w)$, you'll need to divide the count of \"the *w*\" by the count of all bigram tokens (*hint*: this is just how long the list of bigrams is.)\n\nTo get the probability of just \"the\", you'll actually have to get a separate count of just all individual tokens, count how frequent \"the\" is, and divide that by the number of total tokens.\n\n## Strategy\n\nTake a moment or two to list out each piece of code or information you're going to need to get to do this project, at a high level. It doesn't need to be complete, and you'll probably come back to this list and revise it. But having a list like this will help guide you to what the next step in the process is.\n\n",
    "supporting": [
      "04_session5_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}