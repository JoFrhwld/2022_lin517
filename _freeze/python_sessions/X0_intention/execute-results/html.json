{
  "hash": "968203f1300103df099740588c633516",
  "result": {
    "markdown": "---\ntitle: \"From Intention to Code\"\neditor: visual\nauthor: \"Josef Fruehwald\"\ncategories: \n  - \"python\"\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The request to `use_python(\"/Users/joseffruehwald/Library/\nCloudStorage/OneDrive-UniversityofKentucky/Courses/Lin517/2022_fall/\nlin517_site/python_sessions/env/bin/python\")` will be ignored because the\nenvironment variable RETICULATE_PYTHON is set to \"/Users/joseffruehwald/Library/\nCloudStorage/OneDrive-UniversityofKentucky/Courses/Lin517/2022_fall/lin517_site/\nlectures/data_processing/env/bin/python\"\n```\n:::\n:::\n\n\nIn the lecture on [data processing](../lectures/data_processing/index.qmd), we talked about using byte pair encoding. What was kind of wrong about what we did there was I trained a bpe tokenizer on one text (Frankenstein) and evaluated it on the *same* text. So, I'll try to fix that here by training a bpe tokenizer on Frankenstein, then see how it works on Jules Verne's *Voyage to the Center of the Earth*.\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Code for downloading books\"}\nimport gutenbergpy.textget\n\nraw_book = gutenbergpy.textget.get_text_by_id(84)\nclean_book  = gutenbergpy.textget.strip_headers(raw_book)\nwith open(\"texts/frank.txt\", 'wb') as file:\n  x = file.write(clean_book)\n  file.close()\n\nraw_book = gutenbergpy.textget.get_text_by_id(18857)\nclean_book  = gutenbergpy.textget.strip_headers(raw_book)\nwith open(\"texts/voyage.txt\", 'wb') as file:\n  x = file.write(clean_book)\n  file.close()\n```\n:::\n\n\nHere's the code for training the tokenizer.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sentencepiece as spm\nspm.SentencePieceTrainer.train(input = \"texts/frank.txt\", \n                               model_prefix = \"frank\",\n                               vocab_size = 10000, \n                               model_type = \"bpe\")\n```\n:::\n\n\nAnd here, I load the bpe tokenizer, and use it to tokenize both *Frankenstein* and *Voyage to the Center of the Earth*.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsp = spm.SentencePieceProcessor(model_file='frank.model')\n\nwith open(\"texts/voyage.txt\", 'r') as file:\n  voyage_lines = file.readlines()\n\nwith open(\"texts/frank.txt\", 'r') as file:\n  frank_lines = file.readlines()\n  \n\nvoyage_megaline = \"\".join(voyage_lines)\nfrank_megaline = \"\".join(frank_lines)\n\nvoyage_tokens = sp.encode_as_pieces(voyage_megaline)\nfrank_tokens = sp.encode_as_pieces(frank_megaline)\n```\n:::\n\n\nLet's take a look at the first handful of tokens from each.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(frank_tokens[0:30])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['▁Frankenstein', ';', '▁or', ',', '▁the', '▁M', 'od', 'ern', '▁Promethe', 'us', '▁by', '▁Mary', '▁W', 'ollst', 'onecraft', '▁', '(', 'Godwin', ')', '▁Shelley', '▁CONTENTS', '▁Letter', '▁1', '▁Letter', '▁', '2', '▁Letter', '▁', '3', '▁Letter']\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(voyage_tokens[0:30])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['▁A', '▁J', 'O', 'U', 'R', 'N', 'E', 'Y', '▁T', 'O', '▁T', 'H', 'E', '▁C', 'E', 'NT', 'R', 'E', '▁O', 'F', '▁T', 'H', 'E', '▁E', 'A', 'R', 'T', 'H', '▁By', '▁J']\n```\n:::\n:::\n\n\n## The intention\n\nI think there's probably a lot more short tokens in the `voyage_tokens` than there is in `frank_tokens`, just because there's probably a lot of sequences that are frequent in *Voyage* that weren't frequent in *Frankenstein*. Given that that's what I think, then I need to figure out how I can tell if I'm right or wrong.\n\nThere's not only one way I could decide to do this. Let's \"choose your own adventure\" this. There might be some very fancy approaches, but let's stick to\n\n## What measure are we going to use?\n\nLet's keep it at simple as possible\n\n### What's a formula for the measure?\n\n## What values are we going to have to get?\n\n## How do we convert that into code?\n\n### First pass\n\n### Any refinement needed?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}