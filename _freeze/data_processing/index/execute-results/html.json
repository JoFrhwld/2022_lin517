{
  "hash": "b5810f3f4ea61dd6955f089244bee6f5",
  "result": {
    "markdown": "---\ntitle: \"Data Processing\"\nauthor:\n  - name: Josef Fruehwald\n    url: https://jofrhwld.github.io/\ndate: \"2022-9-6\"\neditor: visual\n---\n\n\n## Most data analysis time is spent on data wrangling\n\nBefore we even get to substantive issues of \"text normalization\" and \"tokenization\", we need to also deal with basic data wrangling. For example, let's say I wanted to download 4 works from Mary Shelly from Project Gutenberg and calculate what the most common 4 word sequences in her work are, I might quickly write some code like this.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# python\n\n# urllib.request will download the books\nimport urllib.request\n\n# using a dictionary just to show the title of books here in the code.\nshelley_dict = {\"Tales and stories\": \"https://www.gutenberg.org/cache/epub/56665/pg56665.txt\",\n                \"Frankenstein\" : \"https://www.gutenberg.org/files/84/84-0.txt\",\n                \"The Last Man\" : \"https://www.gutenberg.org/cache/epub/18247/pg18247.txt\",\n                \"Mathilda\" : \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\"}\n\n# A collector list for all of the 4 word sequences\nall_grams4 = []\n\n# Loop over every url\nfor url in shelley_dict.values():\n  book_dat = urllib.request.urlopen(url)\n  \n  # this deals with the \n  # 1. character encoding\n  # 2. trailing whitespace\n  # 3. simplistic tokenization on spaces\n  book_lines = [line.decode(\"utf-8-sig\").strip().split(\" \") \n                for line in book_dat]\n  \n  # This flattens the list above into one long list of words\n  book_words = [word \n                for line in book_lines \n                  for word in line \n                    if len(word) > 0]\n  \n  # Collector list of 4grams from just this book\n  grams4 = []\n  \n  # loop over every index postion up to 4 words short of the end.\n  for i in range(len(book_words)-4):\n    \n    # glue together 4 word sequences with \"_\"\n    grams4.append(\"_\".join(book_words[i:(i+4)]))\n    \n  # Add this book's 4grams to all of the books' 4grams\n  all_grams4 += grams4\n```\n:::\n\n\nThe list `all_grams4` contains a list of every token of 4grams in these books. Let's count them up and look at the top 10 most frequent 4 word phrases Mary Shelley used in her writing!\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom collections import Counter\n\ngram_count = Counter(all_grams4)\ntop10 = gram_count.most_common(10)\n\nfor gram in top10:\n  print(gram)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n('Project_Gutenberg_Literary_Archive', 52)\n('the_Project_Gutenberg_Literary', 44)\n('Gutenberg_Literary_Archive_Foundation', 34)\n('the_terms_of_this', 32)\n('Project_Gutenberg-tm_electronic_works', 30)\n('at_the_same_time', 25)\n('to_the_Project_Gutenberg', 24)\n('*_*_*_*', 22)\n('in_the_United_States', 21)\n('for_the_sake_of', 21)\n```\n:::\n:::\n\n\nSo, either Mary Shelly was obsessed with the `Project Gutenberg Literary Archive`, and `the terms of this` and `for the sake of`, or something else is going on.\n\nAs it turns out, every plain text Project Gutenberg book has header information with a short version of the users' rights and other metadata information, and then at the end has the entirety of the Project Gutenberg License, which is written in legal language.\n\nIn any corpus building project, decisions need to be made about how header, footer, and general boilerplate data like this will be treated. There are handy packages for python and R that make stripping out the legal language easy\n\n-   python: [`gutenbergpy`](https://github.com/raduangelescu/gutenbergpy)\n-   R: [`gutenbergr`](https://docs.ropensci.org/gutenbergr/)\n\nOr, you might decide to leave it all in. It seems pretty clear this is the approach to the dataset they trained GPT-3 on, because if you prompt it wit the first few lines of the Project Gutenberg license, it will continue it.\n\n::: {#fig-licenses layout-ncol=\"2\"}\n![](assets/gut_license.png){#orig-gut}\n\n![](assets/gpt3_gut.png){#gpt_gut}\n\nThe original Project Gutenberg License vs what GPT3 reproduces\n:::\n\n### Markup is everywhere\n\nSetting aside the issue of headers and footers, we also need to deal with the fact that \"markup\" is everywhere. Even in the relatively plain text of Project Gutenberg books, they use underscores `_` to indicate italics or emphasized text.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# python\nunderscore_lines = [line \n                      for line in book_lines \n                        if any([\"_\" in word \n                                  for word in line])]\nfor i in range(4):\n  print(\" \".join(underscore_lines[i]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMathilda _is being published\nof_ Studies in Philology.\nnovelette _Mathilda_ together with the opening pages of its rough\ndraft, _The Fields of Fancy_. They are transcribed from the microfilm\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}