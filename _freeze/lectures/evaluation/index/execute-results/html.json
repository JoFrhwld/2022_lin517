{
  "hash": "e90d7ed4a1cd3fe76e5b661f733d913d",
  "result": {
    "markdown": "---\ntitle: \"Evaluating models\"\ndate: \"2022-10-3\"\neditor: visual\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\n    error: false\ncategories: \n  - \"models\" \n---\n\n::: {.cell}\n\n:::\n\n\nLanguage Models ([including ngram models](../ngram/index.qmd#language-prediction)) are focused on \"string prediction\", meaning we need to evaluate them like we would any predictive model. In both statistics and machine learning, there are some conventionalized approaches to this task that we can discuss in general.\n\n::: callout-note\n## The code\n\nUnless indicated otherwise, the code shown in this lesson is R code. I'm using R code instead of Python, even though Python is the language we're learning in the course, because I'm more familiar with how to so what I want to show you in R.\n:::\n\n## Training vs testing\n\nThe best way to evaluate a prediction model is to see how good its predictions are on some data that you already *know* what the predictions should be. The workflow, then, is\n\n1.  \"Train\" the model on a training data set.\n2.  \"Test\" the model on a test data set.\n\nNow, it's rarely the case that people collect and curate a large training dataset to train a model, then go out and collect and curate a whole nother test dataset to test the model's predictions. Instead, what they do is take their original dataset and split it into two pieces: \"train\" and \"test\". Usually, \"train\" contains most of the data (80% to 90%), while \"test\" is a smaller held-out dataset (10% to 20% of the data).\n\n### Why not train and test on the whole dataset?\n\nWhen the model is \"learning\" how to make its predictions, the values it learns to make those predictions with will always be dependent on the training data. For example, compare the mini-bigram models below, one based on *Frankenstein* and the other based on *Count of Monte Cristo*.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n::: {layout-ncol=\"2\"}\n\n::: {.cell}\n::: {.cell-output-display}\n|title                                   |word |next word |  n|\n|:---------------------------------------|:----|:---------|--:|\n|Frankenstein; Or, The Modern Prometheus |the  |same      | 62|\n|Frankenstein; Or, The Modern Prometheus |the  |most      | 56|\n|Frankenstein; Or, The Modern Prometheus |the  |cottage   | 41|\n|Frankenstein; Or, The Modern Prometheus |the  |sun       | 39|\n|Frankenstein; Or, The Modern Prometheus |the  |first     | 35|\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|title                                  |word |next word |    n|\n|:--------------------------------------|:----|:---------|----:|\n|The Count of Monte Cristo, Illustrated |the  |count     | 1107|\n|The Count of Monte Cristo, Illustrated |the  |young     |  530|\n|The Count of Monte Cristo, Illustrated |the  |same      |  442|\n|The Count of Monte Cristo, Illustrated |the  |door      |  410|\n|The Count of Monte Cristo, Illustrated |the  |first     |  295|\n:::\n:::\n\n:::\n\nIf we used the bigram model trained on *Frankenstein* to predict the most likely word to come after \"the\" on *Frankenstein* itself, we would do pretty well! But if we tried to use it to predict the most likely word to come after \"the\" in *The Count of Monte Cristo*, we'd do a lot worse! The highest frequency `the w` bigram in *The Count of Monte Cristo* (`the count`) doesn't even appear in *Frankenstein*.\n\n### Bias vs Variance\n\nThis issue of the predictions of a model being too particularized to training data is related to the concept of the \"Bias-Variance Tradeoff\".\n\n-   High Bias, Low Variance: One way to think of a high-Bias model is that it \"over simplifies\" the relationships in the data, but it also means that it will generalize to new data sets in a way that's comparable to the training set.\n\n    -   high-Bias model of morphology: All past tense is formed by adding *-ed*. This will produce some errors for verbs like *run*, and *go*, but will generalize well to new verbs, like *yeet*.\n\n-   Low Bias, High Variance: A high-Variance model captures much more detail to the relationships in the data, but that means it might be \"overfit\" on the training data.\n\n    -   high-Variance model of morphology: You just have to memorize every *present* ➡️ *past* mapping verb by verb. This won't any errors on verbs we already know like *wait*, *run* or *go*, but won't generalize well to new verbs, like *yeet*.\n\n### Measuring error (or Loss)\n\nInevitably, the model we're fitting will make predictions that are wrong, and then we need some way to quantify, or measure, how wrong the predictions were. The specific measure we use is going to depend a lot on the kind of prediction task we have, but some you might have already seen (or will see) are\n\n-   **M**ean **S**quared **E**rror, or MSE\n\n-   Cross Entropy\n\n-   F(1) score\n\n-   Accuracy\n\n-   BLEU score\n\nOften the numbers these scores put out aren't meaningful in and of themselves, but rather are used to compare models. But when you see a \"score\" or a \"loss function\" mentioned, understand it, generally, to be a measure of the difference between the values we expected in the test set, vs the values the model predicted.\n\n## A Linear Model example\n\nAs a brief example, let's look at a data set of body measurements of penguins.\n\n![Artwork by \\@allison_horst](assets/penguins.png){fig-align=\"center\" width=\"328\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(palmerpenguins::penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n\n```{.r .cell-code}\nnrow(palmerpenguins::penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 344\n```\n:::\n:::\n\n\nWe're going to look at two models that try to predict the bill length of these penguins, one trying to predict it based on body mass, and the other based on bill depth.\n\n\n::: {#fig-peng1 .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Bill length predicted by body mass](index_files/figure-html/fig-peng1-1.png){#fig-peng1-1 width=480}\n:::\n\n::: {.cell-output-display}\n![Bill length predicted by bill depth](index_files/figure-html/fig-peng1-2.png){#fig-peng1-2 width=480}\n:::\n\nBody measurements of penguins\n:::\n\n\nFirst, well split the data into train and test sets. I'm going to choose a random 80% of the data to be the training set, and use remaining 20% to be the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(517)\n\npenguins_id <- penguins %>% \n                  drop_na() %>%\n                  mutate(row_id = 1:n())\ntrain <- penguins_id %>%\n          slice_sample(prop = 0.8)\ntest <- penguins_id %>%\n          filter(!row_id %in% train$row_id)\nnrow(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 266\n```\n:::\n\n```{.r .cell-code}\nnrow(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 67\n```\n:::\n:::\n\n\nNow, I'll fit two linear models with the training set. The linear model is saying \"the predicted bill length for a given predictor (body mass or bill depth) is whatever value is on the line.\"\n\n::: {#fig-train layout-ncol=\"2\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=480}\n:::\n:::\n\n\nModels trained on `train`.\n:::\n\nThese are both \"High Bias\" models. Hardly any of the points are exactly on the lines.\n\nNow, let's see how well these models perform on the held out test data.\\\\\n\n\n::: {.cell}\n\n:::\n\n\n::: {#fig-test layout-ncol=\"2\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=480}\n:::\n:::\n\n\nModel performance on the test set\n:::\n\nIt's kind of hard to see from just the figures which model performed better on the test set. To evaluate the error, or loss, of these models on the test set, I'll use the Mean Squared error, which is calculated like so:\n\n-   For every data point, subtract the predicted value from the actual value, to get the difference between the prediction and reality.\n\n-   Multiply each difference by itself, a.k.a. square it.[^1]\n\n-   Get the mean of these squared errors.\n\n[^1]: The point of doing this is to get a measure that is always positive. If for one data point actual-predicted = 2, then squaring it gets $2^2=4$. If another data point had an error of the same size, but in the opposite direction, then actual-predicted=-2, and squaring it gets us $-2^2=4$.\n\nIn math terms, let's say\n\n-   $y_i$ is the *actual* value of some data point.\n\n-   $\\hat{y}_i$ is the *predicted* value\n\n-   $N$ is the total number of data points.\n\nThen the mean squared error is\n\n$$\n\\text{MSE} = \\frac{\\sum_{i=1}^N(y_i-\\hat{y}_i)^2}{N}\n$$\n\nI'll show you the actual code I'm using to get the MSE, in case it demystifies the mathematical formula.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff_body        = test$bill_length_mm - test$body_pred\nsquare_diff_body = diff_body^2\nmse_body         = mean(square_diff_body)\nprint(mse_body)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16.72451\n```\n:::\n\n```{.r .cell-code}\ndiff_bill        = test$bill_length_mm - test$bill_pred\nsquare_diff_bill = diff_bill^2\nmse_bill         = mean(square_diff_bill)\nprint(mse_bill)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 26.83769\n```\n:::\n:::\n\n\nIt looks like the model predicting bill length using body mass has a smaller MSE than the model predicting bill length using bill depth. So if we had to choose between these two models, we'd go with the body mass model.\n\n## Precision, Recall, and F-Measure\n\nFor NLP tasks, we're not usually trying to predict a continuous measure, but rather trying to categorize words, sentences, or documents. In these kinds of tasks, when we evalulate a model, we'll often want to use measures known as \"Precision\", \"Recall\" and \"F-measure\".\n\nTo illustrate these measures, let's say we've built a robot to pick raspberries, and we want it to pick all of the ripe berries (if we leave any behind, they'll rot!) and none of the unripe berries (if we pick them before they're ripe, we've missed out on future berries!). Let's present the robot with this following scene that has 9 berries in the foreground. 5 of them are ripe and 4 of them are unripe.\n\n![](assets/raspberries.jpeg){fig-align=\"center\" width=\"50%\"}\n\n### Scenario 1: It picks all of the berries\n\nLet's say the robot picked all of the berries.\n\n| Picked                  | Unpicked |\n|-------------------------|----------|\n| [ripe berry]{.ripe}     |          |\n| [ripe berry]{.ripe}     |          |\n| [ripe berry]{.ripe}     |          |\n| [ripe berry]{.ripe}     |          |\n| [ripe berry]{.ripe}     |          |\n| [unripe berry]{.unripe} |          |\n| [unripe berry]{.unripe} |          |\n| [unripe berry]{.unripe} |          |\n| [unripe berry]{.unripe} |          |\n\nThis strategy has high \"Recall\". There were a total of 5 ripe berries, and the robot picked all 5 of them.\n\n$$\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{5}{5} = 1\n$$ But, it has low \"precision\". Of all the berries it picked, a lot of them were unripe.\n\n$$\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{5}{9} = 0.55\n$$\n\nThe \"F-measure\" or \"F1 score\" is a way to combine the precision and recall score into one overall performance score.\n\n$$\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{0.55\\times 1}{0.55 + 1}=2\\frac{0.55}{1.55} = 0.71\n$$\n\n### Scenario 2: The robot picks only 1 ripe raspberry\n\n| Picked              | Unpicked                |\n|---------------------|-------------------------|\n| [ripe berry]{.ripe} |                         |\n|                     | [ripe berry]{.ripe}     |\n|                     | [ripe berry]{.ripe}     |\n|                     | [ripe berry]{.ripe}     |\n|                     | [ripe berry]{.ripe}     |\n|                     | [unripe berry]{.unripe} |\n|                     | [unripe berry]{.unripe} |\n|                     | [unripe berry]{.unripe} |\n|                     | [unripe berry]{.unripe} |\n\nThis strategy has a very low recall. There are 5 ripe berries, and it has only picked 1 of them\n\n$$\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{1}{5} = 0.2\n$$ *But*, it has an extremely high precision. Of all of the berries it picked, it *only* picked ripe ones!\n\n$$\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{1}{1} = 1\n$$ The very low precision winds up dragging down the overall F-measure. $$\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{1 \\times 0.2}{1 + 0.2}=2\\frac{0.2}{1.2} = 0.33\n$$\n\n### Scenario 3: The robot alternates between picking and not picking\n\n| Picked                  | Unpicked                |\n|-------------------------|-------------------------|\n| [ripe berry]{.ripe}     |                         |\n|                         | [ripe berry]{.ripe}     |\n| [ripe berry]{.ripe}     |                         |\n|                         | [ripe berry]{.ripe}     |\n| [ripe berry]{.ripe}     |                         |\n|                         | [unripe berry]{.unripe} |\n| [unripe berry]{.unripe} |                         |\n|                         | [unripe berry]{.unripe} |\n| [unripe berry]{.unripe} |                         |\n\n$$\n\\text{recall} = \\frac{\\text{ripe picked berries}}{\\text{ripe berries}} = \\frac{3}{5} = 0.6\n$$\n\n$$\n\\text{precision} = \\frac{\\text{ripe picked berries}}{\\text{picked berries}} = \\frac{3}{5} = 0.6\n$$\n\n$$\nF_1 = 2\\frac{\\text{precision}\\times \\text{recall}}{\\text{precision} + \\text{recall}}=2\\frac{0.6 \\times 0.6}{0.6 + 0.6}=2\\frac{0.36}{1.2} = 0.6\n$$\n\n### A non-berry example\n\nOne kind of NLP task is \"**N**amed **E**ntity **R**ecognition\" (NER), or detecting and identifying the kind of named entities in a text. Here's an example of `spaCy` doing that with a the sentence\n\n> Dr. Josef Fruehwald is teaching Lin517 at the University of Kentucky in the Fall 2022 semester.\n\n\n\n```{.python .cell-code}\n# python\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ntext = \"Dr. Josef Fruehwald is teaching Lin517 at the University of Kentucky in the Fall 2022 semester.\"\n\ndoc = nlp(text)\nprint(displacy.render(doc, style = 'ent'))\n```\n\n<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Dr. \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Josef Fruehwald\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n is teaching \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Lin517\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n at \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    the University of Kentucky\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n in the Fall \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    2022\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n</mark>\n semester.</div>\n\n\nWhen looking at the original sentence, we could think of each word, or span of words, as a berry, and the named entities as the ripe berries. The precision and recall here would be\n\n$$\n\\text{precision}=\\frac{\\text{named entitites identified}}{\\text{identified words}}\n$$\n\n$$\n\\text{recall}=\\frac{\\text{named entities identified}}{\\text{named entities}}\n$$\n\nIn this example, additional adjustments might need to be made for whether the *span* of the named entities is correct. `2022` is correctly a date, but maybe `Fall` should be included in its span. Also, it has identified `Lin517` as a **G**eo**P**olitical **E**ntity (GPE).\n\n## Edit Distance\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}