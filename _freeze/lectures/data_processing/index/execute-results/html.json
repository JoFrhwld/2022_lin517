{
  "hash": "b08c5593848d68b49bda12bd62b0a3a1",
  "result": {
    "markdown": "---\ntitle: \"Data Processing\"\nauthor:\n  - name: Josef Fruehwald\n    url: https://jofrhwld.github.io/\ndate: \"2022-9-6\"\neditor: visual\nbibliography: references.bib\n---\n\n\n## Most data analysis time is spent on data wrangling\n\nBefore we even get to substantive issues of \"text normalization\" and \"tokenization\", we need to also deal with basic data wrangling. For example, let's say I wanted to download 4 works from Mary Shelly from Project Gutenberg and calculate what the most common 4 word sequences in her work are, I might quickly write some code like this.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# python\n\n# urllib.request will download the books\nimport urllib.request\n\n# using a dictionary just to show the title of books here in the code.\nshelley_dict = {\"Tales and stories\": \"https://www.gutenberg.org/cache/epub/56665/pg56665.txt\",\n                \"Frankenstein\" : \"https://www.gutenberg.org/files/84/84-0.txt\",\n                \"The Last Man\" : \"https://www.gutenberg.org/cache/epub/18247/pg18247.txt\",\n                \"Mathilda\" : \"https://www.gutenberg.org/cache/epub/15238/pg15238.txt\"}\n\n# A collector list for all of the 4 word sequences\nall_grams4 = []\n\n# Loop over every url\nfor url in shelley_dict.values():\n  book_dat = urllib.request.urlopen(url)\n  \n  # this deals with the \n  # 1. character encoding\n  # 2. trailing whitespace\n  # 3. simplistic tokenization on spaces\n  book_lines = [line.decode(\"utf-8-sig\").strip().split(\" \") \n                for line in book_dat]\n  \n  # This flattens the list above into one long list of words\n  book_words = [word \n                for line in book_lines \n                  for word in line \n                    if len(word) > 0]\n  \n  # Collector list of 4grams from just this book\n  grams4 = []\n  \n  # loop over every index postion up to 4 words short of the end.\n  for i in range(len(book_words)-4):\n    \n    # glue together 4 word sequences with \"_\"\n    grams4.append(\"_\".join(book_words[i:(i+4)]))\n    \n  # Add this book's 4grams to all of the books' 4grams\n  all_grams4 += grams4\n```\n:::\n\n\nThe list `all_grams4` contains a list of every token of 4grams in these books. Let's count them up and look at the top 10 most frequent 4 word phrases Mary Shelley used in her writing!\n\n\n::: {#tbl-4gram .cell tbl-cap='Top 10 4grams from Mary Shelley\\'s Work'}\n|  4gram    |  count    |\n|------|------:|\n| `Project_Gutenberg_Literary_Archive` | 52 |\n| `the_Project_Gutenberg_Literary` | 44 |\n| `Gutenberg_Literary_Archive_Foundation` | 34 |\n| `the_terms_of_this` | 32 |\n| `Project_Gutenberg-tm_electronic_works` | 30 |\n| `at_the_same_time` | 25 |\n| `to_the_Project_Gutenberg` | 24 |\n| `*_*_*_*` | 22 |\n| `in_the_United_States` | 21 |\n| `for_the_sake_of` | 21 |\n:::\n\n\nSo, either Mary Shelly was obsessed with the `Project Gutenberg Literary Archive`, and `the terms of this` and `for the sake of`, or something else is going on.\n\nAs it turns out, every plain text Project Gutenberg book has header information with a short version of the users' rights and other metadata information, and then at the end has the entirety of the Project Gutenberg License, which is written in legal language.\n\nIn any corpus building project, decisions need to be made about how header, footer, and general boilerplate data like this will be treated. There are handy packages for python and R that make stripping out the legal language easy\n\n-   python: [`gutenbergpy`](https://github.com/raduangelescu/gutenbergpy)\n-   R: [`gutenbergr`](https://docs.ropensci.org/gutenbergr/)\n\nOr, you might decide to leave it all in. It seems pretty clear this is the approach to the dataset they trained GPT-3 on, because if you prompt it with the first few lines of the Project Gutenberg license, it will continue it.\n\n::: {#fig-licenses layout-ncol=\"2\"}\n![](assets/gut_license.png){#orig-gut fig-alt=\"The original plain text Project Gutenberg license\"}\n\n![](assets/gpt3_gut.png){#gpt_gut fig-alt=\"The GPT3 api, which has been given the first few lines of the Project Gutenberg license, and has completed the rest.\"}\n\nThe original Project Gutenberg License vs what GPT3 reproduces\n:::\n\n### Markup is everywhere\n\nSetting aside the issue of headers and footers, we also need to deal with the fact that \"markup\" is everywhere. Even in the relatively plain text of Project Gutenberg books, they use underscores `_` to indicate italics or emphasized text.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# python\nunderscore_lines = [line \n                      for line in book_lines \n                        if any([\"_\" in word \n                                  for word in line])]\nfor i in range(4):\n  print(\" \".join(underscore_lines[i]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMathilda _is being published\nof_ Studies in Philology.\nnovelette _Mathilda_ together with the opening pages of its rough\ndraft, _The Fields of Fancy_. They are transcribed from the microfilm\n```\n:::\n:::\n\n\nThis, again, is something we need to decide whether or not we want to include in our corpora. For these massive language models that focus on text generation, they may *want* the model to generate markup along with the text, so they might leave it in. Some text markup that's intended to indicate prosodic patterns could be useful to leave in from a linguistic theory perspective.\n\nEither way, though, it is still a *decision* that needs to be made about the data.\n\n## Text Normalization\n\nI called the issues above \"data wrangling\", since it's mostly about identifying the *content* we want to be including in our modelling. But once you've done that, there are still questions about how we process data for analysis which fall under \"text normalization\".\n\nConsider the following sentences\n\n> The 2019 film Cats is a movie about cats. Cats appear in every scene. A cat can always be seen.\n\nLet's split this sentence up along whitespace[^1], and count how many times \"cats\" appears.\n\n[^1]: The regex shortcut `\\s` stands for all white space, including spaces, tabs, newlines and character returns.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport re\nphrase = \"\"\"The 2019 film Cats is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\nwords = re.split(\"\\s\", phrase)\ncat_c = Counter(words)\n```\n:::\n\n::: {#tbl-cats .cell tbl-cap='frequency of \"cats\"' tbl-colwidths='[25,25]'}\n|  word    |  count    |\n|---|---:|\nCats | 2\ncats. | 1\ncat | 1\n:::\n\n\nA very important thing to keep in mind is that our language models will treat the words in these rows as three completely separate word types.[^2] That even includes the period `.` in the second row. Some typical steps involve\n\n[^2]: At least, before we start doing fancier models that start taking into account distributional semantics.\n\n-   separating punctuation from words\n\n-   \"case folding\" or converting everything to lowercase.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwords2 = re.split(r\"\\s\", phrase)\nwords2 = [re.sub(r\"\\W\", '', word) for word in words2]\nwords2 = [word.lower() for word in words2]\nwords2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['the', '2019', 'film', 'cats', 'is', 'a', 'movie', 'about', 'cats', '', 'cats', 'appear', 'in', 'every', 'scene', '', 'a', 'cat', 'can', 'always', 'be', 'seen']\n```\n:::\n:::\n\n::: {#tbl-cats_token .cell tbl-cap='frequency of tokenized \"cats\"'}\n|  word    |  count    |\n|---|---:|\ncats | 3\ncat | 1\n:::\n\n\nWe've now got a slightly better set of counts. With the punctuation stripped and everything pushed to lowercase, there's now just two word forms: `cats` and `cat`.\n\nOne downside, though, is we've also collapsed together the title *Cats*, which refers to either a Broadway musical or a 2019 film, and the word \"cats\" which refers to furry felines. Merging these two together could be sub-optimal for later tasks, like, say, sentiment analysis of movie reviews.\n\n> 'Cats' is both a horror and an endurance test, a dispatch from some neon-drenched netherworld where the ghastly is inextricable from the tedious. -- [LA Times](https://www.latimes.com/entertainment-arts/movies/story/2019-12-18/cats-review-andrew-lloyd-webber)\n\n<details>![](assets/sadcat.jpeg){fig-alt=\"A cat looking at the camera that has been photoshopped to appear as if it is crying.\" fig-align=\"center\"}</details>\n\n## Tokenization (*or*, text is complex)\n\nSetting aside *semantic* issues, there are a lot of things that happen inside of text, especially if it is transcribed speech, that makes normalizing text and **tokenizing** it way more challenging than just splitting up on white space and stripping out punctuation, even just for English.\n\n#### Places to leave in punctuation\n\nSome examples given by Jurafsky & Martin for where you might want to leave in punctuation are:\n\n-   You don't want to eliminate punctuation from inside `Ph.D`, or `m.p.h.`. You also don't want to eliminate it from some proper names, like ampersands in `Procter & Gamble`, `Texas A&M`, `A&W`, `m&m's`.\n\n-   You'll want to keep formatting in numerals, and not split them into separate words. These are all possible numeric formats cross culturally for the same quantity\n\n    -   `1,000.55`\n\n    -   `1.000,55`\n\n    -   `1 000,55`\n\n-   Currency symbols should probably be kept together with their numerals, and depending on the culture & denomination.\n\n    -   `$0.99`\n\n    -   `99¢`\n\n    -   `€0,99`\n\n-   Dates: There are so many different permutations on how dates can be formatted that I shouldn't list them all here, but here are some.[^3]\n\n    -   yyyy-mm-dd `2022-09-12`, yyyy/mm/dd `2022/09/12`\n\n    -   yyyy-m-dd `2022-9-12`, yyyy/m/dd `2022/9/12`\n\n    -   dd-mm-yyyy `12-09-2022`, dd/mm/yyyy `12/09/2022`\n\n    -   dd-m-yyyy `12-9-2022`, dd/m/yyyy `12/9/2022`\n\n    -   dd-mm-yy `12-09-22`, dd/mm/yy `12/09/2022`\n\n    -   mm-dd-yyyy `09-12-2022`, mm/dd/yyyy `09/12/2022`\n\n    -   m-dd-yyyy `9-12-2022`, m/dd/yyyy `9/12/2022`\n\n    -   mm-dd-yy `09-12-22`, mm/dd/yy `09/12/22`\n\n    -   m-dd-yy `9-12-22`, m/dd/yy `9/12/22`\n\n-   Emoticons,[^4] where the token is entirely punctuation `:)`, `>.<`.\n\n[^3]: I'm being tedious here on purpose, because you have to keep in mind that if you wrote a function to handle just one of these possible date formats, it would not immediately translate over to the others!\n\n[^4]: This example *isn't* from Jurafsky & Martin.\n\n#### Places to split up words\n\nSometimes the tokens you get back from whitespace tokenization ought to be split up even further. One example might be hyphenated words, like `hard-won`.\n\n-   `hard-won` ➔ `hard`, `won` or `hard`, `-`, `won`.\n\nAnother example involves clitics, like `n't` or `'re` in English.\n\n-   `isn't` ➔ `is`, `n't`\n\n-   `can't` ➔ `ca`, `n't`\n\n-   `what're` ➔ `what`, `'re`\n\n#### Places to glue words together\n\nYou might want to also glue together tokens from whitespace tokenization.\n\n-   `New`, `York`, `City` ➔ `New York City`\n\n-   `Super`, `Smash`, `Brothers` ➔ `Super Smash Brothers`.\n\n#### Challenges with speech and text\n\n-   \n\n    {{< fa keyboard >}}: $1500\n\n    -   \n\n        {{< fa bullhorn >}}: \"one thousand five hundred dollars\"\n\n    -   \n\n        {{< fa bullhorn >}}: \"fifteen hundred dollars\"\n\n    -   \n\n        {{< fa bullhorn >}}: \"one and a half thousand dollars\"\n\n    -   \n\n        {{< fa bullhorn >}}: \"one point five thousand dollars\"\n\n## Tokenizers\n\nThere seem to be *broadly* two kinds of tokenizers people use, depending on their goals.\n\n1.  Tokenizers that try to hew to linguistic structure, and can generate relatively large vocabulary sizes (number of tokens).\n2.  Tokenizers that try to keep the vocabulary size *relatively* small, to make neural network training possible.\n\n### Word/language piece based tokenizers\n\nThere are a number of tokenizers available through the nltk (Natural Language Took Kit) [@bird2009] python package. They all have slightly different settings and outcomes. Here I'll compare the PennTreeBank tokenizer, a simpler punctuation-based tokenizer, and a \"casual\" tokenizer.\n\n#### PennTreeBank\n\nThe [PennTreeBank tokenizer](https://www.nltk.org/_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer) is built up out of regular expressions (more on that soon). It\n\n-   separates out punctuation and non-alphanumeric characters as their own tokens\n\n-   Separates off contractions as their own tokens, [using a fixed list](https://github.com/nltk/nltk/blob/2e9cf65072f8cecdc5f2beaa7a66ac8de8f5b31e/nltk/tokenize/destructive.py#L18)\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n\nphrase2 = \"\"\"CATS had a budget of $100,000,000, most of which went into the so-called 'digital fur technology'.\nIt's a little hard to believe, but it only made $75.5 million at the box office. \n#badmovie :-P\n\"\"\"\n\nsentences = sent_tokenize(phrase2)\ntokens = [TreebankWordTokenizer().tokenize(s) for s in sentences]\n```\n:::\n\n|   |   |   |   |   |   |\n|---|---|---|---|---|---|\n|`CATS`|`had`|`a`|`budget`|`of`|`$`|\n|`100,000,000`|`,`|`most`|`of`|`which`|`went`|\n|`into`|`the`|`so-called`|`'digital`|`fur`|`technology`|\n|`'`|`.`|`It`|`'s`|`a`|`little`|\n|`hard`|`to`|`believe`|`,`|`but`|`it`|\n|`only`|`made`|`$`|`75.5`|`million`|`at`|\n|`the`|`box`|`office`|`.`|`#`|`badmovie`|\n|`:`|`-P`|\n\n\n#### Simple whitespace + punctuation tokenizer\n\nSplits strings based on whitespace & non-alphanum\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\ntokens = [wordpunct_tokenize(s) for s in sentences]\n```\n:::\n\n|   |   |   |   |   |   |\n|---|---|---|---|---|---|\n|`CATS`|`had`|`a`|`budget`|`of`|`$`|\n|`100`|`,`|`000`|`,`|`000`|`,`|\n|`most`|`of`|`which`|`went`|`into`|`the`|\n|`so`|`-`|`called`|`'`|`digital`|`fur`|\n|`technology`|`'.`|`It`|`'`|`s`|`a`|\n|`little`|`hard`|`to`|`believe`|`,`|`but`|\n|`it`|`only`|`made`|`$`|`75`|`.`|\n|`5`|`million`|`at`|`the`|`box`|`office`|\n|`.`|`#`|`badmovie`|`:-`|`P`|\n\n\n#### Tweet Tokenizer\n\nIntended to be more apt for [tokenizing tweets](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/casual.py).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.tokenize import TweetTokenizer\ntokens = [TweetTokenizer().tokenize(s) for s in sentences]\n```\n:::\n\n|   |   |   |   |   |   |\n|---|---|---|---|---|---|\n|`CATS`|`had`|`a`|`budget`|`of`|`$`|\n|`100,000`|`,`|`000`|`,`|`most`|`of`|\n|`which`|`went`|`into`|`the`|`so-called`|`'`|\n|`digital`|`fur`|`technology`|`'`|`.`|`It's`|\n|`a`|`little`|`hard`|`to`|`believe`|`,`|\n|`but`|`it`|`only`|`made`|`$`|`75.5`|\n|`million`|`at`|`the`|`box`|`office`|`.`|\n|`#badmovie`|`:-P`|\n\n\n### Fixed Vocab Tokenizing\n\nThe downside of tokenizers like the three above is that you can't pre-specify how many types you will get out. That is, you can't pre-specify your vocabulary size. That isn't ideal for neural-network based models, which need to use matrices of finite and pre-specified size. So there are also tokenizers that keep a fixed cap on the vocabulary size, even if they result in tokens that aren't really linguistically meaningful.\n\n#### Byte Pair Encoding\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\nWords\n\n    c a t s _\n    c a n ' t _\n    c a n t e r _\n\nVocabulary\n\n    {'a', 'e', 't', 'r', \n    's', 'n', 'c', \"'\", ' '}\n:::\n\n\n::: {.cell}\n\n:::\n\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\nWords\n\n    ca t s _\n    ca n ' t _\n    ca n t e r _\n\nVocabulary\n\n    {'a', 'e', 't', 'r', \n    's', 'n', 'c', \"'\", ' ', 'ca'}\n:::\n\n::: {layout-ncol=\"2\" style=\"border-style: solid;\"}\nWords\n\n    ca t s _\n    can ' t _\n    can t e r _\n\nVocabulary\n\n    {'a', 'e', 't', 'r', \n    's', 'n', 'c', \"'\", ' ', 'ca', 'can'}\n:::\n\n#### Training\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport gutenbergpy.textget\nimport sentencepiece as spm\n\nraw_book = gutenbergpy.textget.get_text_by_id(84)\nclean_book  = gutenbergpy.textget.strip_headers(raw_book)\nwith open(\"texts/frank.txt\", 'wb') as file:\n  x = file.write(clean_book)\n  file.close()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nspm.SentencePieceTrainer.train(input = \"texts/frank.txt\", \n                               model_prefix = \"m\",\n                               vocab_size = 5000, \n                               model_type = \"bpe\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nsp = spm.SentencePieceProcessor(model_file='m.model')\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code .code-overflow-wrap}\npara = \"\"\"\nYou will rejoice to hear that no disaster has accompanied the\ncommencement of an enterprise which you have regarded with such evil\nforebodings. I arrived here yesterday, and my first task is to assure\nmy dear sister of my welfare and increasing confidence in the success\nof my undertaking\n\"\"\"\n\nsp.encode_as_pieces(para)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['▁You', '▁will', '▁rejo', 'ice', '▁to', '▁hear', '▁that', '▁no', '▁disaster', '▁has', '▁accompanied', '▁the', '▁commencement', '▁of', '▁an', '▁enterprise', '▁which', '▁you', '▁have', '▁regarded', '▁with', '▁such', '▁evil', '▁fore', 'bod', 'ings', '.', '▁I', '▁arrived', '▁here', '▁y', 'esterday', ',', '▁and', '▁my', '▁first', '▁task', '▁is', '▁to', '▁assure', '▁my', '▁dear', '▁sister', '▁of', '▁my', '▁we', 'lf', 'are', '▁and', '▁incre', 'asing', '▁confidence', '▁in', '▁the', '▁success', '▁of', '▁my', '▁undertaking']\n```\n:::\n:::\n\n\n#### The Benefit?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|method | once| total| proportion once|\n|:------|----:|-----:|---------------:|\n|bpe    |  262|  4652|           0.056|\n|ptb    | 3533|  7710|           0.458|\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}