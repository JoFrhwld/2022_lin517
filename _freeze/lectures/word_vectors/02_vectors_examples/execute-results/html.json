{
  "hash": "37570f271d38b5a46ded758ce3a54098",
  "result": {
    "markdown": "---\ntitle: \"Term-Document and Term-Term matrices\"\neditor: visual\nbibliography: references.bib\n---\n\n\n## Outline\n\n## Term-Document Matrices\n\nLast time, we started looking at limited view of Term-Document matrices. Here we've got books in the rows, and the words `monster` and `sea` as the dimensions of the vector space.\n\n\n::: {#tbl-monster-sea .cell .tbl-cap-location-bottom tbl-cap='Books by Mary Shelley and Jules Verne in the \\'monster\\', \\'sea\\' vector space.'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Code to download and process books from Project Gutenberg.\"}\nimport gutenbergpy.textget\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\nfrom tabulate import tabulate\nfrom os.path import exists\n\ndef getbook(book, outfile):\n  \"\"\"\n  Download a book from project Gutenberg and save it \n  to the specified outfile\n  \"\"\"\n  if exists(outfile):\n    pass\n  else:\n    print(f\"Downloading Project Gutenberg ID {book}\")\n    raw_book = gutenbergpy.textget.get_text_by_id(book)\n    clean_book = gutenbergpy.textget.strip_headers(raw_book)\n    if not outfile:\n      outfile = f'{book}.txt'\n      print(f\"Saving book as {outfile}\")\n    with open(outfile, 'wb') as file:\n      file.write(clean_book)\n      file.close()\n\ndef get_unigram_counts(path):\n  \"\"\"\n    Given a path, generate a counter dictionary of unigrams\n  \"\"\"\n  with open(path, 'r') as f:\n    text = f.read()\n  text = text.replace(\"\\n\", \" \").lower()\n  unigrams = RegexpTokenizer(r\"\\w+\").tokenize(text)\n  count = Counter(unigrams)\n  return(count)\n\ndef get_term_count(book_dict, term):\n  \"\"\"\n    return a list of the number of times a term has appeared\n    in a book\n  \"\"\"\n  out = [book_dict[book][term] for book in book_dict]\n  return(out)\n    \nmary_shelley_ids = [84, 15238, 18247, 64329]\nmary_shelley_files =  [f\"gen/books/shelley/{x}.txt\" for x in mary_shelley_ids]\nmary_shelley_titles = [\"Frankenstein\", \"Mathilda\", \"The Last Man\", \"Falkner\"]\njules_verne_ids = [103, 164, 1268, 18857]\njules_verne_files = [f\"gen/books/verne/{x}.txt\" for x in jules_verne_ids]\njules_verne_titles = [\"80days\", \"ThousandLeagues\", \"MysteriousIsland\", \"CenterOfTheEarth\"]\n\nfoo = [getbook(x, f\"gen/books/shelley/{x}.txt\") for x in mary_shelley_ids]\nfoo = [getbook(x, f\"gen/books/verne/{x}.txt\") for x in jules_verne_ids]\n\n\n\nshelley_words = {k:get_unigram_counts(v) \n                    for k, v in zip(mary_shelley_titles, mary_shelley_files)}\nverne_words = {k:get_unigram_counts(v) \n                    for k, v in zip(jules_verne_titles, jules_verne_files)}\n\nmonster = [\"monster\"] + \\\n          get_term_count(shelley_words, \"monster\") + \\\n          get_term_count(verne_words, \"monster\")\nsea  = [\"sea\"] + \\\n          get_term_count(shelley_words, \"sea\") + \\\n          get_term_count(verne_words, \"sea\")                 \n\ntranspose = list(zip(mary_shelley_titles+jules_verne_titles, monster[1:], sea[1:]))\nprint(tabulate(transpose, headers=[\"book\", \"monster\", \"sea\"]))\n```\n\nbook                monster    sea\n----------------  ---------  -----\nFrankenstein             31     34\nMathilda                  3     20\nThe Last Man              2    118\nFalkner                   2     31\n80days                    0     52\nThousandLeagues          44    357\nMysteriousIsland          8    277\nCenterOfTheEarth         19    122\n:::\n\n\nI call this a \"limited\" term-document matrix, since we're only looking at the frequency of two hand-picked word dimensions. If we'd chosen some other words to serve as the dimensions, some of them will have very high counts, and others will be mostly 0. For example, `the` appears very frequently in all books, and `illustration` doesn't appear at all in most of the books.\n\n\n::: {#tbl-the-illustration .cell .tbl-cap-location-margin tbl-cap='Books by Mary Shelley and Jules Verne in the \\'the\\', \\'illustration\\' vector space.'}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Code to generate the 'the', 'illustration' table.\"}\nthe =  [verne_words[book][\"the\"] for book in verne_words] + \\\n                [shelley_words[book][\"the\"] for book in shelley_words]\nillustration = [verne_words[book][\"illustration\"] for book in verne_words] + \\\n                [shelley_words[book][\"illustration\"] for book in shelley_words]\ntitles = [book for book in verne_words] + [book for book in shelley_words]\n\nprint(\n  tabulate(\n    list(zip(titles, the, illustration)), \n    headers=[\"book\", \"the\", \"illustration\"],\n    intfmt=\",\"\n  )\n)\n          \n```\n\nbook                 the    illustration\n----------------  ------  --------------\n80days             4,715               1\nThousandLeagues    8,578              12\nMysteriousIsland  17,003               0\nCenterOfTheEarth   5,651               4\nFrankenstein       4,195               0\nMathilda           2,214               0\nThe Last Man      11,494               0\nFalkner            7,222               0\n:::\n\n\nThere' also nothing particularly special about any two words chosen words in each book. Ideally we'd be representing each book with the *entire* word vector.\n\n### Getting the whole term-document matrix: python time\n\nRight now, I have Counter dictionaries for each book stored like this:\n\n    # This is pseudocode\n    author_words = {\n      book: {w1: c1,\n             w2: c2,\n             ...},\n      ...\n    }\n\nTo get the complete term-document matrix, I'm going to have to:\n\n-   combine the words from each dictionary into one big set\n\n-   get the count of each word in each dictionary.\n\nI'll then convert the lists I get into one big numpy matrix.\n\n::: callout-note\nThere are some ways to make term-document matrices with the `nltk` or `scikit-learn` that don't involve writing so much code, but they also don't show how they work as explicitly as the code below. So for the purpose of teaching, I'm writing it all out long-hand.\n:::\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n```\n:::\n\n\nFirst, I'll get a list of the book titles, since this will be handy for making tables later on.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nbook_titles = [book for book in shelley_words] + [book for book in verne_words]\nbook_titles\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Frankenstein', 'Mathilda', 'The Last Man', 'Falkner', '80days', 'ThousandLeagues', 'MysteriousIsland', 'CenterOfTheEarth']\n```\n:::\n:::\n\n\nI need to get one big vocabulary that has just one entry per word that appears in *all* of the books. I'm using [python sets](https://www.w3schools.com/python/python_sets.asp) to do this.\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Start with empty set\nbig_vocab= set()\n\n# For every book in Shelley's works, \n# get the Union of `big_vocab` and that book's vocabulary.\nfor book in shelley_words:\n  this_vocab = set(shelley_words[book].keys())\n  big_vocab = big_vocab.union(this_vocab)\n  \n# Repeat for Jules Verne\nfor book in verne_words:\n  this_vocab = set(verne_words[book].keys())\n  big_vocab = big_vocab.union(this_vocab)\n  \n# Convert the set to a list so that we can index it normally\nbig_vocab = list(big_vocab)\n\n# Total vocab size:\nprint(f\"The total vocabulary size is {len(big_vocab):,} words\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe total vocabulary size is 24,681 words\n```\n:::\n:::\n\n\nHere, I create a list of each word's frequency in each book, then convert it all to a numpy matrix.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nword_counts = []\nfor word in big_vocab:\n  document_vector = [shelley_words[book][word] for book in shelley_words] +\\\n                      [verne_words[book][word] for book in verne_words]\n  word_counts.append(document_vector)\n\nword_matrix = np.array(word_counts)\n```\n:::\n\n\nLet's double check what this matrix looks like:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(word_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 0  0  0 ...  1  0  0]\n [ 1  1  7 ...  1  3  2]\n [ 0  0  0 ...  1  0  0]\n ...\n [ 0  0  3 ...  0  0  0]\n [ 6  0 12 ...  2  0  1]\n [ 0  1  0 ...  0  0  0]]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nword_matrix.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(24681, 8)\n```\n:::\n:::\n\n\nSo, there are 24,681 rows, and 8 columns in the matrix. So 1 row for each word, 1 column for each book. We can double check that the numbers look like we expect by getting the indices for specific words, and slicing the term-document matrix:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nexample_words = [\"the\", \"illustration\", \"monster\", \"sea\"]\nexample_idx = [big_vocab.index(w) for w in example_words]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(word_matrix[example_idx, :])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 4195  2214 11494  7222  4715  8578 17003  5651]\n [    0     0     0     0     1    12     0     4]\n [   31     3     2     2     0    44     8    19]\n [   34    20   118    31    52   357   277   122]]\n```\n:::\n:::\n\n\n::: callout-note\n## Sparse Matrix\n\nTerm-document matrices are almost always \"sparse\" matrices. \"Sparse\" meaning a *lot* of its values are 0. We can calculate how many cells of this matrix have counts greater than zero with some numpy tricks.\n\nFirst, we say `word_matrix>0`, it will give us back a matrix of the same size, with `True` where the expression is true and `False` where the expression is false.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nword_matrix>0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[False, False, False, ...,  True, False, False],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [False, False, False, ...,  True, False, False],\n       ...,\n       [False, False,  True, ..., False, False, False],\n       [ True, False,  True, ...,  True, False,  True],\n       [False,  True, False, ..., False, False, False]])\n```\n:::\n:::\n\n\nThe nifty thing is that we can treat a numpy array of `True` and `False` like a matrix of `1` and `0` values, where `True` gets converted to `1` and `False` gets converted to `0`. If we just use `np.mean()` on this `True`/`False` matrix, we'll just get the proportion of values that are greater than 0!\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.mean(word_matrix>0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.3449515821887282\n```\n:::\n:::\n\n\nOnly about 34% of all cells in the matrix have a count greater than 0! This is a matrix *mostly* of 0s.\n:::\n\n### What's an important word *for* each document (tf--idf)?\n\nWe could start comparing documents with the cosine similarity of their word counts. Here's how we'd do it for *Frankenstein* (index `0`) and *Around the world in 80 Days* (index 4).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.spatial.distance import cosine\n1 - cosine(word_matrix[:,0], word_matrix[:, 4])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8759771803390621\n```\n:::\n:::\n\n\nLooks like they're very similar! But then, they *would*. For most of the words they have in common, those words are going to have very large frequencies.\n\n::: {layout-ncol=\"2\"}\n\n| Frankenstein   |       |\n|:---------------|------:|\n| the            | 4,195 |\n| and            | 2,976 |\n| i              | 2,846 |\n| of             | 2,642 |\n| to             | 2,089 |\n| my             | 1,776 |\n| a              | 1,391 |\n| in             | 1,128 |\n| was            | 1,021 |\n| that           | 1,018 |\n\n| 80days   |       |\n|:---------|------:|\n| the      | 4,715 |\n| and      | 1,909 |\n| of       | 1,814 |\n| to       | 1,696 |\n| a        | 1,330 |\n| in       | 1,056 |\n| was      | 1,005 |\n| he       |   989 |\n| his      |   858 |\n| fogg     |   646 |\n\n:::\n\nWe want to treat frequent words in each document as *important* for characterizing that document, while at the same time not giving too much weight to words that are frequent in *every* document. In comes \"tf--idf\".\n\n#### Tf--idf\n\n\"Tf--idf\" stands for \"term frequency-inverse document frequency\". Annoyingly, the \"--\" in its name is a *hyphen*, so we're not doing subtraction.\n\n\"Term frequency\" is the frequency of each word within each document. It's really just the `word_matrix` we've already made. Except we take the log-transform of the frequency.\n\nWe've looked at the log transform before, but just to remind you, it has the effect of squashing down the right side of a distribution, and stretching out the left side of a distribution.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_vectors_examples_files/figure-html/unnamed-chunk-16-1.png){width=288}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](02_vectors_examples_files/figure-html/unnamed-chunk-17-1.png){width=288}\n:::\n:::\n\n\n*But* remember how most of the numbers in `word_matrix` are 0?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.log10(0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-inf\n\n<string>:1: RuntimeWarning: divide by zero encountered in log10\n```\n:::\n:::\n\n\nSo, what we do to fix this is add 1 to every value (yes, again) and take the log10 of that.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntf = np.log10(word_matrix + 1)\n```\n:::\n\n\nNext, for every word we get a count of how many documents it appeared in. So, \"the\" appeared in all 8 books, so it will have a document frequency of 8. \"Illustration\" only appeared in 3 books, so it will have a document frequency of 3.\n\nWe can use another handy feature of numpy, and tell it to sum across the columns (`axis=1`)\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndf = np.sum(word_matrix > 0, axis = 1)\ndf.shape\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(24681,)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([1, 8, 1, ..., 2, 6, 2])\n```\n:::\n:::\n\n\nBut the measure we use is *inverse* document frequency. For that, we actually do $\\frac{N}{df}$ where $N$ is the total number of documents. And then, for good measure, we also take the log10 transform.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nidf = np.log10(8/df)\n```\n:::\n\n\nTo get the tf-idf, we just multiply each book's term frequency vector by the inverse document frequency vector.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntf_idf = tf * idf[:, np.newaxis]\n```\n:::\n\n\n#### The upshot\n\nAfter all of this, we have a measure for each word within each book that balances out its frequency in *this* book and its appearance frequency across all books.\n\n| tf                                      | idf                                   | tf-idf            |\n|-----------------------------------------|---------------------------------------|-------------------|\n| Frequent word in this book (large tf)   | Appears in most books (small idf)     | Mediocre tf-idf   |\n| Infrequent word in this book (small tf) | Appears in most books (small idf)     | Very small tf-idf |\n| Frequent word in this book (large tf)   | Appears in very few books (large idf) | Large tf-idf      |\n\n#### The Results\n\nLet's explore these tf-idf values. First, we can get the indicies of the words in each book with the largest tf-idf values with `.argmax(axis=0)`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlargest_tfidf = tf_idf.argmax(axis = 0)\nlargest_tfidf_words = [big_vocab[x] for x in largest_tfidf]\n```\n:::\n\n----------------  ----------\nFrankenstein      clerval\nMathilda          mathilda\nThe Last Man      raymond\nFalkner           falkner\n80days            fogg\nThousandLeagues   _nautilus_\nMysteriousIsland  pencroft\nCenterOfTheEarth  hans\n----------------  ----------\n\n\nWe can get the indicies of the top 5 using `.argsort()` like this:\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntop_five = (tf_idf * -1).argsort(axis = 0)[0:5, :]\n```\n:::\n\n\n```{.python .cell-code}\ntop_five_words = np.empty(shape = (5,8), dtype = 'object')\nfor i in range(top_five.shape[0]):\n  for j in range(top_five.shape[1]):\n    top_five_words[i,j] = big_vocab[top_five[i,j]]\n```\n\n::: {.cell}\n\n```{.python .cell-code}\nbook_titles\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Frankenstein', 'Mathilda', 'The Last Man', 'Falkner', '80days', 'ThousandLeagues', 'MysteriousIsland', 'CenterOfTheEarth']\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntop_five_words.T\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([['clerval', 'justine', 'safie', 'agatha', 'dæmon'],\n       ['mathilda', '_f', '_mathilda_', 'woodville', 'a_'],\n       ['raymond', 'adrian', 'perdita', 'idris', 'evadne'],\n       ['falkner', 'neville', 'osborne', 'boyvill', 'raby'],\n       ['fogg', 'passepartout', 'phileas', 'aouda', 'detective'],\n       ['_nautilus_', 'ned', 'conseil', 'aronnax', 'nemo'],\n       ['pencroft', 'harding', 'neb', 'spilett', 'reporter'],\n       ['hans', 'sneffels', 'saknussemm', 'hardwigg', 'icelandic']],\n      dtype=object)\n```\n:::\n:::\n\nFrankenstein    Mathilda    The Last Man    Falkner    80days        ThousandLeagues    MysteriousIsland    CenterOfTheEarth\n--------------  ----------  --------------  ---------  ------------  -----------------  ------------------  ------------------\nclerval         mathilda    raymond         falkner    fogg          _nautilus_         pencroft            hans\njustine         _f          adrian          neville    passepartout  ned                harding             sneffels\nsafie           _mathilda_  perdita         osborne    phileas       conseil            neb                 saknussemm\nagatha          woodville   idris           boyvill    aouda         aronnax            spilett             hardwigg\ndæmon           a_          evadne          raby       detective     nemo               reporter            icelandic\n\n\nWe can even calculate the cosine similarity of each book from every other book with these tf-idf vectors.\n\n\n\n```{.python .cell-code}\nfrom scipy.spatial.distance import cosine\ndists = np.empty(shape = (8,8))\nfor i in range(8):\n  for j in range(8):\n    dists[i,j] = 1-cosine(tf_idf[:, i], tf_idf[:, j])\nprint(tabulate(dists, headers=book_titles, showindex=book_titles,floatfmt=\".2f\"))\n```\n\n                    Frankenstein    Mathilda    The Last Man    Falkner    80days    ThousandLeagues    MysteriousIsland    CenterOfTheEarth\n----------------  --------------  ----------  --------------  ---------  --------  -----------------  ------------------  ------------------\nFrankenstein                1.00        0.11            0.21       0.22      0.05               0.06                0.07                0.08\nMathilda                    0.11        1.00            0.14       0.11      0.03               0.04                0.04                0.04\nThe Last Man                0.21        0.14            1.00       0.28      0.08               0.09                0.11                0.10\nFalkner                     0.22        0.11            0.28       1.00      0.07               0.07                0.08                0.08\n80days                      0.05        0.03            0.08       0.07      1.00               0.11                0.10                0.07\nThousandLeagues             0.06        0.04            0.09       0.07      0.11               1.00                0.21                0.14\nMysteriousIsland            0.07        0.04            0.11       0.08      0.10               0.21                1.00                0.17\nCenterOfTheEarth            0.08        0.04            0.10       0.08      0.07               0.14                0.17                1.00\n\n\n## Term-context matrix\n\nTerm-document matrices can be useful for classifying and describing documents, but if we wanted to come up with vector representations to describe *words*, we need to build a term-context matrix. The basic intuition behind most vector-semantics draws from the Distributional Hypothesis [@harris1954], which we can illustrate like this.\n\nTry to come up with words that you think are likely to appear in the blank:\n\n-   The elderly \\_\\_ spoke.\n\nNow do the same thing with this phrase:\n\n-   The playful \\_\\_ jumped.\n\nYou probably came up with different sets of words in each context. The idea here is that certain words are more *likely* to appear in certain contexts, and the more contexts two words share, the more similar they are.\n\n### A quick and dirty term-context matrix\n\nWe'll build a quick and dirty term-context matrix with Frankenstein. Often people exclude \"stopwords\", like function words at this stage.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwith open(mary_shelley_files[0], 'r') as f:\n  text = f.read()\nunigrams = RegexpTokenizer(r\"\\w+\").tokenize(text.replace(\"\\n\", \" \").lower())\n```\n:::\n\n\nTo build a term-context matrix, we basically look at a \"concordance\" of every word in the book. We set a context size of some number of words preceding and some number of words following the target word, and then pull those examples out. Let's do that for \"monster.\"\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncontext_size = 3\nfor idx in range(context_size, len(unigrams)-context_size):\n  if unigrams[idx] == \"monster\":\n    full_context = unigrams[idx-context_size : idx+1+context_size]\n    print(full_context)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['wretch', 'the', 'miserable', 'monster', 'whom', 'i', 'had']\n['to', 'behold', 'this', 'monster', 'but', 'i', 'feared']\n['imagined', 'that', 'the', 'monster', 'seized', 'me', 'i']\n['form', 'of', 'the', 'monster', 'on', 'whom', 'i']\n['i', 'was', 'the', 'monster', 'that', 'he', 'said']\n['fear', 'lest', 'the', 'monster', 'whom', 'i', 'had']\n['remaining', 'friends', 'abhorred', 'monster', 'fiend', 'that', 'thou']\n['in', 'reality', 'the', 'monster', 'that', 'i', 'am']\n['i', 'then', 'a', 'monster', 'a', 'blot', 'upon']\n['you', 'form', 'a', 'monster', 'so', 'hideous', 'that']\n['only', 'a', 'detestable', 'monster', 'that', 'is', 'indeed']\n['go', 'he', 'cried', 'monster', 'ugly', 'wretch', 'you']\n['with', 'me', 'hideous', 'monster', 'let', 'me', 'go']\n['and', 'let', 'the', 'monster', 'depart', 'with', 'his']\n['promise', 'fulfilled', 'the', 'monster', 'would', 'depart', 'for']\n['but', 'that', 'the', 'monster', 'followed', 'me', 'and']\n['my', 'rage', 'the', 'monster', 'saw', 'my', 'determination']\n['on', 'whom', 'the', 'monster', 'might', 'satisfy', 'his']\n['fingers', 'of', 'the', 'monster', 'already', 'grasping', 'my']\n['eyes', 'of', 'the', 'monster', 'as', 'i', 'first']\n['me', 'and', 'the', 'monster', 'of', 'my', 'creation']\n['happy', 'if', 'the', 'monster', 'executed', 'his', 'threat']\n['magic', 'powers', 'the', 'monster', 'had', 'blinded', 'me']\n['face', 'of', 'the', 'monster', 'he', 'seemed', 'to']\n['their', 'cause', 'the', 'monster', 'whom', 'i', 'had']\n['to', 'seize', 'the', 'monster', 'be', 'assured', 'that']\n['cursed', 'and', 'hellish', 'monster', 'drink', 'deep', 'of']\n['information', 'a', 'gigantic', 'monster', 'they', 'said', 'had']\n['apparition', 'of', 'the', 'monster', 'seen', 'from', 'our']\n['connected', 'such', 'a', 'monster', 'has', 'then', 'really']\n['my', 'lips', 'the', 'monster', 'continued', 'to', 'utter']\n```\n:::\n:::\n\n\nHere, we'll call `monster` the target, or \\$w\\$, and every other word in the context a context word, or $c$. To build a term-context matrix, we would need a row of the matrix to be dedicated to the word `monster`, and columns for every possible word that could occur around `monster`. We'd then go and add 1 to the relevant column each time we saw a word in the context of `monster`.\n\nTo do this in practice, we need to get a vocuabulary of unique words that appear in the book, and also convenient ways to convert a word string into an index, and a convenient way to convert an index to a word.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvocabulary = set(unigrams)\nword_to_index = {w:idx for idx, w in enumerate(vocabulary)}\nindex_to_word = {idx:w for idx, w in enumerate(vocabulary)}\n```\n:::\n\n\nThen, we need to create a matrix full of zeros with a row and column for each word in the vocabulary.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nterm_context = np.zeros(shape = (len(vocabulary), len(vocabulary)))\n```\n:::\n\n\nThen, we just loop through the book, adding 1 to every cell where the target word (in the rows) appears in the context of another word (in the columns).\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncontext_size = 3\nfor i in range(context_size, len(unigrams)-context_size):\n  word = unigrams[i]\n  word_index = word_to_index[word]\n  prewindow = unigrams[i-context_size : i]\n  postwindow = unigrams[i+1 : i+1+context_size]\n  context = prewindow + postwindow\n  for c in context:\n    c_index = word_to_index[c]\n    term_context[word_index, c_index] += 1\n```\n:::\n\n\nNow, if the term-document matrix was sparse, this is *super* sparse.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.mean(term_context>0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.0044127177791784865\n```\n:::\n:::\n\n\nLet's get the 5 most common words that appear in the context of \"monster\".\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmonster_idx = word_to_index[\"monster\"]\nmonster_array = term_context[monster_idx, :]\ntop_five_monster_idx = (monster_array*-1).argsort()[0:5]\ntop_five_monster_word = [index_to_word[idx] for idx in top_five_monster_idx]\ntop_five_monster_word\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['the', 'i', 'that', 'of', 'me']\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndist_from_monster = []\nfor i in range(len(vocabulary)):\n  dist_from_monster.append(cosine(monster_array, term_context[i, :]))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmonster_disr_arr = np.array(dist_from_monster)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmonster_sim = monster_disr_arr.argsort()[0:10]\nmonster_sim_word = [index_to_word[idx] for idx in monster_sim]\nmonster_sim_word\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['monster', 'on', 'which', 'from', 'fiend', 'and', 'in', 'towards', 'at', 'dæmon']\n```\n:::\n:::\n\n\n### Positive Pointwise Mutual Information\n\nSimilar problem as before, with words appearing very similar because very frequent words show up in a lot of contexts.\n\n\n::: {.cell}\n\n```{.python .cell-code}\njoint_prob = term_context/sum(term_context)\n\nword_C = np.sum(term_context, axis = 1)\nword_prob = word_C / sum(word_C)\n\ncontext_C = np.sum(term_context, axis = 0)\ncontext_prob =context_C/sum(context_C)\n\njoint_exp = np.outer(word_prob, context_prob)\n\nPMI = np.log2(joint_prob/joint_exp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<string>:1: RuntimeWarning: divide by zero encountered in log2\n```\n:::\n\n```{.python .cell-code}\nPMI[PMI < 0] = 0\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmonster_array = PMI[monster_idx, :]\ndist_from_monster = []\nfor i in range(len(vocabulary)):\n  dist_from_monster.append(cosine(monster_array, PMI[i, :]))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmonster_disr_arr = np.array(dist_from_monster)\nmonster_sim = monster_disr_arr.argsort()[0:10]\nmonster_sim_word = [index_to_word[idx] for idx in monster_sim]\nmonster_sim_word\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['monster', 'let', 'accurate', 'denote', 'wretch', 'neck', 'hellish', 'behold', 'supposition', 'enjoy']\n```\n:::\n:::\n",
    "supporting": [
      "02_vectors_examples_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}