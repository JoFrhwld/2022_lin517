{
  "hash": "27d94f9f5d4b0808522ee0b0a8805dbb",
  "result": {
    "markdown": "---\ntitle: \"Word Vectors - Concepts\"\nauthor: Josef Fruehwald\ndate: 2022-10-26\neditor: visual\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\ncategories:\n  - \"word vectors\"\n---\n\n::: {.cell}\n\n:::\n\n\n## Features\n\n### Categorical Features\n\nIn linguistics, you've probably already encountered our tendency to categorize *things* using a bunch of features. For example, in Phonology we often categorize phonemes according to \"distinctive features.\"\n\n|     | voice | continuant | LAB | COR | DORS | anterior |\n|-----|:-----:|:----------:|:---:|:---:|:----:|:--------:|\n| p   |  ➖   |     ➖     | ➕  | ➖  |  ➖  |    ➖    |\n| t   |  ➖   |     ➖     | ➖  | ➕  |  ➖  |    ➖    |\n| k   |  ➖   |     ➖     | ➖  | ➖  |  ➕  |    ➖    |\n| b   |  ➕   |     ➖     | ➕  | ➖  |  ➖  |    ➖    |\n| d   |  ➕   |     ➖     | ➖  | ➕  |  ➖  |    ➖    |\n| g   |  ➕   |     ➖     | ➖  | ➖  |  ➕  |    ➖    |\n| f   |  ➖   |     ➕     | ➕  | ➖  |  ➖  |    ➖    |\n| s   |  ➖   |     ➕     | ➖  | ➕  |  ➖  |    ➖    |\n| ʃ   |  ➖   |     ➕     | ➖  | ➕  |  ➖  |    ➕    |\n\nMore relevant to the topic of word vectors, we could do the same with semantic features and words\n\n|      | domesticated | feline |\n|------|:------------:|:------:|\n| cat  |      ➕      |   ➕   |\n| puma |      ➖      |   ➕   |\n| dog  |      ➕      |   ➖   |\n| wolf |      ➖      |   ➖   |\n\n### Numeric Features\n\nInstead of using categorical values for these features, let's use a numeric score. These represent my own subjective scores for these animals.\n\n|      | domesticated | feline |\n|------|-------------:|-------:|\n| cat  |           70 |    100 |\n| puma |            0 |     90 |\n| dog  |           90 |     30 |\n| wolf |           10 |     10 |\n\nThe sequence of numbers associated with \"cat\", \\[70, 100\\], we'll call a \"vector\". A lot of the work we're going to do with vectors can be understood if we start with two dimensional vectors like this. We can plot each animal as a point in the \\[domesticated, feline\\] \"vector space\".\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![four animals plotted in the domesticated/feline space](01_concept_files/figure-html/fig-vecpoints-1.png){#fig-vecpoints width=480}\n:::\n:::\n\n\n## Vectors\n\nThe word \"vector\" might conjure up ideas of \"direction\" for you, as it should! The way we really want to think about vectors when we're doing word vectors is like a line with an arrow at the end, pointing at the location in \"vector space\" where each animal is.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![four animals plotted in the domesticated/feline space](01_concept_files/figure-html/fig-vecarrow-1.png){#fig-vecarrow width=480}\n:::\n:::\n\n\n### The \"magnitude\" of a vector.\n\nWe're going to have to calculate the magnitudes of vectors, so let's start with calculating the magnitude of the \"dog\" vector.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![dog plotted in the domesticated/feline space](01_concept_files/figure-html/fig-dogvec1-1.png){#fig-dogvec1 width=480}\n:::\n:::\n\n\nThe magnitude of \"dog\" in this vector space is the length of the line that reaches from the \\[0,0\\] point to the location of \"dog\". The mathematical notation we'd use to indicate \"the length of the dog vector\" is $|\\text{dog}|$. We can work out the distance of the vector by thinking about it like a right triangle.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![dog plotted in the domesticated/feline space](01_concept_files/figure-html/fig-dogvec2-1.png){#fig-dogvec2 width=480}\n:::\n:::\n\n\nLooking at the dog vector this way, we can use the Pythagorean Theorem to get its length\n\n$$\n|\\text{dog}|^2 = \\text{domesticated}^2 + \\text{feline}^2\n$$\n\n$$\n|\\text{dog}| = \\sqrt{\\text{domesticated}^2 + \\text{feline}^2}\n$$\n\nI won't go through the actual numbers here, but it turns out the magnitude of dog is 94.87.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\ndog = np.array([90, 30])\ndog_mag1 = np.sqrt(sum([x**2 for x in dog]))\n\n#or\n\ndog_mag2 = np.linalg.norm(dog)\n\nprint(f\"{dog_mag1:.2f} or {dog_mag2:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n94.87 or 94.87\n```\n:::\n:::\n\n\n#### In General\n\nThe way things worked out for dog is how we'd calculate the magnitude of any vector of any dimensionality. You square each value, sum them up, then take the square root of that sum.\n\n$$\n|v|^2 = v_1^2 + v_2^2 + v_3^2  + \\dots +v_i^2\n$$\n\n$$\n|v|^2 = \\sum_{i = 1}^nv_i^2\n$$\n\n$$\n|v| = \\sqrt{\\sum_{i = 1}^nv_i^2}\n$$\n\n### Comparing Vectors\n\nNow, let's compare the vectors for \"cat\" and \"dog\" in this \"vector space\"\n\n\n::: {.cell}\n::: {.cell-output-display}\n![dog and cat plotted in the domesticated/feline space](01_concept_files/figure-html/fig-catdog1-1.png){#fig-catdog1 width=480}\n:::\n:::\n\n\nHow should we compare the closeness of these two vectors in the vector space? What's most common is to estimate the angle between the two, usually notated with $\\theta$, or more specifially, to get the cosine of the angle, $\\cos\\theta$.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![dog and cat plotted in the domesticated/feline space](01_concept_files/figure-html/fig-catdog2-1.png){#fig-catdog2 width=480}\n:::\n:::\n\n\nWhere did cosine come in?? This is a bit of a throwback to trigonometry, again being related to formulas for estimating angles of triangles.\n\nThe specific formula to get $\\cos\\theta$ for dog and cat involves a \"dot product\", which for dog and cat in particular goes like this $$\n\\text{dog}\\cdot \\text{cat} = (\\text{dog}_{\\text{domesticated}}\\times\\text{cat}_{\\text{domesticated}}) +  (\\text{dog}_{\\text{feline}}\\times\\text{cat}_{\\text{feline}})\n$$\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndog = np.array([90, 30])\ncat = np.array([70, 100])\n\ndot1 = sum([x * y for x,y in zip(dog, cat)])\n\n# or!\n\ndot2 = np.dot(dog, cat)\n\nprint(f\"{dot1} or {dot2}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9300 or 9300\n```\n:::\n:::\n\n\nIn general, the dot product of any two vectors will be\n\n$$\na\\cdot b = a_1b_1 + a_2b_2 + a_3b_3 +\\dots a_ib_i\n$$\n\n$$\na\\cdot b= \\sum_{i=1}^n a_ib_i\n$$\n\nOne way to think of the dot product here is if two vectors have very similar values along many dimensions, their dot product will be large. On the other hand, if they're very different, and one had a lot of zeros where the other has large values, the dot product will be small.\n\nThe full formula for $\\cos\\theta$ normalizes the dot product by dividing it by the product the magnitude of dog and cat.\n\n$$\n\\cos\\theta = \\frac{\\text{dog}\\cdot\\text{cat}}{|\\text{dog}||\\text{cat}|}\n$$\n\nThe reason why we're dividing like this is because if the two vectors had the *same* direction, their dot product would equal multiplying their magnitudes.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![two vectors with the same $\\theta$](01_concept_files/figure-html/fig-sametheta-1.png){#fig-sametheta width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nbig_dog = np.array([90, 30])\nsmall_dog = np.array([30, 10])\n\nbig_dog_mag = np.linalg.norm(big_dog)\nsmall_dog_mag = np.linalg.norm(small_dog)\nprint(f\"Product of magnitudes is {(big_dog_mag * small_dog_mag):.0f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProduct of magnitudes is 3000\n```\n:::\n\n```{.python .cell-code}\nbig_small_dot = np.dot(big_dog, small_dog)\nprint(f\"Dot produtct of vectors is {big_small_dot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDot produtct of vectors is 3000\n```\n:::\n:::\n\n\nNormalizing like this means that $\\cos\\theta$ is always going to be some number between -1 and 1, and for the vectors we're going to be looking at, usually between 0 and 1.\n\nFor the actual case of dog and cat\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndog = np.array([90, 30])\ncat = np.array([70, 100])\n\ndog_dot_cat = np.dot(dog, cat)\ndog_mag = np.linalg.norm(dog)\ncat_mag = np.linalg.norm(cat)\n\ncat_dog_cos = dog_dot_cat / (dog_mag * cat_mag)\n\nprint(f\"The cosine similarity of dog and cat is {cat_dog_cos:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe cosine similarity of dog and cat is 0.803\n```\n:::\n:::\n\n\nor\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy import spatial\ncat_dog_cos2 = 1 - spatial.distance.cosine(dog, cat)\nprint(f\"The cosine similarity of dog and cat is {cat_dog_cos2:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe cosine similarity of dog and cat is 0.803\n```\n:::\n:::\n\n\n### With more dimensions\n\nThe basic principles remain the same even if we start including even more dimensions. For example, let's say we added size to the set of features for each animal.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|animal | domesticated| feline| size|\n|:------|------------:|------:|----:|\n|cat    |           70|    100|   10|\n|puma   |            0|     90|   90|\n|dog    |           90|     30|   30|\n|wolf   |           10|     10|   60|\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n```{=html}\n<div id=\"htmlwidget-b36b8f6d5a5398d24030\" style=\"width:100%;height:464px;\" class=\"plotly html-widget\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b36b8f6d5a5398d24030\">{\"x\":{\"visdat\":{\"865f71073904\":[\"function () \",\"plotlyVisDat\"],\"865f11306f95\":[\"function () \",\"data\"],\"865f2eda804\":[\"function () \",\"data\"]},\"cur_data\":\"865f2eda804\",\"attrs\":{\"865f11306f95\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":{},\"y\":{},\"z\":{},\"color\":{},\"mode\":\"lines\",\"colors\":\"Dark2\",\"inherit\":true},\"865f2eda804\":{\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"x\":{},\"y\":{},\"z\":{},\"color\":{},\"colors\":\"Dark2\",\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"range\":[0,100],\"title\":\"domesticated\"},\"yaxis\":{\"range\":[0,100],\"title\":\"feline\"},\"zaxis\":{\"range\":[0,100],\"title\":\"size\"}},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"x\":[0,70],\"y\":[0,100],\"z\":[0,10],\"mode\":\"lines\",\"type\":\"scatter3d\",\"name\":\"cat\",\"marker\":{\"color\":\"rgba(27,158,119,1)\",\"line\":{\"color\":\"rgba(27,158,119,1)\"}},\"textfont\":{\"color\":\"rgba(27,158,119,1)\"},\"error_y\":{\"color\":\"rgba(27,158,119,1)\"},\"error_x\":{\"color\":\"rgba(27,158,119,1)\"},\"line\":{\"color\":\"rgba(27,158,119,1)\"},\"frame\":null},{\"x\":[0,90],\"y\":[0,30],\"z\":[0,30],\"mode\":\"lines\",\"type\":\"scatter3d\",\"name\":\"dog\",\"marker\":{\"color\":\"rgba(217,95,2,1)\",\"line\":{\"color\":\"rgba(217,95,2,1)\"}},\"textfont\":{\"color\":\"rgba(217,95,2,1)\"},\"error_y\":{\"color\":\"rgba(217,95,2,1)\"},\"error_x\":{\"color\":\"rgba(217,95,2,1)\"},\"line\":{\"color\":\"rgba(217,95,2,1)\"},\"frame\":null},{\"x\":[0,0],\"y\":[0,90],\"z\":[0,90],\"mode\":\"lines\",\"type\":\"scatter3d\",\"name\":\"puma\",\"marker\":{\"color\":\"rgba(117,112,179,1)\",\"line\":{\"color\":\"rgba(117,112,179,1)\"}},\"textfont\":{\"color\":\"rgba(117,112,179,1)\"},\"error_y\":{\"color\":\"rgba(117,112,179,1)\"},\"error_x\":{\"color\":\"rgba(117,112,179,1)\"},\"line\":{\"color\":\"rgba(117,112,179,1)\"},\"frame\":null},{\"x\":[0,10],\"y\":[0,10],\"z\":[0,60],\"mode\":\"lines\",\"type\":\"scatter3d\",\"name\":\"wolf\",\"marker\":{\"color\":\"rgba(231,41,138,1)\",\"line\":{\"color\":\"rgba(231,41,138,1)\"}},\"textfont\":{\"color\":\"rgba(231,41,138,1)\"},\"error_y\":{\"color\":\"rgba(231,41,138,1)\"},\"error_x\":{\"color\":\"rgba(231,41,138,1)\"},\"line\":{\"color\":\"rgba(231,41,138,1)\"},\"frame\":null},{\"x\":[70],\"y\":[100],\"z\":[10],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"cat\",\"marker\":{\"color\":\"rgba(27,158,119,1)\",\"line\":{\"color\":\"rgba(27,158,119,1)\"}},\"textfont\":{\"color\":\"rgba(27,158,119,1)\"},\"error_y\":{\"color\":\"rgba(27,158,119,1)\"},\"error_x\":{\"color\":\"rgba(27,158,119,1)\"},\"line\":{\"color\":\"rgba(27,158,119,1)\"},\"frame\":null},{\"x\":[90],\"y\":[30],\"z\":[30],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"dog\",\"marker\":{\"color\":\"rgba(217,95,2,1)\",\"line\":{\"color\":\"rgba(217,95,2,1)\"}},\"textfont\":{\"color\":\"rgba(217,95,2,1)\"},\"error_y\":{\"color\":\"rgba(217,95,2,1)\"},\"error_x\":{\"color\":\"rgba(217,95,2,1)\"},\"line\":{\"color\":\"rgba(217,95,2,1)\"},\"frame\":null},{\"x\":[0],\"y\":[90],\"z\":[90],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"puma\",\"marker\":{\"color\":\"rgba(117,112,179,1)\",\"line\":{\"color\":\"rgba(117,112,179,1)\"}},\"textfont\":{\"color\":\"rgba(117,112,179,1)\"},\"error_y\":{\"color\":\"rgba(117,112,179,1)\"},\"error_x\":{\"color\":\"rgba(117,112,179,1)\"},\"line\":{\"color\":\"rgba(117,112,179,1)\"},\"frame\":null},{\"x\":[10],\"y\":[10],\"z\":[60],\"type\":\"scatter3d\",\"mode\":\"markers\",\"name\":\"wolf\",\"marker\":{\"color\":\"rgba(231,41,138,1)\",\"line\":{\"color\":\"rgba(231,41,138,1)\"}},\"textfont\":{\"color\":\"rgba(231,41,138,1)\"},\"error_y\":{\"color\":\"rgba(231,41,138,1)\"},\"error_x\":{\"color\":\"rgba(231,41,138,1)\"},\"line\":{\"color\":\"rgba(231,41,138,1)\"},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\nWe can do all the same things we did before, with the same math.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndog = np.array([90, 30, 30])\ncat = np.array([70, 100, 10])\n\ndog_mag = np.linalg.norm(dog)\ncat_mag = np.linalg.norm(cat)\n\nprint(f\"dog magnitude: {dog_mag:.2f}, cat magnitude: {cat_mag:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndog magnitude: 99.50, cat magnitude: 122.474\n```\n:::\n\n```{.python .cell-code}\ndog_cat_cos = np.dot(dog, cat)/(dog_mag * cat_mag)\nprint(f\"dog and cat cosine similarity: {dog_cat_cos:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndog and cat cosine similarity: 0.79\n```\n:::\n:::\n\n\n## What does this have to do with NLP?\n\nBefore we get to word, vectors, we can start talking about \"document\" vectors. I've collapsed the next few code blocks for downloading a bunch of books by Mary Shelley and Jules Verne so we can focus on the \"vectors\" part.\n\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"a get book function\"}\nimport gutenbergpy.textget\ndef getbook(book, outfile):\n  \"\"\"\n  Download a book from project Gutenberg and save it \n  to the specified outfile\n  \"\"\"\n  print(f\"Downloading Project Gutenberg ID {book}\")\n  raw_book = gutenbergpy.textget.get_text_by_id(book)\n  clean_book = gutenbergpy.textget.strip_headers(raw_book)\n  if not outfile:\n    outfile = f'{book}.txt'\n    print(f\"Saving book as {outfile}\")\n  with open(outfile, 'wb') as file:\n    file.write(clean_book)\n    file.close()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code  code-fold=\"true\" code-summary=\"Project Gutenberg information\"}\nmary_shelley_ids = [84, 15238, 18247, 64329]\nmary_shelley_files =  [f\"gen/books/shelley/{x}.txt\" for x in mary_shelley_ids]\nmary_shelley_titles = [\"Frankenstein\", \"Mathilda\", \"The Last Man\", \"Falkner\"]\njules_verne_ids = [103, 164, 1268, 18857]\njules_verne_files = [f\"gen/books/verne/{x}.txt\" for x in jules_verne_ids]\njules_verne_titles = [\"80days\", \"ThousandLeagues\", \"MysteriousIsland\", \"CenterOfTheEarth\"]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfoo = [getbook(x, f\"gen/books/shelley/{x}.txt\") for x in mary_shelley_ids]\nfoo = [getbook(x, f\"gen/books/verne/{x}.txt\") for x in jules_verne_ids]\n```\n:::\n\n\nWe're going to very quickly tokenize these books into words, and then get just unigram counts for each book\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.tokenize import RegexpTokenizer\nfrom collections import Counter\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_unigram_counts(path):\n  \"\"\"\n    Given a path, generate a counter dictionary of unigrams\n  \"\"\"\n  with open(path, 'r') as f:\n    text = f.read()\n  text = text.replace(\"\\n\", \" \").lower()\n  unigrams = RegexpTokenizer(r\"\\w+\").tokenize(text)\n  count = Counter(unigrams)\n  return(count)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nshelley_words = {k:get_unigram_counts(v) \n                    for k, v in zip(mary_shelley_titles, mary_shelley_files)}\nverne_words = {k:get_unigram_counts(v) \n                    for k, v in zip(jules_verne_titles, jules_verne_files)}\n```\n:::\n\n\nSo now, `shelley_words` is a dictionary with keys for each book:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nshelley_words.keys()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndict_keys(['Frankenstein', 'Mathilda', 'The Last Man', 'Falkner'])\n```\n:::\n:::\n\n\nAnd the value associated with each key is the unigram count of word in that book:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nshelley_words[\"Frankenstein\"].most_common(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('the', 4195), ('and', 2976), ('i', 2846), ('of', 2642), ('to', 2089), ('my', 1776), ('a', 1391), ('in', 1128), ('was', 1021), ('that', 1018)]\n```\n:::\n:::\n\n\n### Books in word count vector space\n\nBefore were were classifying animals in the \"feline\" and \"domesticated\" vector space. What if we classified These book by Mary Shelley and Jules Verne in the \"monster\" and \"sea\" vector space. We'll just compare them in terms of how many times the word \"monster\" and \"sea\" appeared in each of their books.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_term_count(book_dict, term):\n  \"\"\"\n    return a list of the number of times a term has appeared\n    in a book\n  \"\"\"\n  out = [book_dict[book][term] for book in book_dict]\n  return(out)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmonster = [\"monster\"] + \\\n          get_term_count(shelley_words, \"monster\") + \\\n          get_term_count(verne_words, \"monster\")\nsea  = [\"sea\"] + \\\n          get_term_count(shelley_words, \"sea\") + \\\n          get_term_count(verne_words, \"sea\")\n          \n```\n:::\n\nbook                monster    sea\n----------------  ---------  -----\nFrankenstein             31     34\nMathilda                  3     20\nThe Last Man              2    118\nFalkner                   2     31\n80days                    0     52\nThousandLeagues          44    357\nMysteriousIsland          8    277\nCenterOfTheEarth         19    122\n\n\nSo, in the \"monster\", \"sea\" vector space, we'd say *Frankenstein* has a vector of \\[31, 31\\], and *Around the World in 80 Days* has a vector of \\[0, 52\\]. We can make a vector plot for these books in much the same way we did for the animals in the \"feline\" and \"domesticated\" vector space.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Selected Mary Shelley and Jules Verne novels in the `monster`,`sea` vector space](01_concept_files/figure-html/fig-monster-vec-1.png){#fig-monster-vec width=672}\n:::\n:::\n\n\nWe can also do all of the same vector computations we did before. In @fig-monster-vec, *Frankenstein* and *Around the World in 80 Days* seem to have the largest angle between them. Let's calculate it!\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrank = [shelley_words[\"Frankenstein\"][word] for word in [\"monster\", \"sea\"]]\neighty = [verne_words[\"80days\"][word] for word in [\"monster\", \"sea\"]]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(frank)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[31, 34]\n```\n:::\n\n```{.python .cell-code}\nprint(eighty)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0, 52]\n```\n:::\n:::\n\n\nLet's get their cosine similarity the fast way with `scipy.spatial.distance.cosine()`\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# scipy already imported\n1 - spatial.distance.cosine(frank, eighty)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7389558439133294\n```\n:::\n:::\n\n\nThe novels *Mathilda* and *Journey to the Center of the Earth*, on the other hand, look like they have almost identical angles.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmathilda = [shelley_words[\"Mathilda\"][word] for word in [\"monster\", \"sea\"]]\ncenter = [verne_words[\"CenterOfTheEarth\"][word] for word in [\"monster\", \"sea\"]]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(mathilda)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[3, 20]\n```\n:::\n\n```{.python .cell-code}\nprint(center)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[19, 122]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n1 - spatial.distance.cosine(mathilda, center)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9999842826707133\n```\n:::\n:::\n\n\nHere's table of every novel's cosine similarity from the other in \"monster\", \"sea\" vector space,\n\n\n::: {.cell}\n\n:::\n\n                  Fankenstein    Mathilda    EightyDay      CenterOfTheEarth\n----------------  -------------  ----------  -----------  ------------------\nFankenstein       1              0.83        0.74                       0.83\nMathilda                         1           0.99                       1\nEightyDay                                    1                          0.99\nCenterOfTheEarth                                                        1\n\n\n### The *full* vector space\n\nOf course, we are not limited to calculating cosine similarity on just *two* dimensions. We could use the *whole* shared vocabulary between two novels to compute the cosine similarity.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nshared_vocab = set(list(shelley_words[\"Frankenstein\"].keys()) + \n                   list(verne_words[\"80days\"].keys()))\nprint(f\"Total dimensions: {len(shared_vocab)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal dimensions: 10544\n```\n:::\n\n```{.python .cell-code}\nfrankenstein_vector = [shelley_words[\"Frankenstein\"][v] for v in shared_vocab]\neighty_vector = [verne_words[\"80days\"][v] for v in shared_vocab]\n\n1 - spatial.distance.cosine(frankenstein_vector, eighty_vector)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8759771803390622\n```\n:::\n:::\n\n\n### Author Identification?\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmystery1 = getbook(6447, outfile=\"gen/books/shelley/6447.txt\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloading Project Gutenberg ID 6447\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmystery = get_unigram_counts(\"gen/books/shelley/6447.txt\")\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef get_dist(book1, book2):\n  \"\"\"\n    given unigram counts from two books\n    return the cosine distance\n  \"\"\"\n  shared_vocab = set(list(book1.keys()) + list(book2.keys()))\n  \n  book1_vec = [book1[v] for v in shared_vocab]\n  book2_vec = [book2[v] for v in shared_vocab]\n  \n  sim = 1-spatial.distance.cosine(book1_vec, book2_vec)\n  return(sim)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nshelley_dist = [get_dist(shelley_words[book], mystery) for book in shelley_words]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.mean(shelley_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9314257403627573\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nverne_dist = [get_dist(verne_words[book], mystery) for book in verne_words]\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.mean(verne_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.9376880437662538\n```\n:::\n:::\n",
    "supporting": [
      "01_concept_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/htmlwidgets-1.5.4/htmlwidgets.js\"></script>\n<script src=\"../../site_libs/plotly-binding-4.10.0/plotly.js\"></script>\n<script src=\"../../site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"../../site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"../../site_libs/crosstalk-1.2.0/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/crosstalk-1.2.0/js/crosstalk.min.js\"></script>\n<link href=\"../../site_libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/plotly-main-2.5.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}