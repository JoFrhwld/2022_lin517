{
  "hash": "76102e7358934194a1b2eaef2d11f0f3",
  "result": {
    "markdown": "---\ntitle: \"Lemmatizing and Stemming\"\neditor: visual\nauthor:\n  - name: Josef Fruehwald\n    url: https://jofrhwld.github.io/\nknitr: \n  opts_chunk: \n    echo: true\ndate: \"2022-9-13\"\n---\n\n::: {.cell}\n\n:::\n\n\n## What tokenizing does *not* get for you\n\nComing back to the example sentences from [the first data processing](../data_processing/index.qmd) lecture, properly tokenizing these sentences will only partly help us with our linguistic analysis.\n\n\n\n```{.python .cell-code}\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom tabulate import tabulate\n\nphrase = \"\"\"The 2019 film CATS is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\n# case folding\nphrase_lower = phrase.lower()\n\n# tokenization\ntokens = word_tokenize(phrase_lower)\n\n# counting\ntoken_count = Counter(tokens)\n\n# cat focus\ncat_list = [[k,token_count[k]] for k in token_count if \"cat\" in k]\n\nprint(tabulate(cat_list,\n               headers = [\"type\", \"count\"]))\n```\n\ntype      count\n------  -------\ncats          3\ncat           1\n\n\nWe've still got the plural `cats` being counted as a separate word from `cat`, which for our weird use case, we don't want. Our options here are to either \"stem\" or \"lemmatize\" our tokens.\n\n## Stemming\n\nStemming is focused on cutting off morphemes and, to some degree, providing a consistent stem across all types that share a stem. So the outcomes aren't always a recognizable word. The way it does this is all rule-based. For example, [the first step of the Porter stemmer contains the following rewrite rules](https://snowballstem.org/algorithms/porter/stemmer.html).\n\n    i.   sses -> ss\n    ii.  ies -> i\n    iii. ss -> ss\n    iv.  s -> \n\nWhen a word comes into the first step, if its end matches any of the left hand sides, it will get re-written as the right hand side. If it could match multiple, the one it has the longest match with wins, so\n\n-   \"passes\" matches `i.` , so it gets rewritten as \"pass\"\n\n-   \"pass\" matches `iii.` and `iv.`, but has the largest overlap with `iii.` so it gets rewritten as \"pass\"\n\n-   \"parties\" matches `ii.`, so it gets rewritten as \"parti\"\n\n-   \"pas\" (as in \"faux pas\") matches `iv.` so it gets rewritten as \"pa\"\n\n-   \"cats\" matches `iv.` so it gets rewritten as \"cats\"\n\nThis works basically correctly to the various /+z/ morphemes in English, but it over does it (\"pas\" should be left alone) and it produces some stems that don't *look* like the actual root word (\"parti\" vs \"party\").\n\nAfter this step, it contains a lot more hand crafted rules (e.g. `ational - > ate`).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\n```\n:::\n\n\n```{.python .cell-code}\np_stemmer = PorterStemmer()\np_stemmed = [p_stemmer.stem(t) for t in tokens]\nfor t in p_stemmed:\n  print(f\"`{t}` |\", end = \" \")\n```\n\n`the` | `2019` | `film` | `cat` | `is` | `a` | `movi` | `about` | `cat` | `.` | `cat` | `appear` | `in` | `everi` | `scene` | `.` | `a` | `cat` | `can` | `alway` | `be` | `seen` | \n\n\n```{.python .cell-code}\ns_stemmer = SnowballStemmer(\"english\")\ns_stemmed = [s_stemmer.stem(t) for t in tokens]\nfor t in s_stemmed:\n  print(f\"`{t}` |\", end = \" \")\n```\n\n`the` | `2019` | `film` | `cat` | `is` | `a` | `movi` | `about` | `cat` | `.` | `cat` | `appear` | `in` | `everi` | `scene` | `.` | `a` | `cat` | `can` | `alway` | `be` | `seen` | \n\n\nJust to focus on how the stemmers operate over a specific paradigm:\n\n\n\n```{.python .cell-code}\ncry = [\"cry\", \"cries\", \"crying\", \"cried\", \"crier\"]\n\nprint(\n  tabulate(\n    [[c, s_stemmer.stem(c)] for c in cry],\n    headers=[\"token\", \"stem\"]\n  )\n)\n```\n\ntoken    stem\n-------  ------\ncry      cri\ncries    cri\ncrying   cri\ncried    cri\ncrier    crier\n\n\nAlso, when something like inflectional morphology makes a change to the stem, it won't get undone by the stemmer.\n\n\n\n```{.python .cell-code}\nrun = [\"run\", \"runs\", \"running\", \"ran\", \"runner\"]\n\nprint(\n  tabulate(\n    [[r, s_stemmer.stem(r)] for r in run],\n    headers=[\"token\", \"stem\"]\n  )\n)\n```\n\ntoken    stem\n-------  ------\nrun      run\nruns     run\nrunning  run\nran      ran\nrunner   runner\n\n\n## Lemmatizing\n\nLemmatizing involves a more complex morphological analysis of words, and as such requires language specific models to work.\n\n### nltk lemmatizing\n\nnltk uses [WordNet](https://wordnet.princeton.edu) for its English lemmatizing. WordNet is a large database of lexical relations that have been hand annotated starting in the 1980s. Its outputs are always recognizable words.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwnl = nltk.WordNetLemmatizer()\n```\n:::\n\n\n```{.python .cell-code}\nprint(\n  tabulate(\n    [[c, wnl.lemmatize(c)] for c in cry],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\ntoken    lemma\n-------  -------\ncry      cry\ncries    cry\ncrying   cry\ncried    cried\ncrier    crier\n\n\n```{.python .cell-code}\nprint(\n  tabulate(\n    [[r, wnl.lemmatize(r)] for r in run],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\ntoken    lemma\n-------  -------\nrun      run\nruns     run\nrunning  running\nran      ran\nrunner   runner\n\n\n### spaCy lemmatizing\n\nspaCy has a number of models that do lemmatizing. They list WordNet along with a few other data sources for the model.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\n```\n:::\n\n\n```{.python .cell-code}\ndoc = nlp(\" \".join(cry))\nprint(\n  tabulate(\n    [[c.text, c.lemma_] for c in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\ntoken    lemma\n-------  -------\ncry      cry\ncries    cry\ncrying   cry\ncried    cry\ncrier    crier\n\n\n```{.python .cell-code}\ndoc = nlp(\" \".join(run))\nprint(\n  tabulate(\n    [[r.text, r.lemma_] for r in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\ntoken    lemma\n-------  -------\nrun      run\nruns     run\nrunning  run\nran      run\nrunner   runner\n\n\n## The use of lemmatizing and stemming\n\nFor a lot of the NLP tasks we're going to be learning about, lemmatizing and stemming don't factor in as part of the pre-processing pipeline. However, they're useful tools to have handy when doing linguistic analyses. For example, for all of the importance of \"word frequency\" in linguistics literature, there's often not much clarity about how the text was pre-processed to get these word frequencies.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}