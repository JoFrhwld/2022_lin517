{
  "hash": "914954e58dd15cf9af2cec11a615f50e",
  "result": {
    "markdown": "---\ntitle: \"Lemmatizing and Stemming\"\neditor: visual\nauthor:\n  - name: Josef Fruehwald\n    url: https://jofrhwld.github.io/\nknitr: \n  opts_chunk: \n    echo: true\ndate: \"2022-9-13\"\n---\n\n::: {.cell}\n\n:::\n\n\n## What tokenizing does *not* get for you\n\nComing back to the example sentences from [the first data processing](../data_processing/index.qmd) lecture, properly tokenizing these sentences will only partly help us with our linguistic analysis.\n\n\n\n```{.python .cell-code}\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nfrom tabulate import tabulate\n\nphrase = \"\"\"The 2019 film CATS is a movie about cats. \nCats appear in every scene. \nA cat can always be seen\"\"\"\n\n# case folding\nphrase_lower = phrase.lower()\n\n# tokenization\ntokens = word_tokenize(phrase_lower)\n\n# counting\ntoken_count = Counter(tokens)\n\n# cat focus\ncat_list = [[k,token_count[k]] for k in token_count if \"cat\" in k]\n\nprint(tabulate(cat_list,\n               headers = [\"type\", \"count\"]))\n```\n\ntype      count\n------  -------\ncats          3\ncat           1\n\n\nWe've still got the plural `cats` being counted as a separate word from `cat`, which for our weird use case, we don't want. Our options here are to either \"stem\" or \"lemmatize\" our tokens.\n\n## Stemming\n\nStemming is focused on cutting off morphemes and, to some degree, providing a consistent stem across all types that share a stem. So the outcomes aren't always a recognizable word,\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import SnowballStemmer\n```\n:::\n\n\n```{.python .cell-code}\np_stemmer = PorterStemmer()\np_stemmed = [p_stemmer.stem(t) for t in tokens]\nfor t in p_stemmed:\n  print(f\"`{t}` |\", end = \" \")\n```\n\n`the` | `2019` | `film` | `cat` | `is` | `a` | `movi` | `about` | `cat` | `.` | `cat` | `appear` | `in` | `everi` | `scene` | `.` | `a` | `cat` | `can` | `alway` | `be` | `seen` | \n\n\n```{.python .cell-code}\ns_stemmer = SnowballStemmer(\"english\")\ns_stemmed = [s_stemmer.stem(t) for t in tokens]\nfor t in s_stemmed:\n  print(f\"`{t}` |\", end = \" \")\n```\n\n`the` | `2019` | `film` | `cat` | `is` | `a` | `movi` | `about` | `cat` | `.` | `cat` | `appear` | `in` | `everi` | `scene` | `.` | `a` | `cat` | `can` | `alway` | `be` | `seen` | \n\n\n```{.python .cell-code}\ncry = [\"cry\", \"cries\", \"crying\", \"cried\", \"crier\"]\n\nprint(\n  tabulate(\n    [[c, s_stemmer.stem(c)] for c in cry],\n    headers=[\"token\", \"stem\"]\n  )\n)\n```\n\ntoken    stem\n-------  ------\ncry      cri\ncries    cri\ncrying   cri\ncried    cri\ncrier    crier\n\n\nAlso, when something like inflectional morphology makes a change to the stem, it won't get undone by the stemmer.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrun = [\"run\", \"runs\", \"running\", \"ran\", \"runner\"]\n\nprint(\n  tabulate(\n    [[r, s_stemmer.stem(r)] for r in run],\n    headers=[\"token\", \"stem\"]\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntoken    stem\n-------  ------\nrun      run\nruns     run\nrunning  run\nran      ran\nrunner   runner\n```\n:::\n:::\n\n\n## Lemmatizing\n\nLemmatizing involves a more complex morphological analysis of words, and as such requires language specific models to work.\n\n### nltk lemmatizing\n\nnltk uses [WordNet](https://wordnet.princeton.edu) for its English lemmatizing.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwnl = nltk.WordNetLemmatizer()\n```\n:::\n\n\n```{.python .cell-code}\nprint(\n  tabulate(\n    [[c, wnl.lemmatize(c)] for c in cry],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\ntoken    lemma\n-------  -------\ncry      cry\ncries    cry\ncrying   cry\ncried    cried\ncrier    crier\n\n\n```{.python .cell-code}\nprint(\n  tabulate(\n    [[r, wnl.lemmatize(r)] for r in run],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\ntoken    lemma\n-------  -------\nrun      run\nruns     run\nrunning  running\nran      ran\nrunner   runner\n\n\n### spaCy lemmatizing\n\nspaCy has a number of models that do lemmatizing. They list WordNet along with a few other data sources for the model.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\nlemmatizer = nlp.get_pipe(\"lemmatizer\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndoc = nlp(\" \".join(cry))\nprint(\n  tabulate(\n    [[c.text, c.lemma_] for c in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntoken    lemma\n-------  -------\ncry      cry\ncries    cry\ncrying   cry\ncried    cry\ncrier    crier\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndoc = nlp(\" \".join(run))\nprint(\n  tabulate(\n    [[r.text, r.lemma_] for r in doc],\n    headers=[\"token\", \"lemma\"]\n  )\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntoken    lemma\n-------  -------\nrun      run\nruns     run\nrunning  run\nran      run\nrunner   runner\n```\n:::\n:::\n\n\n## The use of lemmatizing and stemming\n\nFor a lot of the NLP tasks we're going to be learning about, lemmatizing and stemming don't factor in as much. However, they're useful tools to have handy when doing linguistic analyses. For example, for all of the importance of \"word frequency\" in linguistics literature, there's often not much clarity about how the text was pre-processed to get these word frequencies.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}