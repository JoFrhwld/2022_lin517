{
  "hash": "f6c51154bf7f80c64525e5909ec520c5",
  "result": {
    "markdown": "---\ntitle: \"Sequence Models\"\nauthor: Josef Fruehwald\ndate: 2022-12-5\ncategories:\n  - \"Neural Networks\"\neditor: visual\n---\n\n## The Models We've seen\n\nThe models we (tried) training on Friday looked something like this:\n\n![](assets/linear.svg){fig-align=\"center\"}\n\nThere could have been more or less layers in between the input and output, and we could have had different activation functions. But essentially, we were trying to classify each verb one at time in isolation.\n\n## What about *sequences*?\n\nFor other kinds of tasks, though, this \"one at a time in isolation\" isn't a great idea! Let's say that we were trying to build a part of speech classifier, where we would assign each word in a sentence to a part of speech. The words that come before and after contain crucial information for that purpose.\n\n-   I went to the **bank**. (should be Noun)\n\n-   You can **bank** on us! (should be verb)\n\nWe could approach this problem a lot like the ngram models we were working with before, and just concatenate together the two word vectors preceding our target word.\n\n![](assets/ngram_nn.svg){fig-align=\"center\"}\n\n## Recurrent Neural Networks\n\nBut what people found to be a little bit more effective was to use \"recurrent neural networks\" or RNNs. An important conceptual addition for RNNs is that we don't have to map our hidden layer to *only* the output layer. If our matrix `V` maps the hidden state `h` out to the prediction layer, we could use another matrix `U` to map our hidden state out as input to the next word. The network processing the second word then has two input sources: the incoming word vector *and* the transformed hidden state from the previous word.\n\n![](assets/rnn.svg){fig-align=\"center\"}\n\nThe idea of what's going on here is a little bit of information about the previous word is being passed on to the next word and affecting its hidden state, which then gets mapped to the prediction layer.\n\nMore technically speaking, order of operations here is:\n\n-   the values in the hidden layer $h_{t-1}$ are being transformed a bit by the matrix $U$, and then passed forward. ($Uh_{t-1}$)\n\n-   the word vector at time $t$, $w_t$, is being transformed by matrix $W$ and fed into the network. ($Ww_{t}$)\n\n-   The output of $Ww_{t}$ is then elementwise added to the output of $Uh_{t-1}$ and passed through an activation layer to make $h_t$. ($h_t = \\sigma(Ww_t + Uh_{t-1})$).\n\n-   Then, $h_t$ gets sent off to the output layer, by being multiplied by $V$ and passed through softmax ($y_t = \\sigma(Vh_t)$)\n\n-   $h_t$, also gets passed onto the next word's network by $U$ ($Uh_t$)\n\nThe matrices of weights which pass word vectors and hidden states into and out of the networks, $W$, $U$ and $V$, are the same throughout the whole process, so it's their weights that get updated by backpropogation.\n\nThe detail of every matrix, vector, and activation function often get elided over in simplified diagrams of RNNs which depict them as a sequence of cells.\n\n![](assets/rnn_lines.svg){fig-align=\"center\"}\n\nThe thing to assume is that every arrow indicates a matrix multiplication followed by an activation function.\n\nThis first RNN diagram represents a \"many to many\" model. In the illustration example, we're trying to classify each word in the sequence with a part of speech, using a little bit of information that came from the word before.\n\nThere are other ways you could wire up an RNN. For example, maybe we want to feed it words from a product review, one by one, and have it make just one prediction of how many stars to give it at the very end. The only tweak we'd make is to not send the hidden state to output until the very last cell.\n\n![](assets/rnn_many_2_one.svg){fig-align=\"center\"}\n\nOr, if wanted a model that would output a new sequence of words after being given an input sequence of words (e.g. summarizing a long article into a short paragraph, or just continuing a story after being given a prompt), you would tweak this architecture to take input word vectors for the first however many nodes, then just generate output vectors for the rest.\n\n![](assets/rnn_summary.svg){fig-align=\"center\"}\n\nOr, if your RNN isn't performing well, you could try stacking one sequence on top of the other.\n\n![](assets/rnn_stacked.svg){fig-align=\"center\"}\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport tensorflow as tf\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmodel = tf.keras.Sequential(\n  [\n    tf.keras.layers.Embedding(input_dim = 1000, output_dim = 100),\n    tf.keras.layers.SimpleRNN(100),\n    tf.keras.layers.Dense(10)\n  ]\n)\nmodel.summary() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel: \"sequential_5\"\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n Layer (type)                Output Shape              Param #   \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n embedding_5 (Embedding)     (None, None, 100)         100000    \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n simple_rnn_5 (SimpleRNN)    (None, 100)               20100     \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n dense_5 (Dense)             (None, 10)                1010      \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                 \n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=================================================================\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal params: 121,110\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTrainable params: 121,110\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nNon-trainable params: 0\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n_________________________________________________________________\n```\n:::\n:::\n\n\n### Long Short-Term Memory\n\nOne issue with RNNs is that they have a relatively short \"memory\". The only way\n\n",
    "supporting": [
      "RNN_files"
    ],
    "filters": [],
    "includes": {}
  }
}