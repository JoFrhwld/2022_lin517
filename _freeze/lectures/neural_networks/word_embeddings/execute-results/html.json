{
  "hash": "1112e75d32b679e254e46081b749f572",
  "result": {
    "markdown": "---\ntitle: \"Additional Neural Network Concepts\"\nauthor: \"Josef Fruehwald\"\ndate: 2022-11-27\ncategories:\n  - \"Neural Networks\"\neditor: visual\nbibliography: references.bib\n---\n\n## Word Embeddings\n\nLet's say we were working on a problem with a very restrictive vocabulary with just 3 vocabulary items, and we were also using a very small word vector length of just 5 values. And also, unlike with real cases, the values in each dimension of the vector space was the same within each word.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nembedding_space = np.array([[1]*5, [2]*5, [3] * 5])\n\n# 3 rows = 1 row for each vocabulary item\n# 5 columns = 5 vector dimensions\nembedding_space\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\narray([[1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3]])\n```\n:::\n:::\n\n\nAnd let's say we've already encoded our input data as a sequence of integers.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndata = np.array([1, 1, 0, 0, 2])\n```\n:::\n\n\nTo get the word vector for the first datapoint, we could do matrix indexing.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nembedding_space[data[0],:]\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([2, 2, 2, 2, 2])\n```\n:::\n:::\n\n\nWe can even get the word vectors for all of the datapoints this way.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nembedding_space[data, :]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[2, 2, 2, 2, 2],\n       [2, 2, 2, 2, 2],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1],\n       [3, 3, 3, 3, 3]])\n```\n:::\n:::\n\n\nThis isn't usually how it's done in neural network models, though. One reason is that the `embedding_space` is a matrix of numbers that the model is trying to *learn*, and the learning mechanism (backpropogation) doesn't work right out of the box with indexing this way.\n\nInstead, let's convert our data into one-hot encoding\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import OneHotEncoder\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\none_hot = OneHotEncoder(sparse = False)\ndata_onehot = one_hot.fit_transform(data.reshape(-1,1))\ndata_onehot\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.]])\n```\n:::\n:::\n\n\nNow, to get the word vector for the first data point, we can use matrix multiplication.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# indexing the first row\ndata_onehot[0, :] @ embedding_space\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([2., 2., 2., 2., 2.])\n```\n:::\n:::\n\n\nAnd we can get the word embeddings for the whole dataset the same way.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndata_onehot @ embedding_space\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([[2., 2., 2., 2., 2.],\n       [2., 2., 2., 2., 2.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [3., 3., 3., 3., 3.]])\n```\n:::\n:::\n\n\nTo see why this works, let's quickly refresh on how matrix multiplication works. The first datapoint has the one-hot encoding `[0, 1, 0]`. When we do matrix multiplication, we take the dot product of the first row of matrix A and the first column of matrix B.\n\n    A first row:     [0,  1,  0]\n    B first column:  [1,  2,  3]\n                       multiply\n                     [0,  2,  0]\n                         sum\n                         [2]\n\nThen we move on to the second column of matrix B, etc. So, what the one-hot encoding vector `[0, 1, 0]` winds up doing is pulling out just the second row of a three row matrix. When we do matrix multiplication for the whole dataset we get back the word embeddings for each datapoint.\n\nYou'll often see this whole process presented in figures & diagrams of neural networks something like this:\n\n![](assets/embedding.svg){fig-align=\"center\"}\n\n## Pretraining and fine tuning\n\nIf we were wanting to train a model that classified the valence of words as positive, negative, or neutral, we might set it up like this:\n\n![](assets/pretraining.svg){fig-align=\"center\"}\n\nGlossing over details like layer size and activation functions, the process might look like this:\n\n1.  Convert text labels to one-hot encoding.\n2.  Get the word vectors for each word from the embedding space.\n3.  Feed each word embedding through a two layer neural network.\n4.  Feed the output into a three node output layer.\n\nNow, depending on the vocabulary size and the word vector size, the time spent training this model from scratch (that is, starting off all weights from a random value), *most* of the time this model would spend in training would probably in getting all of the weights just right in the embedding space. That's not necessarily the best use of time and energy.\n\nSo, what's commonly done is to get \"pretrained\" word embeddings. For example, word2vec or GloVe embeddings which were trained on the task of predicting whether words appeared in the context of other words could be re-used. The idea here is that these embeddings already encode some kind of word \"meaning\", and will at least be closer to the optimal values for the word embedding space than a completely random initialization.\n\nThere are then one of two ways you can approach training this sentiment model. One would be just to take the pretrained embeddings as-is, and make it the job of the neural network layers to figure out how to weigh and recombine the inputs to classify the words. This is sometimes called \"freezing\" the word embedding layer.\n\nAlternatively, you could choose not to \"freeze\" the word embeddings, and as the model tries to minimize the loss function by updating the weights between layers, it will also update the weights in the embedding space. This is called \"fine tuning\".\n\nIt seems like most of the pretraining of large embedding spaces (whether it's text, audio, etc) is being carried out by well funded and resourced companies like Google, Meta, OpenAI, etc, and most additional work done by researchers or smaller organizations fine tunes these embeddings.\n\n## \"Feature Engineering\"\n\nTechnically, we've already done a *lot* of feature engineering in the course, we just haven't called it that yet. \"Feature Engineering\" is the process of moving from your raw data (structured or unstructured) to whatever numbers go into your model counts as \"feature engineering.\"\n\n### Tokenizing & One-Hot encoding\n\nFor example, taking plain text and tokenizing it is a feature engineering step. Let's work over Jabberwocky.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\njabberwocky = \"’Twas brillig, and the slithy toves\\\n      Did gyre and gimble in the wabe:\\\nAll mimsy were the borogoves,\\\n      And the mome raths outgrabe.\\\n\\\n“Beware the Jabberwock, my son!\\\n      The jaws that bite, the claws that catch!\\\nBeware the Jubjub bird, and shun\\\n      The frumious Bandersnatch!”\\\n\\\nHe took his vorpal sword in hand;\\\n      Long time the manxome foe he sought—\\\nSo rested he by the Tumtum tree\\\n      And stood awhile in thought.\\\n\\\nAnd, as in uffish thought he stood,\\\n      The Jabberwock, with eyes of flame,\\\nCame whiffling through the tulgey wood,\\\n      And burbled as it came!\\\n\\\nOne, two! One, two! And through and through\\\n      The vorpal blade went snicker-snack!\\\nHe left it dead, and with its head\\\n      He went galumphing back.\\\n\\\n“And hast thou slain the Jabberwock?\\\n      Come to my arms, my beamish boy!\\\nO frabjous day! Callooh! Callay!”\\\n      He chortled in his joy.\\\n\\\n’Twas brillig, and the slithy toves\\\n      Did gyre and gimble in the wabe:\\\nAll mimsy were the borogoves,\\\n      And the mome raths outgrabe.\"\n```\n:::\n\n\nFirst, we need to decide how to tokenize it. Given that most of these words are intentionally novel, something like Byte Pair Encoding would be a good tokenizing approach. I'll use a pre-trained tokenizer from the [`bpemb` package](https://bpemb.h-its.org/).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom bpemb import BPEmb\nbpemb_en = BPEmb(lang=\"en\")\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\njabberwocky_tokens = bpemb_en.encode(jabberwocky)\njabberwocky_tokens[0:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n['▁', '’', 'tw', 'as', '▁br', 'ill', 'ig', ',', '▁and', '▁the']\n```\n:::\n:::\n\n\nDepending on what I'm trying to do with the data, I'll probably want to get an embedding space for for the tokens, which comes with the `bpemb` models.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nembedding_space = bpemb_en.vectors\nembedding_space.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n(10000, 100)\n```\n:::\n:::\n\n\nIt's not completely straightforward to go from the text-based tokens to their embeddings. Instead, I'll create a one-hot encoding of the tokens. This takes two steps. First, get the index ids for each token\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\njabberwocky_ids = bpemb_en.encode_ids(jabberwocky)\njabberwocky_ids[0:10]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n[9912, 9958, 4265, 35, 473, 141, 81, 9934, 34, 7]\n```\n:::\n:::\n\n\nThen, make a one-hot encoding matrix\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# first, a big matrix of all zeros\njabberwocky_onehot = np.zeros(shape = (len(jabberwocky_ids), 10000))\n\n# then, set each data row-by-tokenindex cell to 1\njabberwocky_onehot[range(len(jabberwocky_ids)), jabberwocky_ids] = 1\n```\n:::\n\n\nWe can now use `jabberwocky_onehot` as the input to a model which uses `embedding_space` either as a frozen or a finetunable layer.\n\nThe process of\n\n1.  Choosing a tokenization procedure\n2.  Converting the tokens to a one-hot encoding\n\nIs feature engineering.\n\n### Data normalizing\n\nThe process of data normalizing data is another kind of \"feature engineering\". For example, vowel formant usually has values in terms of Hz.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nimport pandas as pd\nvowels = pd.read_csv(\"data/s03.txt\", sep = \"\\t\")\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nvowels[\"F1\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n0       771.5\n1       639.5\n2       501.2\n3       441.3\n4       425.7\n        ...  \n6447    525.4\n6448    607.6\n6449    428.8\n6450    618.1\n6451    429.3\nName: F1, Length: 6452, dtype: float64\n```\n:::\n:::\n\n\nConverting these Hz values to \"standard\" or \"z-score\" values is a form of feature engineering.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nvowels[\"F1_z\"] = (vowels[\"F1\"] - vowels[\"F1\"].mean())/vowels[\"F1\"].std()\nvowels[\"F1_z\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n0       2.601486\n1       1.450600\n2       0.244786\n3      -0.277472\n4      -0.413486\n          ...   \n6447    0.455782\n6448    1.172470\n6449   -0.386457\n6450    1.264017\n6451   -0.382098\nName: F1_z, Length: 6452, dtype: float64\n```\n:::\n:::\n\n\n### Hand crafted \"synthetic\" features, and other coding\n\nData processing that in other areas of quantitative analysis might be called \"coding\" could be called \"feature engineering.\" For example, @purse2022 recently explored the effect of different word frequency measures on the rate of TD Deletion. One of the measures we used was \"conditional\" frequency. For example, the word *walked* will have some freqyency, $C(\\text{walked})$. But the lemma walk appears in many different words, like walk, walking, walker, walks. So we also got calculated the lemma fequency, which is just the sum of all of these counts: $C_L(\\text{walk}) = C(\\text{walk})+C(\\text{walked}) + C(\\text{walks})+\\ldots$.\n\nWe though that maybe, the \"conditional frequency\", or the frequency with which the lemma *walk* appears as *walked* might also play a role. So $C_c(\\text{walked}) = \\frac{C(\\text{walked})}{C_L(\\text{walk})}$. Coming up with that measure is an example of \"feature engineering\", as is other features we can extract, like the preceeding segment, the length of the word, etc.\n\n",
    "supporting": [
      "word_embeddings_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}