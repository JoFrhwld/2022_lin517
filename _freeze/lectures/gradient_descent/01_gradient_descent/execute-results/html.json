{
  "hash": "37a323148c2e5f7c38cc403fc28f8456",
  "result": {
    "markdown": "---\ntitle: \"Gradient Descent\"\neditor: visual\n---\n\n\n## Getting there by little steps\n\n\n::: {.cell}\n\n:::\n\n\nWhat if wanted to convert inches to centimeters, but didn't know that the formula is inches \\* 2.54? But what we *did* have was the following table of belt sizes from the Gap!\n\n| Waist Size | Belt Length (in) | Belt Length (cm) |\n|-----------:|-----------------:|-----------------:|\n|         28 |             30.5 |               77 |\n|         30 |             32.5 |               83 |\n|         32 |             34.5 |               88 |\n|         34 |             36.5 |               93 |\n|         36 |             38.5 |               98 |\n|         38 |             40.5 |              103 |\n|         40 |             42.5 |              108 |\n|         42 |             44.5 |              113 |\n|         44 |             46.5 |              118 |\n|         46 |             48.5 |              123 |\n\nWhat we could do is *guess* the multiplier, and see how wrong it is.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nbelt_in = np.array([30.5, 32.5, 34.5, 36.5, 38.5, 40.5, 42.5, 44.5, 46.5, 48.5])\nbelt_cm = np.array([77, 83, 88, 93, 98, 103, 108, 113, 118, 123])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmultiplier_guess = 1.5\ncm_guess = belt_in * multiplier_guess\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# If our guess was right, this should all be 0\ncm_guess - belt_cm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([-31.25, -34.25, -36.25, -38.25, -40.25, -42.25, -44.25, -46.25,\n       -48.25, -50.25])\n```\n:::\n:::\n\n\nOur guess wasn't a great guess. With this multiplier, our guesses are all too small. Let's describe how bad our guess was with one number, and call it the \"loss.\" The usual loss function for data like this is the Mean Squared Error.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef mse(actual, guess):\n  \"\"\"\n    Given the actual target outcomes and the outcomes we guessed,\n    calculate the mean squared error.\n  \"\"\"\n  error = actual-guess\n  squared_error = np.power(error, 2)\n  mean_squared_error = np.mean(squared_error)\n  return(mean_squared_error)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmse(belt_cm, cm_guess)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1728.2125\n```\n:::\n:::\n\n\nIf we made our multiplier guess a little closer to what it ought to be, though, our mean squared error, or loss, should get smaller.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmultiplier_guess += 0.2\ncm_guess = belt_in * multiplier_guess\nmse(belt_cm, cm_guess)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1128.2125\n```\n:::\n:::\n\n\nOne thing we could try doing is make a long list of possible multipliers, and try them all to see which one has the smallest loss. This is also known as a \"grid search\". I'll have to re-write the loss function to calculate the loss for specific multipliers\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# This gives us 50 evenly spaced numbers between 0 and 50\npossible_mults = np.linspace(start = 0., stop = 5., num = 50)\n\ndef mse_loss(multiplier, inches, cm):\n  \"\"\"\n    given a multiplier, and a set of traning data,\n    (inches and their equivalent centimeters), return the \n    mean squared error obtained by using the given multiplier\n  \"\"\"\n  cm_guess = inches * multiplier\n  loss = mse(cm_guess, cm)\n  return(loss)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nlosses = np.array([mse_loss(m, belt_in, belt_cm) for m in possible_mults])\n```\n:::\n\n\nIt's probably best to visualize the relationship between the multiplier and the loss in a graph.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_gradient_descent_files/figure-html/unnamed-chunk-11-1.png){width=480}\n:::\n:::\n\n\nIf we get the index of the smallest loss and get the associated multiplier, we can see that we're not too far off!\n\n\n::: {.cell}\n\n```{.python .cell-code}\npossible_mults[losses.argmin()]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.5510204081632653\n```\n:::\n:::\n\n\n### Why not always just do grid search?\n\nOne thing that is going to remain the same no matter how complicated the models get is the measure of how well they've done, or the loss, is going to get boiled down to one number. But in real modelling situations, or neural networks, the number of *parameters* is going to get huge. Here we have only one parameter, but if we had even just 5 parameters, and tried doing a grid search over 50 evenly spaced values of each parameter, the number of possible combinations of parameter values will get intractable.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nf\"{(5 ** 50):,}\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'88,817,841,970,012,523,233,890,533,447,265,625'\n```\n:::\n:::\n\n\n### Without seeing the whole map, we can tell which way is the right direction.\n\nLet's look at the plot of our parameter vs the loss again:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_gradient_descent_files/figure-html/unnamed-chunk-14-1.png){width=480}\n:::\n:::\n\n\nThere are a few really important features of this loss function:\n\n1.  As the estimate gets further away from the ideal value in either direction, the loss increases.\n2.  The increase is \"monotonic\", meaning it's not bumpy or sometime going up, sometimes going down.\n3.  The further away the guess gets from the optimal value, the *steeper* the \"walls\" of the curve get.\n\nLet's say we were just these two point here, and we couldn't \"see\" the whole curve, but we knew features 1 through 3 were true. With that in hand, and information about how the loss function is calculated, we *can* get the *slope* of the function at each point (indicated by the arrows).\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_gradient_descent_files/figure-html/unnamed-chunk-15-1.png){width=480}\n:::\n:::\n\n\nIf we were able to to update our parameter in a way that is proportional to the slope of the loss, then we would gradually get closer and closer to the optimal value. The updates would be very large at first, while the parameter values are far away from the optimal value, and then would start updating by smaller and smaller amounts as we home in on the optimal value because the slopes get shallower and shallower the closer we get.\n\nThe slope of the loss function at any given point is the **gradient**, and this process of gradually descending downwards is called **gradient descent**.\n\n## Gradient Descent\n\n\"But Joe!\" you exclaim, \"How *do* you calculate the slope of the loss for a single point without seeing the whole distribution?\"\n\nThe answer to that question used to be \"with calculus.\" But nowadays, people do it with \"autograd\" or \"autodiff\", which basically means \"we let the computer figure it out.\" There isn't autograd functionality in numpy, but there is in a closely related library called [Jax, which is being developed by Google](https://jax.readthedocs.io/en/latest/#). Jax has a module called `numpy` which is designed to operate exactly the same way as `numpy`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax.numpy as jnp\nfrom jax import grad\n```\n:::\n\n\nI'm going to rewrite the inches to centimeter functions over again, this time making sure to use jax functions to ensure everything runs smoothly.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef inch_to_cm_jax(multiplier, inches):\n  \"\"\"\n    a function that converts inches to cm\n  \"\"\"\n  cm = jnp.dot(inches, multiplier)\n  return(cm)\n\ndef cm_loss_jax(multiplier, inches, cm):\n  \"\"\"\n    estimate the mismatch between the\n  \"\"\"\n  est = inch_to_cm_jax(multiplier, inches)\n  diff = est - cm\n  sq_err = jnp.power(diff, 2)\n  mean_sq_err = jnp.mean(sq_err)\n  return(mean_sq_err)\n\n```\n:::\n\n\nThen we pass the new loss function to a jax function called `grad()` to create a new gradient function.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncm_loss_grad_jax = grad(cm_loss_jax, argnums=0)\n```\n:::\n\n\nWhere `cm_loss_jax()` will give use the mean-squared error for a specific multiplier, `cm_loss_grad_jax()` will give us the *slope* for that multiplier, automatically.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(multiplier_guess)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.7\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# This is the mean-squared-error\nprint(cm_loss_jax(multiplier_guess, belt_in, belt_cm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1128.2124\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# This is the slope\nprint(cm_loss_grad_jax(multiplier_guess, belt_in, belt_cm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-2681.3499\n```\n:::\n:::\n\n\n## Learning Rates and \"Epochs\"\n\nNow we can write a for-loop to iteratively update out multiplier guess, changing it just a little bit proportional to the gradient. There are two \"hyper parameters\" we need to choose here.\n\n1.  The \"learning rate\". We can't go adding the gradient *itself* to the multiplier. The gradient right now is in the thousands, and we're trying to nudge 1.7 to 2.54. So, we pick a \"learning rate\", which is just a very small decimal to multiply the gradient by before we add it to the parameter. I'll say let's start at 1/100,000\n2.  The number of \"epochs.\" We need to decide how many for loops we're going to go through before we decide to call it and check on how the learning has gone. I'll say let's go for 1000.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nlearning_rate = 1/100_000\nepochs = 1000\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# I want to be able to plot everything after, so I'm going to create collectors.\nepoch_list    = []\nparam_list    = []\nloss_list     = []\ngradient_list = []\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nmultiplier_guess = 0.\nfor i in range(epochs):\n  # append the current epoch\n  epoch_list.append(i)\n  # append the current guess\n  param_list.append(multiplier_guess)\n  \n  loss = cm_loss_jax(multiplier_guess, belt_in, belt_cm)\n  loss_list.append(loss)\n  gradient = cm_loss_grad_jax(multiplier_guess, belt_in, belt_cm)\n  gradient_list.append(gradient)\n  \n  multiplier_guess += -(gradient * learning_rate)\n\nprint(f\"The final guess was {multiplier_guess:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe final guess was 2.541\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](01_gradient_descent_files/figure-html/unnamed-chunk-27-1.png){width=768}\n:::\n:::\n\n\n## This will all work with more parameters\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\npenguins = load_penguins()\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nbill_length = np.array(penguins.dropna()[\"bill_length_mm\"])\nbill_depth = np.array(penguins.dropna()[\"bill_depth_mm\"])\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nX = np.stack([np.ones(bill_length.size), bill_length], axis = 1)\nX[0:10, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[ 1. , 39.1],\n       [ 1. , 39.5],\n       [ 1. , 40.3],\n       [ 1. , 36.7],\n       [ 1. , 39.3],\n       [ 1. , 38.9],\n       [ 1. , 39.2],\n       [ 1. , 41.1],\n       [ 1. , 38.6],\n       [ 1. , 34.6]])\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparam_guess = np.array([1., 3.])\ndepth_guess = np.dot(X, param_guess)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.mean(np.power(bill_depth - depth_guess, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n13699.713063063064\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ndef fit_depth(params, X):\n  \"\"\"\n    Given some values and parameters\n    guess the outcome \n  \"\"\"\n  est = jnp.dot(X, params)\n  return(est)\n\ndef fit_loss(params, X, actual):\n  \"\"\"\n    Return the loss of the params\n  \"\"\"\n  est = fit_depth(params, X)\n  err = est - actual\n  sq_err = jnp.power(err, 2)\n  mse = jnp.mean(sq_err)\n  return(mse)\n\nfit_grad = grad(fit_loss, argnums=0)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfit_loss(param_guess, X, bill_depth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDeviceArray(13699.714, dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfit_grad(param_guess, X, bill_depth)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDeviceArray([  231.62701, 10373.727  ], dtype=float32)\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\n# I want to be able to plot everything after, so I'm going to create collectors.\nepoch_list    = []\nparam_list    = []\nloss_list     = []\ngradient_list = []\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparam_guess = np.array([18., 0.])\nlearning_rate = 0.00001\nvt = np.array([0., 0.])\nfor i in range(10):\n  # append the current epoch\n  epoch_list.append(i)\n  # append the current guess\n  for j in range(bill_depth.size):\n    param_list.append(param_guess)\n    loss = fit_loss(param_guess, X[j,:], bill_depth[j])\n    loss_list.append(loss)\n    gradient = fit_grad(param_guess, X, bill_depth[j])\n    gradient_list.append(gradient)\n  \n    param_guess += -(gradient*learning_rate)\n\nprint(f\"Final param guess was {param_guess}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal param guess was [1.799911e+01 4.096483e-03]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nparam_arr = np.array(param_list)\ngradient_arr = np.array(gradient_list)\n```\n:::\n",
    "supporting": [
      "01_gradient_descent_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}