{
  "hash": "49bc682e562f7c6a4c45c14f6f40d763",
  "result": {
    "markdown": "---\ntitle: \"ngram - Smoothing\"\neditor: visual\nauthor: Josef Fruehwald\ndate: 2022-10-11\ndescription: \"How we work around the problems of data sparsity\"\n---\n\n## Perplexity Review\n\nThe notes on [Perplexity](index.qmd), describe how we can get a measure of how well a given n-gram model predicts strings in a test set of data. Roughly speaking:\n\n-   The better the model gets, the higher a probability it will assign to each $P(w_i|w_{i-1})$ .\n\n-   The higher the probabilities, the lower the perplexities.\n\n-   The lower the perplexities, the better the model\n\nAs a quick demonstration, I've written some code here in collapsible sections to build a bigram model of Frankenstein, and to get the conditional probabilities for every bigram in an input sentence.\n\n::: {.cell filename='python' execution_count=1}\n``` {.python .cell-code}\nimport nltk\nfrom collections import Counter\nimport gutenbergpy.textget\nfrom tabulate import tabulate\nimport numpy as np\n```\n:::\n\n\n::: {.cell filename='python' execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"`getbook()` function\"}\ndef getbook(book, outfile):\n  \"\"\"\n  Download a book from project Gutenberg and save it \n  to the specified outfile\n  \"\"\"\n  print(f\"Downloading Project Gutenberg ID {book}\")\n  raw_book = gutenbergpy.textget.get_text_by_id(book)\n  clean_book = gutenbergpy.textget.strip_headers(raw_book)\n  if not outfile:\n    outfile = f'{book}.txt'\n    print(f\"Saving book as {outfile}\")\n  with open(outfile, 'wb') as file:\n    file.write(clean_book)\n    file.close()\n```\n:::\n\n\n::: {.cell filename='python' execution_count=3}\n``` {.python .cell-code}\ngetbook(book = 84, outfile = \"gen/frankenstein.txt\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDownloading Project Gutenberg ID 84\n```\n:::\n:::\n\n\n::: {.cell filename='python' execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"From a file string to ngrams\"}\ndef ngramize(filename, n = 2):\n  \"\"\"\n    given a file name, generate the ngrams and n-1 grams\n  \"\"\"\n  with open(filename, 'r') as f:\n    lines = f.read()\n    \n  sentences = nltk.sent_tokenize(lines)\n  sentences = [sent.strip().replace(\"\\n\", \" \") \n                      for sent in sentences]\n                      \n  sentences_tok = [nltk.word_tokenize(sent) \n                      for sent in sentences]\n                      \n  sentences_padn = [list(nltk.lm.preprocessing.pad_both_ends(sent, n = n)) \n                      for sent in sentences_tok]\n                      \n  sentences_ngram = [list(nltk.ngrams(sent, n = n)) \n                      for sent in sentences_padn]\n  sentences_ngram_minus = [list(nltk.ngrams(sent, n = n-1)) \n                      for sent in sentences_padn]                      \n  \n  flat_ngram = sum(sentences_ngram, [])\n  flat_ngram_minus = sum(sentences_ngram_minus, [])  \n                      \n  return(flat_ngram, flat_ngram_minus)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Getting bigrams and unigrams from frankenstein\"}\nbigram, unigram = ngramize(\"gen/frankenstein.txt\", n = 2)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Getting counts of bigrams and unigrams\"}\nbigram_count = Counter(bigram)\nunigram_count = Counter(unigram)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"A function to get the conditional probability of a bigram\"}\ndef get_conditional_prob(x, bigram_count, unigram_count):\n  \"\"\"\n    for a tuple x, get the conditional probability of x[1] | x[0]\n  \"\"\"\n  if x in bigram_count:\n    cond = bigram_count[x] / unigram_count[x[0:-1]]\n  else:\n    cond = 0\n    \n  return(cond)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=8}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"A function to get the conditional probability of every ngram in a sentence\"}\ndef get_sentence_probs(sentence, bigram_count, unigram_count, n = 2):\n  \"\"\"\n    given a sentence, get its list of conditional probabilities\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  sent_conditionals = [get_conditional_prob(gram, bigram_count, unigram_count) \n                        for gram in sent_ngram]\n  return(sent_conditionals)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Given a sentence, get the conditional probability expression, for printing.\"}\ndef get_conditional_strings(sentence, n = 2):\n  \"\"\"\n    given a sentence, return the string of conditionals\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens, n = n)\n  sent_pad = [x.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\") for x in sent_pad]\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  out_cond = [f\"P({x[-1]} | {' '.join(x[0:-1])})\" for x in sent_ngram]\n  return(out_cond)\n```\n:::\n\n\nHaving built the bigram model with the code above, we can take this sample sentence:\n\n> I saw the old man.\n\nWe can calculate the conditional probability of every word in the sentence given the word before, as well as [the surprisal for each word](01-ngram-eval.qmd#from-probability-to-bits-a.k.a.-surprisal).[^1]\n\n[^1]: $-\\log_2(p)$\n\n::: {.cell filename='python' execution_count=10}\n``` {.python .cell-code}\nsentence = \"I saw the old man.\"\ncond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_strings(sentence, n = 2)\n```\n:::\n\n\nconditional          probability    surprisal\n-----------------  -------------  -----------\nP(I | &lt;s&gt;)          0.1876       2.4139\nP(saw | I)                0.0162       5.9476\nP(the | saw)              0.2340       2.0952\nP(old | the)              0.0064       7.2865\nP(man | old)              0.6800       0.5564\nP(. | man)                0.1364       2.8745\nP(&lt;/s&gt; | .)         0.9993       0.0011\n\n\nSumming up the surprisal column, we get the total surprisal of the sentence (about 21 bits). We can then get the number of bits per word (about 3) which gives us our ngram perplexity for the sentence (about 8).\n\n  total surprisal    surprisal/word    perplexity\n-----------------  ----------------  ------------\n          21.1752            3.0250        8.1400\n\n\n### A familiar problem approaches\n\nBut, not everything is so neat and tidy. Let's try this again for the sentence\n\n> I saw the same man.\n\n::: {.cell filename='python' execution_count=13}\n``` {.python .cell-code}\nsentence = \"I saw the same man.\"\ncond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_strings(sentence, n = 2)\n```\n:::\n\n\nconditional          probability            surprisal\n-----------------  -------------  -------------------\nP(I | &lt;s&gt;)          0.1876               2.4139\nP(saw | I)                0.0162               5.9476\nP(the | saw)              0.2340               2.0952\nP(same | the)             0.0154               6.0235\nP(man | same)             0.0000  {{< fa infinity >}}\nP(. | man)                0.1364               2.8745\nP(&lt;/s&gt; | .)         0.9993               0.0011\n\n\ntotal surprisal      surprisal/word       perplexity\n-------------------  -------------------  --------------------\n{{< fa infinity >}}  {{< fa infinity >}}  {{< fa infinity >}}!\n\n\nIt looks like the bigram `(\"same\", \"man\")` just didn't appear in the novel. This is zero percolates up through all of our calculations.\n\n$$\nC(\\text{same man}) = 0\n$$\n\n$$\nP(\\text{same man}) = \\frac{C(\\text{same man)}}{N} = \\frac{0}{N} = 0\n$$\n\n$$\nP(\\text{man}~|~\\text{same}) = \\frac{P(\\text{same man)}}{P(\\text{same)}} = \\frac{0}{P(\\text{same})} = 0\n$$\n\n$$\ns(\\text{man}~|~\\text{same}) = -\\log_2(P(\\text{man}~|~\\text{same})) = -\\log_2(0) = \\infty\n$$\n\n$$\npp(\\text{I saw the same man.)} = \\frac{\\sum_{i=1}^Ns(w_i|w_{i-1})}{N} = \\frac{\\dots+\\infty+\\dots}{N} = \\infty\n$$\n\nIn other words, our bigram model's \"mind\" is **completely** blown by a sentence with the sequence `same man` in it.\n\n::: {#fig-mindblow}\n[<iframe src=\"https://giphy.com/embed/lXu72d4iKwqek\" width=\"100%\" height=\"100%\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>]{fig-align=\"center\"}\n\nOur our ngram model, upon seeing `same man`\n:::\n\nThis is, of course [data sparsity](../data_sparsity/data_sparsity.qmd) rearing its head again. On the one hand, we *are* building an n-gram model out of a fairly small corpus. But on the other, the data sparsity problem will never go away, and we are always going to be left with the following two issues:\n\n-   Out Of Vocabulary items\n\n-   Missing ngrams of words that *were* in the vocabulary.\n\n## OOV - Out of Vocabulary\n\n\"Out Of Vocabulary\", commonly referred to OOV, problems, are going to come up if you ever do any computational work with language of any variety.\n\n::: callout-note\n## OOV Example\n\nA lot of phoneticians today use \"forced alignment\", which tries to time align words and phones to audio. Step one of the process is taking a transcription, tokenizing it, then looking up each token in a pre-specified pronunciation dictionary. A commonly used pronunciation dictionary is the CMU pronunciation dictionary. Here's what a few entries of it around `Frankenstein` look like\n\n    ...\n    FRANKENFOOD  F R AE1 NG K AH0 N F UW2 D\n    FRANKENHEIMER  F R AE1 NG K AH0 N HH AY2 M ER0\n    FRANKENSTEIN  F R AE1 NG K AH0 N S T AY2 N\n    FRANKENSTEIN(1)  F R AE1 NG K AH0 N S T IY2 N\n    FRANKENSTEIN'S  F R AE1 NG K AH0 N S T AY2 N Z\n    FRANKENSTEIN'S(1)  F R AE1 NG K AH0 N S T IY2 N Z\n    FRANKFORT  F R AE1 NG K F ER0 T\n    FRANKFORT'S  F R AE1 NG K F ER0 T S\n    FRANKFURT  F R AE1 NG K F ER0 T\n    ...\n\nLet's say I tokenized this sentence and looked up each word\n\n> I ate the po'boy.\n\nWe'd wind up with this:\n\n    I  AY1\n    ATE  EY1 T\n    THE  DH AH0\n    <UNK>  <UNK>\n\nWe're getting `<UNK>` for \"po'boy\" because it's not in the CMU dictionary. It's an Out Of Vocabulary, or OOV word.\n:::\n\nOur example of perplexity blowing up was due to a specific bigram, `('same', 'man')` not appearing in the corpus, even though each individual word does appear. The same thing will happen if any individual word in a sentence is oov.\n\n::: {.cell filename='python' execution_count=16}\n``` {.python .cell-code}\n# literally blowing the mind of a victorian child eating a cool ranch dorito\nsentence = \"I ate a cool ranch Dorito.\"\ncond_probs = get_sentence_probs(sentence, bigram_count, unigram_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_strings(sentence, n = 2)\n```\n:::\n\n\nconditional          probability            surprisal\n-----------------  -------------  -------------------\nP(I | &lt;s&gt;)          0.1876               2.4139\nP(ate | I)                0.0007              10.4712\nP(a | ate)                0.2500               2.0000\nP(cool | a)               0.0000  {{< fa infinity >}}\nP(ranch | cool)           0.0000  {{< fa infinity >}}\nP(Dorito | ranch)         0.0000  {{< fa infinity >}}\nP(. | Dorito)             0.0000  {{< fa infinity >}}\nP(&lt;/s&gt; | .)         0.9993               0.0011\n\n\n### Solutions?\n\nOne approach SLP suggests is to convert every vocabulary item that occurs below a certain frequency to `<UNK>`, then re-estimate all of the ngram values. Here, I'm\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Getting a list of unigrams that occurred once\nto_unk = [x for x in unigram_count if unigram_count[x] == 1]\n\n# <UNK> conversion\nunigram_unk = [(\"<UNK>\",) if x in to_unk else x for x in unigram]\nbigram_unk = [(\"<UNK>\", \"<UNK>\") if ((x[0],) in to_unk and (x[1],) in to_unk) else\n              (\"<UNK>\", x[1]) if (x[0],) in to_unk else\n              (x[0], \"<UNK>\") if (x[1],) in to_unk else\n              x for x in bigram ]\n\n# <UNK> count              \nunigram_unk_count = Counter(unigram_unk)\nbigram_unk_count = Counter(bigram_unk)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=19}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"A function to get the conditional probability of every ngram in a sentence\"}\ndef get_sentence_unk_probs(sentence, bigram_count, unigram_count, n = 2):\n  \"\"\"\n    given a sentence, get its list of conditional probabilities\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_tokens_unk = [x if (x,) in unigram_count else \"<UNK>\" for x in sent_tokens]\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens_unk, n = n)\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  sent_conditionals = [get_conditional_prob(gram, bigram_count, unigram_count) \n                        for gram in sent_ngram]\n  return(sent_conditionals)\n```\n:::\n\n\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nsentence = \"I ate a Dorito.\"\ncond_probs = get_sentence_unk_probs(sentence, bigram_unk_count, unigram_unk_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_unk_strings(sentence, unigram_count, n = 2)\n```\n:::\n\n\nconditional           probability    surprisal\n------------------  -------------  -----------\nP(I | &lt;s&gt;)           0.1876       2.4139\nP(ate | I)                 0.0007      10.4712\nP(a | ate)                 0.2500       2.0000\nP(&lt;UNK&gt; | a)         0.1173       3.0912\nP(. | &lt;UNK&gt;)         0.0600       4.0588\nP(&lt;/s&gt; | .)          0.9993       0.0011\n\n\nConverting low frequency words to `<UNK>` means that now when the ngram model meets a word it doesn't know, like `Dorito`, there is still some probability it can assign.\n\n## Real Zeros\n\nThis `<UNK>`ification of the data doesn't solve everything, though. Here's the longer sentence:\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nsentence = \"I ate a cool ranch Dorito.\"\ncond_probs = get_sentence_unk_probs(sentence, bigram_unk_count, unigram_unk_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_unk_strings(sentence, unigram_unk_count, n = 2)\n```\n:::\n\n\nconditional                     probability            surprisal\n----------------------------  -------------  -------------------\nP(I | &lt;s&gt;)              0.1876                      2.4139\nP(ate | I)                    0.0007                     10.4712\nP(a | ate)                    0.2500                      2.0000\nP(cool | a)                   0.0000         {{< fa infinity >}}\nP(&lt;UNK&gt; | cool)         0.0000         {{< fa infinity >}}\nP(&lt;UNK&gt; | &lt;UNK&gt;)  0.0391                      4.6782\nP(. | &lt;UNK&gt;)            0.0600                      4.0588\nP(&lt;/s&gt; | .)             0.9993                      0.0011\n\n\nThe problem here is that there *is* a known word, `cool`, that just happens never to occur in the bigrams `(a, cool)` or `(cool, <UNK>)`. Maybe what we want is some way of assigning a small probability, of bigrams that could have happened, but didn't.\n\n### Add 1 smoothing (Laplace Smoothing)\n\nThe first, simple idea, is to make a grid of all possible bigrams, and add 1 to all of their counts.\n\n::: {.cell filename='python' execution_count=25}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"A function to get the add 1 smoothed conditional probability of a bigram\"}\ndef get_conditional_prob_add1(x, bigram_count, unigram_count):\n  \"\"\"\n    for a tuple x, get the conditional probability of x[1] | x[0]\n  \"\"\"\n  if x in bigram_count:\n    cond = (bigram_count[x]+1) / (unigram_count[x[0:-1]] + len(unigram_count))\n  else:\n    cond = 1/ (unigram_count[x[0:-1]] + len(unigram_count))\n    \n  return(cond)\n```\n:::\n\n\n::: {.cell filename='python' execution_count=26}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"A function to get the conditional probability of every ngram in a sentence\"}\ndef get_sentence_unk_probs_add1(sentence, bigram_count, unigram_count, n = 2):\n  \"\"\"\n    given a sentence, get its list of conditional probabilities\n  \"\"\"\n  sent_tokens = nltk.word_tokenize(sentence)\n  sent_tokens_unk = [x if (x,) in unigram_count else \"<UNK>\" for x in sent_tokens]\n  sent_pad = nltk.lm.preprocessing.pad_both_ends(sent_tokens_unk, n = n)\n  sent_ngram = nltk.ngrams(sent_pad, n = n)\n  sent_conditionals = [get_conditional_prob_add1(gram, bigram_count, unigram_count) \n                        for gram in sent_ngram]\n  return(sent_conditionals)\n```\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nsentence = \"I ate a cool ranch Dorito.\" \ncond_probs = get_sentence_unk_probs_add1(sentence, bigram_unk_count, unigram_unk_count, n = 2)\ncond_surp = [-np.log2(x) for x in cond_probs]\ncond_strings = get_conditional_unk_strings(sentence, unigram_unk_count, n = 2)\n```\n:::\n\n\nconditional                     probability    surprisal\n----------------------------  -------------  -----------\nP(I | &lt;s&gt;)                     0.0797       3.6498\nP(ate | I)                           0.0004      11.1921\nP(a | ate)                           0.0005      11.0307\nP(cool | a)                          0.0002      12.4299\nP(&lt;UNK&gt; | cool)                0.0002      12.0300\nP(&lt;UNK&gt; | &lt;UNK&gt;)         0.0180       5.7941\nP(. | &lt;UNK&gt;)                   0.0276       5.1784\nP(&lt;/s&gt; | .)                    0.3912       1.3539\n\n\n2 things to notice here:\n\n1.  There are no more zeros!\n2.  The probabilities are all different!\n\nThe probabilities jumped around because by adding 1 to every bigram count, we've given many bigrams a larger portion of the probability pie than they had before, and in a probability space, everything has to sum to 1. So that means we've also *taken away* a portion of the probability space from many bigrams.\\\n\n\n\n\n\n\n\nconditional                     bigram count    w1 count    add 1 prob    implied counts\n----------------------------  --------------  ----------  ------------  ----------------\nP(I | &lt;s&gt;)                         577       3,075        0.0797          244.9828\nP(ate | I)                                 2       2,839        0.0004            1.2134\nP(a | ate)                                 1           4        0.0005            0.0019\nP(cool | a)                                0       1,338        0.0002            0.2425\nP(&lt;UNK&gt; | cool)                      0           2        0.0002            0.0005\nP(&lt;UNK&gt; | &lt;UNK&gt;)             138       3,533        0.0180           63.6700\nP(. | &lt;UNK&gt;)                       212       3,533        0.0276           97.5663\nP(&lt;/s&gt; | .)                      2,686       2,688        0.3912        1,051.6389\n\n\n### Absolute Discounting\n\nThe add 1 method effectively shaved off a little bit of probability from bigrams we *did* see to give it to bigrams we *didn't* see. For example, we had 2 observations of `(I, ate)`, but after redistributing probabilities, we'd effectively shaved off 0.79 observations. Things are even more extreme for other bigrams. Like `(<s>, I)` which got 323 observations shaved off, to redistribute to unseen bigrams.\n\nThe idea behind Absolute Discounting is instead of shaving variable amounts of probability off of every ngram, we instead shave off a *fixed* amount. The Greek letter $\\delta$ is used to indicate this \"shave off\" amount.\n\nOur total number of observed bigrams, after `<UNK>`ifying, 36,744. If we shaved off 0.25 observations off of each bigram, that would give us $36,744\\times0.75=27,558$ observations to spread around to the bigrams we *didn't* observe. If we just did that uniformly, the unobserved bigrams would just get a sliver of that probability mass. There are 4,179 unigrams in our data, meaning we would *expect* there to be $4179^2=17,464,041$ possible bigrams, that means there are $17,464,041-36,744 = 17,427,297$ bigrams trying to get a piece of those 8,936 observations we just shaved off, coming out to just 0.0016 observations each.\n\nSome more clever approaches try not to distribute the probability surplus evenly, though. For example Kneser-Ney smoothing tries to distribute it proportionally to how often the $w_i$ word in a $(w_{i-1}w_i)$ bigram appears as the second word in a bigram.\n\n",
    "supporting": [
      "02_smoothing_files"
    ],
    "filters": [],
    "includes": {}
  }
}