{
  "hash": "2b1ed06d9a88632567406c874b8d08c1",
  "result": {
    "markdown": "---\ntitle: \"ngram Language Models\"\neditor: visual\ndate: \"2022-9-28\"\nauthor:\n  - name: Josef Fruehwald\n    url: https://jofrhwld.github.io/\nknitr: \n  opts_chunk: \n    echo: false\n    message: false\nformat: \n  html: \n    smooth-scroll: true\nbibliography: references.bib\nfreeze: auto\ndescription: \"ngram langauge model notes. The structure of these notes broadly follows SLP V3. \"\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: The request to `use_python(\"/Users/joseffruehwald/Library/CloudStorage/\nOneDrive-UniversityofKentucky/Courses/Lin517/2022_fall/lin517_site/lectures/\nngram/env/bin/python\")` will be ignored because the environment variable\nRETICULATE_PYTHON is set to \"/opt/anaconda3/envs/authoring/bin/python3.10\"\n```\n:::\n:::\n\n\nI'm going to split up the \"ngram model\" materials into explaining how they work in principle, vs the how we have to make engineering decisions to make them work in reality.\n\n## Language Prediction\n\nWhen we are perceiving language, we are constantly and in real-time making predictions about what we are about to hear next. While we're going to be talking about this in terms of predicting the next word, It's been shown that we do this even partway through a word [@allopenna1998].\n\nSo, let's say I spoke this much of a sentence to you:\n\n> I could tell he was angry from the tone of his\\_\\_\\_\n\nAnd then a sudden noise obscured the final word, and you only caught part of it. Which of the following three words was I *probably* trying to say?\n\na.  boys\nb.  choice\nc.  voice\n\nYour ability to guess which word it was is based on your i) experience with English turns of phrase and ii) the information in the context.\n\nOne goal of Language Models is to assign probabilities across the vocabulary for what the next word will be, and hopefully assign higher probabilities to the \"correct\" answer than the \"incorrect\" answer. Applications for this kind of prediction range from speech-to-text (which could suffer from a very similar circumstance as the fictional one above) to autocomplete or spellcheck.\n\n## Using context (ngrams)\n\nIn the example sentence above, one way we could go about trying to predict which word is most likely is to count up how many times the phrase \"I could tell he was angry from the tone of his\\_\\_\\_\" is finished by the candidate words. Here's a table of google hits for the three possible phrases, as well as all hits for just the context phrase.\n\n|   \"I could tell he was angry from the tone of his\" | count |\n|---------------------------------------------------:|------:|\n|                                               boys |     0 |\n|                                             choice |     0 |\n|                                              voice |     3 |\n| *\"I could tell he was angry from the tone of his\"* |     3 |\n\nWe're going to start diving into mathematical formulas now (fortunately the numbers are easy right now).\n\nTo represent the count of a word or string of words in a corpus. We'll use $C(\\text{word})$. So given the table above we have\n\n$$\n\\displaylines{C(\\text{I could tell he was angry from the tone of his})=3\\\\\nC(\\text{I could tell he was angry from the tone of his boys})=0\\\\\nC(\\text{I could tell he was angry from the tone of his choice})=0\\\\\nC(\\text{I could tell he was angry from the tone of his voice})=3}\n$$\n\nTo describe the probability that the next word is \"choice\" given that we've already heard \"I could tell he was angry from the tone of his\", we'll use the notation $P(\\text{choice} | \\text{I could tell he was angry from the tone of his})$. To *calculate* that probability, we'll divide the total count of the whole phrase by the count of the preceding context.\n\n$$\nP(\\text{choice} | \\text{I could tell he was angry from the tone of his}) = \\frac{C(\\text{I could tell he was angry by the tone of his choice})}{C(\\text{I could tell he was angry by the tone of his})} = \\frac{0}{3} = 0\n$$\n\nIn fact, we can estimate the probability of an entire sentence with the *Probability Chain Rule*. The probability of a sequence of events like $P(X_1X_2X_3)$ can be estimated by multiplying out their conditional probabilities like so:\n\n$$\nP(X_1X_2X_3) = P(X_1)P(X_2|X_1)P(X_3|X_1X_2)\n$$\n\nOr, to use a phrase as an example:[^1]\n\n[^1]: Credit here to Kyle Gorman for introducing me to this example.\n\n$$\nP(\\text{du hast mich gefragt})=P(\\text{du})P(\\text{hast}|\\text{du})P(\\text{mich}|\\text{du hast})P(\\text{gefragt}|\\text{du hast mich})\n$$\n\n### Data Sparsity rears its head\n\nThe problem with [data sparsity](../data_sparsity/data_sparsity.qmd) rears its head, though. As we can already see in the table above, long phrases, although *possible*, might not appear in any corpus, giving us a very unreliable probability estimate.\n\nInstead of using the *whole* history, we can use a smaller context in a more strictly defined window. So, instead of looking at the whole sentence, what if we looked at counts of just \"of his\" from the example sentence.\n\n| \"of his\" | count (in millions) |\n|---------:|--------------------:|\n|     boys |                 2.2 |\n|   choice |                14.2 |\n|    voice |                44.5 |\n| \"of his\" |             2,400.0 |\n\n$$\n\\displaylines{\nP(\\text{boys} | \\text{of his}) = \\frac{C(\\text{of his boys)}}{C(\\text{of his})}=\\frac{2.2}{2400} = 0.0009\\\\\nP({\\text{choice}|\\text{of his}})= \\frac{C(\\text{of his choice)}}{C(\\text{of his})}=\\frac{14.2}{2400} = 0.005\\\\\nP({\\text{voice}|\\text{of his}})= \\frac{C(\\text{of his voice)}}{C(\\text{of his})}=\\frac{44.5}{2400} = 0.018}\n$$\n\nThe continuation \"voice\" here is still *relatively* low probability, but has the highest probability of our candidate set.\n\nThis is the basic approach of an ngram model. Instead of using all available words to calculate the probability of the next word, we'll approximate it with a smaller window. The example in the table above is a \"trigram\" model.\n\n::: callout-note\n## \"gram\" names\n\nunigram:\n\n:   Counting up every individual (1) word, and try to estimate the probability of word in isolation.\n\nbigram:\n\n:   Count up every sequence of two words, and try to estimate the probability of a word given just one word before it,\n\ntrigram\n\n:   Count up every sequence of three words, and try to estimate the probability of a word given just the two words before it.\n\n\"Trigrams\" are the last n-gram with a special name. The rest are just called \"4-gram\" or \"5-gram\".\n:::\n\n\n::: {.cell}\n\n:::\n\n\n### Building up a bigram model\n\nLet's look at what happens as we gradually build up a bigram model we'll start with one sentence.\n\n    I saw the dog\n\n\n::: {.cell}\n\n:::\n\n```{dot}\n//| file: figure/1sent.dot\ndigraph {\nrankdir=LR;\n\tdog -> END [penwidth=6, label=1];\n\tI -> saw [penwidth=6, label=1];\n\tsaw -> the [penwidth=6, label=1];\n\tSTART -> I [penwidth=6, label=1];\n\tthe -> dog [penwidth=6, label=1];\n}\n```\n\n\n    I saw the dog\n    We saw a dog\n\n\n::: {.cell}\n\n:::\n\n```{dot}\n//| file: figure/2sent.dot\ndigraph {\nrankdir=LR;\n\ta -> dog [penwidth=6, label=1];\n\tdog -> END [penwidth=6, label=1];\n\tI -> saw [penwidth=6, label=1];\n\tsaw -> a [penwidth=3, label=0.5];\n\tsaw -> the [penwidth=3, label=0.5];\n\tSTART -> I [penwidth=3, label=0.5];\n\tSTART -> We [penwidth=3, label=0.5];\n\tthe -> dog [penwidth=6, label=1];\n\tWe -> saw [penwidth=6, label=1];\n}\n```\n\n\n    I saw the dog\n    We saw a dog\n    I read a book\n\n\n::: {.cell}\n\n:::\n\n```{dot}\n//| file: figure/3sent.dot\ndigraph {\nrankdir=LR;\n\ta -> book [penwidth=3, label=0.5];\n\ta -> dog [penwidth=3, label=0.5];\n\tbook -> END [penwidth=6, label=1];\n\tdog -> END [penwidth=6, label=1];\n\tI -> read [penwidth=3, label=0.5];\n\tI -> saw [penwidth=3, label=0.5];\n\tread -> a [penwidth=6, label=1];\n\tsaw -> a [penwidth=3, label=0.5];\n\tsaw -> the [penwidth=3, label=0.5];\n\tSTART -> I [penwidth=4, label=0.67];\n\tSTART -> We [penwidth=2, label=0.33];\n\tthe -> dog [penwidth=6, label=1];\n\tWe -> saw [penwidth=6, label=1];\n}\n```\n\n\n    I saw the dog\n    We saw a dog\n    I read a book\n    I saw a book\n    I saw a dog\n\n\n::: {.cell}\n\n:::\n\n\n::: {#fig-update .column-page layout-ncol=\"2\"}\n\n```{dot}\n//| fig-responsive: true\n//| file: figure/3sent.dot\n//| fig-caption: \"before update\"\ndigraph {\nrankdir=LR;\n\ta -> book [penwidth=3, label=0.5];\n\ta -> dog [penwidth=3, label=0.5];\n\tbook -> END [penwidth=6, label=1];\n\tdog -> END [penwidth=6, label=1];\n\tI -> read [penwidth=3, label=0.5];\n\tI -> saw [penwidth=3, label=0.5];\n\tread -> a [penwidth=6, label=1];\n\tsaw -> a [penwidth=3, label=0.5];\n\tsaw -> the [penwidth=3, label=0.5];\n\tSTART -> I [penwidth=4, label=0.67];\n\tSTART -> We [penwidth=2, label=0.33];\n\tthe -> dog [penwidth=6, label=1];\n\tWe -> saw [penwidth=6, label=1];\n}\n```\n\n```{dot}\n//| fig-responsive: true\n//| file: figure/5sent.dot\n//| fig-caption: \"After update\"\ndigraph {\nrankdir=LR;\n\ta -> book [penwidth=3, label=0.5];\n\ta -> dog [penwidth=3, label=0.5];\n\tbook -> END [penwidth=6, label=1];\n\tdog -> END [penwidth=6, label=1];\n\tI -> read [penwidth=1.5, label=0.25];\n\tI -> saw [penwidth=4.5, label=0.75];\n\tread -> a [penwidth=6, label=1];\n\tsaw -> a [penwidth=4.5, label=0.75];\n\tsaw -> the [penwidth=1.5, label=0.25];\n\tSTART -> I [penwidth=4.8, label=0.8];\n\tSTART -> We [penwidth=1.2, label=0.2];\n\tthe -> dog [penwidth=6, label=1];\n\tWe -> saw [penwidth=6, label=1];\n}\n```\n\n\nBefore and after update.\n:::\n\n## The probability of a sentence\n\nAnother way to visualize the final state diagram from above is with a matrix, with the \"from\" words along the rows and the \"to\" words along the columns.\n\n\n::: {.cell-output-display}\n|words |a                                 |book                              |dog                               |END                               |I                                 |read                              |saw                               |the                               |We                                |\n|:-----|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|\n|a     |<span style='color:grey'>0</span> |0.5                               |0.5                               |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|book  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|dog   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.25                              |0.75                              |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read  |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw   |0.75                              |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.25                              |<span style='color:grey'>0</span> |\n|START |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.8                               |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.2                               |\n|the   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n:::\n\n\nThere is a non-zero number for every arrow in the state diagram. Every *0* value in the table represents a possible bigram that wasn't observed (so, no arrow in the diagram).\n\nGiven these bigram probabilities we estimated from the corpus *and* our assumption that we can approximate the probability of whole sentences with smaller ngram probabilities, we can estimate the probability of a new sentence like so:\n\n-   We saw the dog.\n\n\n```{=html}\n<style>\n    .bigrampath{\n    }\n    .bg1{\n      color: #440154; \n    }\n    .bg2{\n      color: #3b528b; \n    }\n    .bg3{\n      color: #21918c;\n    }\n    .bg4{\n      color: #5ec962;\n    }\n    .bg5{\n      color: #fde725;\n    }\n    .bigrampath tr:nth-child(7) td:nth-child(10){background : #440154; color: white;}\n    .bigrampath tr:nth-child(9) td:nth-child(8) {background : #3b528b; color: white;}\n    .bigrampath tr:nth-child(6) td:nth-child(9) {background : #21918c; color: white;}\n    .bigrampath tr:nth-child(8) td:nth-child(4) {background : #5ec962; }\n    .bigrampath tr:nth-child(3) td:nth-child(5) {background : #fde725; }\n</style>\n```\n\n[P(We \\| \\<START\\>)]{.bg1} $\\times$ [P(saw \\| We)]{.bg2} $\\times$ [P(the \\| saw)]{.bg3} $\\times$ [P(dog \\| saw)]{.bg4} $\\times$ [P(\\<END\\> \\| dog)]{.bg5}\n\n::: bigrampath\n\n::: {.cell-output-display}\n|words |a                                 |book                              |dog                               |END                               |I                                 |read                              |saw                               |the                               |We                                |\n|:-----|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|\n|a     |<span style='color:grey'>0</span> |0.5                               |0.5                               |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|book  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|dog   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.25                              |0.75                              |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read  |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw   |0.75                              |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.25                              |<span style='color:grey'>0</span> |\n|START |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.8                               |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.2                               |\n|the   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n:::\n\n:::\n\nWe can re-write the probability formula above like so:\n\n$$\nP(s) = \\prod_{i=1}^n P(w_i|w_{i-1})\n$$\n\nWe can also plug in the probabilities of these bigrams into the formula to get our estimated probility of the sentence.\n\n$$\nP(s) = 0.2 \\times 1 \\times 1\\times 0.25 \\times 1 = 0.05\n$$\n\n::: callout-note\n## Log probabilities\n\nOnce you start multiplying probabilities, you're going to get smaller and smaller numbers.\n\n\n::: {.cell}\n::: {.cell-output-display}\n|this                        |   | equals|\n|:---------------------------|:--|------:|\n|0.5                         |=  |  0.500|\n|0.5 × 0.5                   |=  |  0.250|\n|0.5 × 0.5 × 0.5             |=  |  0.125|\n|0.5 × 0.5 × 0.5 × 0.5       |=  |  0.062|\n|0.5 × 0.5 × 0.5 × 0.5 × 0.5 |=  |  0.031|\n:::\n:::\n\n\nEven one very small probability (which you'll get sometimes) can start sending the overall estimate into infintesimally small numbers close to 0, which [computers may not be able to represent](https://en.wikipedia.org/wiki/Arithmetic_underflow).\n\nSo, it's also common to see the log-probability (a.k.a. the log-likelihood, in this case) being calculated instead. The way logarithms work, you add together values that you would have multiplied in the probability space.\n\n$$\n\\log(P(\\text{We saw the dog}))=\\log(P(\\text{We | <START>})) +  \\log(P(\\text{saw | We}))+\\dots\n$$\n\n$$\n\\log(P(s)) = \\sum_{i=1}^n \\log(P(w_i|w_{i-1}))\n$$\n\n$$\n\\log(P(s)) = -1.609438 + 0 + 0 + -1.386294 + 0 = -2.995732\n$$\n:::\n\n### Larger ngrams\n\nLanguage models that take a larger window of adjacent words (3, or 4 grams) work in the same way, and are more \"accurate\" but are harder to visualize.\n\n\n::: {.cell-output-display}\n|prev       |a                                 |book                              |dog                               |END                               |read                              |saw                               |the                               |\n|:----------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|:---------------------------------|\n|a_a        |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_book     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_dog      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_I        |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_read     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_saw      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_the      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|a_We       |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_a        |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_book     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_dog      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_I        |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_read     |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_saw      |0.666666666666667                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.333333333333333                 |\n|I_the      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|I_We       |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_a     |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_book  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_dog   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_I     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_read  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_saw   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_the   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|read_We    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_a      |<span style='color:grey'>0</span> |0.333333333333333                 |0.666666666666667                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_book   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_dog    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_I      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_read   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_saw    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_the    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|saw_We     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_a    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_book |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_dog  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_I    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |0.25                              |0.75                              |<span style='color:grey'>0</span> |\n|START_read |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_saw  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_the  |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|START_We   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |\n|the_a      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_book   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_dog    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_I      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_read   |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_saw    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_the    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|the_We     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_a       |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_book    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_dog     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_I       |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_read    |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_saw     |1                                 |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_the     |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n|We_We      |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |<span style='color:grey'>0</span> |\n:::\n\n\n## Generating text\n\nOnce we've estimated all of these transition probabilities, we can turn them around to *generate* text, if we want. Let's take the final bigram \"model\" we had from before:\n\n\n```{dot}\n//| fig-responsive: true\n//| file: figure/5sent.dot\ndigraph {\nrankdir=LR;\n\ta -> book [penwidth=3, label=0.5];\n\ta -> dog [penwidth=3, label=0.5];\n\tbook -> END [penwidth=6, label=1];\n\tdog -> END [penwidth=6, label=1];\n\tI -> read [penwidth=1.5, label=0.25];\n\tI -> saw [penwidth=4.5, label=0.75];\n\tread -> a [penwidth=6, label=1];\n\tsaw -> a [penwidth=4.5, label=0.75];\n\tsaw -> the [penwidth=1.5, label=0.25];\n\tSTART -> I [penwidth=4.8, label=0.8];\n\tSTART -> We [penwidth=1.2, label=0.2];\n\tthe -> dog [penwidth=6, label=1];\n\tWe -> saw [penwidth=6, label=1];\n}\n```\n\n\nIf we start at `<START>` and roll some dice, there's a 80% chance we'll move to `I` and a 20% chance we'll move to `We`.\n\n-   `<START>`\n\n    -   \n\n        {{< fa dice-d20 >}} = 26\n\n    -   `We`\n\n        -   \n\n            {{< fa dice-d20 >}} = 67\n\n        -   `saw`\n\n            -   \n\n                {{< fa dice-d20 >}} = 67\n\n            -   `a`\n\n                -   \n\n                    {{< fa dice-d20 >}} = 67\n\n                -   `dog`\n\n                    -   \n\n                        {{< fa dice-d20 >}} = 23\n\n                    -   `<END>`\n\nOur bigram model on our boring corpus generates boring results. But here's the output of a tigram model estimated over *Frankenstein*.\n\n\n::: {.cell}\n\n:::\n\n\n<details>\n\n<summary>Details</summary>\n\nThe version of the data I'm working with here hasn't been sentence-ized (so no `<START>` or `<BEGIN>` tags), and has also had punctuation stripped out ([see function here](https://juliasilge.github.io/tidytext/reference/unnest_tokens.html)). So after estimating the trigram probabilities, seed the generator by sampling over all trigrams, then progress by sampling over the distribution of $P(w_i|w_{i-2}w_{i-1})$.\n\n</details>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Trigram R Code\"}\nfrank %>%\n  select(word) %>%\n  mutate(nextw1 = lead(word),\n         nextw2 = lead(word, n = 2)) %>%\n  group_by(word, nextw1) %>%\n  mutate(n_tot = n()) %>%\n  group_by(word, nextw1, nextw2) %>%\n  mutate(n = n(),\n         p = n/n_tot) %>%\n  summarise(p = mean(p),\n            n = n())%>%\n  ungroup()->trigram\n```\n:::\n\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"generator R Code\"}\n#set.seed(517)\ngenerate_frankenstein <- function(trigram, n = 100){\n  trigram %>%\n    mutate(all_p = n/sum(n)) %>%\n    sample_n(size = 1, weight = all_p)->init_df\n  \n  init_vec <- c(init_df$word, init_df$nextw1, init_df$nextw2)\n  for(i in seq(n)){\n    hist = rev(rev(init_vec)[1:2])\n    trigram %>%\n      filter(word == hist[1],\n             nextw1 == hist[2]) %>%\n      sample_n(size = 1, weight = p) %>%\n      pull(nextw2) -> new_w\n      init_vec <- c(init_vec, new_w)\n  }\n  return(str_c(init_vec, collapse = \" \"))\n}\n\ncat(paste0(\"> \", generate_frankenstein(trigram, n = 100)))\n```\n\n> to procure the implements of writing and the world and you will not listen to me the rest of the best means of revenge enkindled in my study day and the world was to enter the cottages allured my appetite acorns and berries afford me respite from thought and feeling but i soon learned that the sailors were persuading to enter on the ground i must travel in a recess of the evil influence the angel of destruction and infallible misery learn from me in an innocent half painful self deceit to call them some time experienced if you were my reflections as\n\n\n## Sparsity, again\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nThe rectangle represents a matrix, with the y-axis representing \"from\" words and the x-axis representing \"to\" words in Frankenstein. There *could* be a point in any location in the rectangle, representing a time that word $w_n$ followed word $w_{n-1}$. Each point represents a cell in that matrix where *any* data was observed.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}