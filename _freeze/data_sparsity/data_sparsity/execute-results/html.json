{
  "hash": "3807c200910aad04a46481171c8a7f37",
  "result": {
    "markdown": "---\ntitle: \"Data Sparsity\"\nformat: \n  html:\n    toc: true\n    toc-location: right\nknitr: \n  opts_chunk: \n    fig-width: 8\n    fig.height: 5\neditor: visual\nauthor: \n  - name: \"Josef Fruehwald\"\n    url: \"https://jofrhwld.github.io/\"\ndate: \"2022-8-31\"\nlicense: CC-BY-SA 4.0\nbibliography: references.bib\nreference-location: margin\n---\n\n\n## Bug Catching\n\nLet's say we're biologists, working in a rain forest, and put out a bug net to survey the biodiversity of the forest. We catch 10 bugs, and each species is a different color:\n\n\\[[{{< fa bug >}}$_1$]{.bug1}, [{{< fa bug >}}$_2$]{.bug1}, [{{< fa bug >}}$_3$]{.bug1}, [{{< fa bug >}}$_4$]{.bug1}, [{{< fa bug >}}$_5$]{.bug1}, [{{< fa bug >}}$_6$]{.bug2}, [{{< fa bug >}}$_7$]{.bug2}, [{{< fa bug >}}$_8$]{.bug3}, [{{< fa bug >}}$_9$]{.bug4}, [{{< fa bug >}}$_{10}$]{.bug5}\\]\n\nWe have 10 bugs in total, so we'll say $N=10$. This is our \"token count.\" We'll use the $i$ subscript to refer to each individual bug (or token).\n\nIf we made a table of each bug species, it would look like:\n\n| species                 | index $j$ | count |\n|-------------------------|-----------|-------|\n| [{{< fa bug >}}]{.bug1} | 1         | 5     |\n| [{{< fa bug >}}]{.bug2} | 2         | 2     |\n| [{{< fa bug >}}]{.bug3} | 3         | 1     |\n| [{{< fa bug >}}]{.bug4} | 4         | 1     |\n| [{{< fa bug >}}]{.bug5} | 5         | 1     |\n\nLet's use $M$ to represent the total number of species, so $M=5$ here. This is our *type* count, and we'll the subscript $j$ to represent the index of specific *types*.\n\nWe can mathematically represent the count of each species like so.\n\n$$ \nc_j = C(\\class{fa fa-bug}{}_j)\n$$\n\nHere, the function $C()$ takes a specific species representation $\\class{fa fa-bug}{}_j$ as input, and returns the specific count $c_j$ for how many times that species showed up in our net. So when $j = {\\color{#785EF0}{1}}$, $\\color{#785EF0}{c_1}=5$, and when $j = {\\color{#FFB000}{4}}$, $\\color{#FFB000}{c_4}=1$.\n\nHere's a plot, with the species id $j$ on the x-axis, and the number of times that species appeared in the net $c_j$ on the y-axis.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n\n### Making Predictions\n\nWhat is the probability that tomorrow, when we put the net out again, that the first bug we catch will be from species [{{< fa bug >}}]{.bug1}? Usually in these cases, we'll use past experience to predict the future. Today, of the $N=10$ bugs we caught, $\\color{#785EF0}{c_1}=5$ of them were species [{{< fa bug >}}]{.bug1}. We can represent this as a fraction like so:\n\n$$\n\\frac{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1, \n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5}\n{{\\color{#785EF0}{\\class{fa fa-bug}{}}}_1, \n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_2,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_3,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_4,\n      {\\color{#785EF0}{\\class{fa fa-bug}{}}}_5,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_6,\n      {\\color{#DC267F}{\\class{fa fa-bug}{}}}_7,\n      {\\color{#FE6100}{\\class{fa fa-bug}{}}}_8,\n      {\\color{#FFB000}{\\class{fa fa-bug}{}}}_9,\n      {\\color{#4C8C05}{\\class{fa fa-bug}{}}}_{10}}\n$$\n\nOr, we can simplify it a little bit. The top part (the numerator) is equal to $\\color{#785EF0}{c_1}=5$, and the bottom part (the denominator) is equal to the total number of bugs, $N$. Simplifying then:\n\n$$\n\\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n$$\n\nWe'll use this as our guesstimate of the probability that the very next bug we catch will be from species [{{< fa bug >}}]{.bug1}. Let's use the function $\\hat{P}()$ to mean \"our method for guessing the probability\", and $\\hat{p}$ to represent the guess we came to. We could express \"our guess that the first bug we catch will be [{{< fa bug >}}]{.bug1}\" like so.\n\n$$\n{\\color{#785EF0}{\\hat{p}_1}} = \\hat{P}({\\color{#785EF0}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#785EF0}{c_1}}{N} = \\frac{5}{10} = 0.5\n$$\n\nWe can then generalize our method to *any* bug like so:\n\n$$\n\\hat{p}_j = \\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}\n$$\n\n### A wild [{{< fa bug >}}]{.bug6} appeared!\n\nLet's say we set out the net again, and the first bug we catch is actually [{{< fa bug >}}]{.bug6}. This is a new species of bug that wasn't in the net the first time. Makes enough sense, the forest is very large. However, what probability *would* we have given catching this new species?\n\nWell, $\\color{#35F448}{c_6} = C({\\color{#35F448}{\\class{fa fa-bug}{}}}) = 0$. So our estimate of the probability would have been ${\\color{#35F448}{\\hat{p}_6}} = \\hat{P}({\\color{#35F448}{\\class{fa fa-bug}{}}}) = \\frac{\\color{#35F448}{c_6}}{N} = \\frac{0}{10} = 0$.\n\nWell obviously, the probability that we would catch a bug from species [{{< fa bug >}}]{.bug6} *wasn't* 0, because events with 0 probability don't happen, and we *did* catch the bug. Admittedly, $N=10$ is a small sample to try and base a probability estimate on, so how large *would* we need the sample to be before we could make probabity estimates for all possible bug species, assuming we stick with the probability estimating function $\\hat{P}(\\class{fa fa-bug}{}_j) = \\frac{c_j}{N}$?\n\n## You'd need {{< fa infinity >}}\n\nThis *kind* of data problem does arise for counting species, but this is really a tortured analogy for language data.[^1] For example, let's take all of the words from Chapter 1 of Mary Shelly's Frankenstein, downloaded from [Project Gutenberg](https://www.gutenberg.org/ebooks/84). I'll count how often each word occurred, and assign it a rank, with 1 being given to the word that occurred the most.\n\n[^1]: For me, I used this analogy to include colorful images of bugs in the lecture notes. For @good1953, they had to use a tortured analogy since the methods for fixing probability estimates were still classified after being used to crack the Nazi Enigma Code in WWII.\n\n\n::: {.cell}\n\n:::\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\nJust to draw the parallels between the two analogies:\n\n| variable | in the analogy                                                | in Frankenstein Chapter 1                                                     |\n|-------------------|------------------------|------------------------------|\n| $N$      | The total number of bugs caught in the net. ($N=10$)          | The total number of words in the first chapter. ($N=1,780$).                  |\n| $x_i$    | An individual bug. e.g. [{{< fa bug >}}]{.bug1}$_1$           | An individual word token. In chapter 1, $x_1$ = \"i\"                           |\n| $w_j$    | A bug species. [{{< fa bug >}}]{.bug1}                        | A word type. The indices are frequency ordered, so for chapter 1 $w_1$ = \"of\" |\n| $c_j$    | The count of how many *individuals* there are of a *species*. | The count of how many *tokens* there are of a *type*.                         |\n\nHere's a table of the top 10 most frequent word types.\n\n\n::: {.cell}\n::: {.cell-output-display}\n|$w_j$ | $c_j$| $j$|\n|:-----|-----:|---:|\n|of    |    75|   1|\n|the   |    75|   2|\n|and   |    70|   3|\n|to    |    61|   4|\n|a     |    52|   5|\n|her   |    52|   6|\n|was   |    40|   7|\n|my    |    33|   8|\n|in    |    32|   9|\n|his   |    29|  10|\n:::\n:::\n\n\nIf we plot out *all* of the word types with the rank ($j$) on the x-axis and the count of each word type ($c_j$) on the y-axis, we get a pattern that if you're not already familiar with it, you will be.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nThis is a \"Zipfian Distribution\" a.k.a. a \"Pareto Distribution\" a.k.a. a \"Power law,\" and it has a few features which make it \\~problematic\\~ for all sorts of analyses.\n\nFor example, let's come back to the issue of predicting the probability of the next word we're going to see. Language Models are \"string prediction models,\" after all, and in order to get a prediction for a specific string, you need to have *seen* the string in the training data. Remember how our bug prediction method had no way of predicting that we'd see a [{{< fa bug >}}]{.bug6} because it had never seen one before?\n\nThere are a lot of possible string types of \"English\" that we have not observed in Chapter 1 of Frankenstein. Good & Turing proposed that you could guesstimate that the probability of seeing a never before seen \"species\" was about equal to the proportion of \"species\" you'd only seen once. With just Chapter 1, that's a pretty high probability that there are words you haven't seen yet.\n\n\n::: {.cell}\n::: {.cell-output-display}\n|seen once? | total| proportion|\n|:----------|-----:|----------:|\n|no         |  1216|      0.683|\n|yes        |   564|      0.317|\n:::\n:::\n\n\nSo, let's increase our sample size. Here's the same plot of rank by count for chapters 1 through 5.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|seen once? | total| proportion|\n|:----------|-----:|----------:|\n|no         |  9928|      0.858|\n|yes        |  1649|      0.142|\n:::\n:::\n\n\nWe increased the size of the whole corpus by a factor of 10, but we've still got a pretty high probability of encountering an unseen word.\n\nLet's expand it out to the whole book now.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n|seen once? | total| proportion|\n|:----------|-----:|----------:|\n|no         | 72122|       0.96|\n|yes        |  3021|       0.04|\n:::\n:::\n\n\n### ðŸŽµ Ain't no corpus large enough ðŸŽµ\n\nAs it turns out, there's no corpus large enough to guarantee observing every possible word at least once, for a few reasons.\n\n1.  The infinite generative capacity of language! The set of all possible words is, *in principle* infinitely large.\n2.  These power law distributions will always have the a *lot* of tokens with a frequency of 1, and even just those tokens are going to have their probabilities poorly estimated.\n\nTo illustrate this, I downloaded the 1-grams of just words beginning with `[Aa]` from the [Google Ngrams data set](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html). This is an ngram dataset based on all of the books scanned by the Google Books project. It's 4 columns wide, 86,618,505 rows long, and 1.8G large, and even then I think it's a truncated version of the data set, because the fewest number of years any given word appears is exactly 40.\n\n\n\n\n\nIf we take just all of the words that start with `[Aa]` published in the year 2000, the most *common* frequency for a word to be is still just 1, even if it is a small proportion of all tokens.\n\n\n::: {.cell tbl-cap='Frequencies of frequencies in words starting with `[Aa]` from the year 2000 in google ngrams'}\n::: {.cell-output-display}\n| word frequency| number of types with frequency| proportion of all tokens|\n|--------------:|------------------------------:|------------------------:|\n|              1|                         205141|                 4.77e-10|\n|              2|                         152142|                 9.55e-10|\n|              3|                         107350|                 1.43e-09|\n|              4|                          80215|                 1.91e-09|\n|              5|                          60634|                 2.39e-09|\n|              6|                          47862|                 2.86e-09|\n:::\n:::\n\n\n### An aside\n\nI'll be plotting the rank vs the frequency with logarithmic axes from here on. Linear axes give equal visual space for every incremental change in the x and y values, while lograrithmic axes put more space between smaller numbers than larger numbers.\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![rank by frequency on linear scales](data_sparsity_files/figure-html/unnamed-chunk-16-1.png){width=480}\n:::\n\n::: {.cell-output-display}\n![rank by frequency on logarithmic scales](data_sparsity_files/figure-html/unnamed-chunk-16-2.png){width=480}\n:::\n:::\n\n\n### It gets worse\n\nWe can maybe get very far with our data sparsity for how often we'll see each individual word by increasing the size of our corpus size, but 1gram word counts are rarely as far as we'll want to go.\n\nTo come back to our bugs example, let's say that bug species [{{< fa bug >}}]{.bug6} actually hunts bug species [{{< fa bug >}}]{.bug4}. If we just caught a [{{< fa bug >}}]{.bug4} in our net, it's a lot more likely that we'll catch a [{{< fa bug >}}]{.bug6} next, coming after the helpless [{{< fa bug >}}]{.bug4} than it would be if we hadn't just caught a [{{< fa bug >}}]{.bug4}. To know what *exactly* the probability catching [{{< fa bug >}}]{.bug4} and then a [{{< fa bug >}}]{.bug6} is, we'd need to count up every 2 bug sequence we've seen.\n\nBringing this back to words, 2 word sequences are called \"bigrams\" and 3 word sequences are called \"trigrams,\" and they are *also* distributed according to a Power Law, and each larger string of words has a worse data sparsity one than the one before. But each larger string of words means more context, which makes for better predictions.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Some Notes on Power Laws\n\nThe power law distribution is pervasive in linguistic data, in almost every domain where we might count how often something happens or is observed. This is absolutely a fact that must be taken into account when we develop our theories or build our models. Some people also think it is an important fact to be explained about language, but I'm deeply skeptical.\n\nA *lot* of things follow power law distributions. The general property of these distributions is that the second most frequent thing will have a frequency about as half as the most frequent thing, the third most frequent thing will have a frequency about a third of the most frequent thing, etc. We could put that mathematically as:\n\n$$\nc_j = \\frac{c_1}{j}\n$$\n\nFor example, here's the log-log plot of baby name rank by baby name frequency in the US between 1880 and 2017.[^2]\n\n[^2]: Data from the `babynames` R package, which in turn got the data from the Social Security Administration.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![rank by frequency of baby names](data_sparsity_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nThe log-log plot isn't perfectly straight (it's common enough for data like this to have two \"regimes\").\n\nHere's the number of ratings each movie on IMDB has received.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nIf we break down the movies by their genre, we get the same kind of result.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nOther things that have been shown to exhibit power law distributions [@newman2005; @jiang2011] are\n\n-   US city populations\n-   number of citations academic papers get\n-   website traffic\n-   number of copies books sell\n-   earthquake magnitudes\n\nThese are all possibly examples of \"preferential attachment\", but we can also create an example that doesn't involve preferential attachment, and still wind up with a power-law. Let's take the first 12 words from Frankenstein:\n\n\n````{=html}\n<table>\n<tr>\n<td><code>\"to\"</code></td><td><code>\"mrs\"</code></td><td><code>\"saville\"</code></td><td><code>\"england\"</code></td><td><code>\"st\"</code></td><td><code>\"petersburgh\"</code></td><td><code>\"dec\"</code></td><td><code>\"11th\"</code></td><td><code>\"17\"</code></td><td><code>\"you\"</code></td><td><code>\"will\"</code></td><td><code>\"rejoice\"</code></td>\n\n</tr>\n</table>\n````\n\nNow, let's paste them all together into one long string with spaces.\n\n````{=html}\n<table>\n<tr>\n<td><code>\"to mrs saville england st petersburgh dec 11th 17 you will rejoice\"</td></code>\n</tr>\n</table>\n````\n\nAnd now, let's choose another arbitrary symbol to split up words besides `\" \"`. I'll go with `e`.\n\n\n````{=html}\n<table>\n<tr>\n<td><code>\"to mrs savill\"</code></td><td><code>\" \"</code></td><td><code>\"ngland st p\"</code></td><td><code>\"t\"</code></td><td><code>\"rsburgh d\"</code></td><td><code>\"c 11th 17 you will r\"</code></td><td><code>\"joic\"</code></td><td><code>\"\"</code></td>\n\n</tr>\n</table>\n````\n\nThe results *aren't* words. They're hardly useful substrings. But, if we do this to the entire novel and plot out the rank and count of thes substrings like they *were* words, we still get a power law distribution.\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\nIn fact, if I take the top 4 most frequent letters, besides spaces, that occur in the text and use them as substring delimiters, the resulting substring distributions are *all* power-law distributed.\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\nThey even have other similar properties often associated with power law distributions in language. For example, it's often been noted that more frequent words tend to be shorter. These weird substrings exhibit that pattern even more strongly than actual words do!\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\nThis is all to say, be cautious about explanations for power-law distributions that are\n\n## Extra\n\nTo work out just how accurate the Good-Turing estimate is, I did the following experiment.\n\nStarting from the beginning of the book, I coded each word $w_i$ for whether or not it had already appeared in the book, 1 if yes, 0 if no. This is my best shot at writing that out in mathematical notation.\n\n$$\na_i = \\left\\{\\begin{array}{ll}1,& x_i\\in x_{1:i-1}\\\\\n                             0,& x_1 \\notin x_{1:i-1}\\end{array}\\right\\}\n$$\n\nThen for every position in the book, I made a table of counts of all the words up to that point in the book so far, and got the proportion of word tokens that had appeared only once. Again, here's my best stab at writing that out mathematically.\n\n$$\nc_{ji} = C(w_j), w_j \\in x_{i:i-1}\n$$\n\n$$\nr_i = \\sum_{j=1}\\left\\{\\begin{array}{ll}1,&c_{ji}=1\\\\0,& c_{ji} >1 \\end{array}\\right\\}\n$$\n\n$$\ng_i = \\frac{r_i}{i-1}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nfrank_words$first_appearance <- NA\nfrank_words$first_appearance[1] <- 1\n\nfrank_words$gt_est <- NA\nfrank_words$gt_est[1] <- 1\nfor(i in 2:nrow(frank_words)){\n  i_minus <- i-1\n  prev_corp <- frank_words$word[1:i_minus]\n  this_word <- frank_words$word[i]\n  \n  frank_words$first_appearance[i] <- ifelse(this_word %in% prev_corp, 0, 1)\n  frank_words$gt_est[i] <- sum(table(prev_corp) == 1)/i_minus\n}\n```\n:::\n\n::: {.cell}\n\n:::\n\nThen, I plotted the Good-Turing estimate for every position as well as a non-linear logistic regression smooth.\n\n::: {.cell}\n::: {.cell-output-display}\n![](data_sparsity_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "data_sparsity_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}