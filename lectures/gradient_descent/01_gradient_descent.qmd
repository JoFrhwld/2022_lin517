---
title: "Gradient Descent"
editor: visual
---

## Getting there by little steps

```{r}
#| echo: false
library(tidyverse)
library(ggforce)
library(ggrepel)
library(khroma)
library(scales)
library(showtext)


font_add_google("Atkinson Hyperlegible", "atkinson")
showtext_auto()

theme_set(theme_minimal() + theme(text = element_text(family = "atkinson", size = 16)))
library(reticulate)

```

What if wanted to convert inches to centimeters, but didn't know that the formula is inches \* 2.54? But what we *did* have was the following table of belt sizes from the Gap!

| Waist Size | Belt Length (in) | Belt Length (cm) |
|-----------:|-----------------:|-----------------:|
|         28 |             30.5 |               77 |
|         30 |             32.5 |               83 |
|         32 |             34.5 |               88 |
|         34 |             36.5 |               93 |
|         36 |             38.5 |               98 |
|         38 |             40.5 |              103 |
|         40 |             42.5 |              108 |
|         42 |             44.5 |              113 |
|         44 |             46.5 |              118 |
|         46 |             48.5 |              123 |

What we could do is *guess* the multiplier, and see how wrong it is.

```{python}
import numpy as np
```

```{python}
belt_in = np.array([30.5, 32.5, 34.5, 36.5, 38.5, 40.5, 42.5, 44.5, 46.5, 48.5])
belt_cm = np.array([77, 83, 88, 93, 98, 103, 108, 113, 118, 123])
```

```{python}
multiplier_guess = 1.5
cm_guess = belt_in * multiplier_guess
```

```{python}
# If our guess was right, this should all be 0
cm_guess - belt_cm
```

Our guess wasn't a great guess. With this multiplier, our guesses are all too small. Let's describe how bad our guess was with one number, and call it the "loss." The usual loss function for data like this is the Mean Squared Error.

```{python}
def mse(actual, guess):
  """
    Given the actual target outcomes and the outcomes we guessed,
    calculate the mean squared error.
  """
  error = actual-guess
  squared_error = np.power(error, 2)
  mean_squared_error = np.mean(squared_error)
  return(mean_squared_error)
```

```{python}
mse(belt_cm, cm_guess)
```

If we made our multiplier guess a little closer to what it ought to be, though, our mean squared error, or loss, should get smaller.

```{python}
multiplier_guess += 0.2
cm_guess = belt_in * multiplier_guess
mse(belt_cm, cm_guess)
```

One thing we could try doing is make a long list of possible multipliers, and try them all to see which one has the smallest loss. This is also known as a "grid search". I'll have to re-write the loss function to calculate the loss for specific multipliers

```{python}
# This gives us 50 evenly spaced numbers between 0 and 50
possible_mults = np.linspace(start = 0., stop = 5., num = 50)

def mse_loss(multiplier, inches, cm):
  """
    given a multiplier, and a set of traning data,
    (inches and their equivalent centimeters), return the 
    mean squared error obtained by using the given multiplier
  """
  cm_guess = inches * multiplier
  loss = mse(cm_guess, cm)
  return(loss)
```

```{python}
losses = np.array([mse_loss(m, belt_in, belt_cm) for m in possible_mults])
```

It's probably best to visualize the relationship between the multiplier and the loss in a graph.

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
tibble(mult = py$possible_mults,
       losses = py$losses) %>%
  ggplot(aes(mult, losses))+
    geom_point()+
    scale_y_continuous(labels = label_comma())
```

If we get the index of the smallest loss and get the associated multiplier, we can see that we're not too far off!

```{python}
possible_mults[losses.argmin()]
```

### Why not always just do grid search?

One thing that is going to remain the same no matter how complicated the models get is the measure of how well they've done, or the loss, is going to get boiled down to one number. But in real modelling situations, or neural networks, the number of *parameters* is going to get huge. Here we have only one parameter, but if we had even just 5 parameters, and tried doing a grid search over 50 evenly spaced values of each parameter, the number of possible combinations of parameter values will get intractable.

```{python}
f"{(5 ** 50):,}"
```

### Without seeing the whole map, we can tell which way is the right direction.

Let's look at the plot of our parameter vs the loss again:

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
tibble(mult = py$possible_mults,
       losses = py$losses) %>%
  ggplot(aes(mult, losses))+
    geom_vline(xintercept = 2.54, color = "firebrick")+  
    geom_point()+
    geom_line()+
    scale_y_continuous(labels = label_comma())

```

There are a few really important features of this loss function:

1.  As the estimate gets further away from the ideal value in either direction, the loss increases.
2.  The increase is "monotonic", meaning it's not bumpy or sometime going up, sometimes going down.
3.  The further away the guess gets from the optimal value, the *steeper* the "walls" of the curve get.

Let's say we were just these two point here, and we couldn't "see" the whole curve, but we knew features 1 through 3 were true. With that in hand, and information about how the loss function is calculated, we *can* get the *slope* of the function at each point (indicated by the arrows).

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
grad <- c(-6472.635, -1595.3387)
delta <- 0.5
x <- py$possible_mults[c(5,20)]
xstart <- x - (delta/2)
xend <- x + (delta/2)
y <- py$losses[c(5,20)]
ystart <- y - (grad*(delta/2))
yend <- y + (grad*(delta/2))

gradients <- tibble(x = xstart, y = ystart, xend=xend, yend = yend)

tibble(mult = py$possible_mults,
       losses = py$losses) %>%
  ggplot(aes(mult, losses))+
    geom_vline(xintercept = 2.54, color = "firebrick")+  
    geom_segment(data = gradients,
                 aes(x =x, y = y, xend = xend, yend = yend),
                 arrow = arrow(length = unit(0.5, "cm")),
                 color = "grey")+  
    geom_point(data = tibble(x = py$possible_mults[c(5, 20)], y = py$losses[c(5, 20)]),
               aes(x = x, y =y))+

    #geom_line()+
    scale_y_continuous(labels = label_comma(),
                       limits = range(py$losses))+
    xlim(0,5)
```

If we were able to to update our parameter in a way that is proportional to the slope of the loss, then we would gradually get closer and closer to the optimal value. The updates would be very large at first, while the parameter values are far away from the optimal value, and then would start updating by smaller and smaller amounts as we home in on the optimal value because the slopes get shallower and shallower the closer we get.

The slope of the loss function at any given point is the **gradient**, and this process of gradually descending downwards is called **gradient descent**.

## Gradient Descent

"But Joe!" you exclaim, "How *do* you calculate the slope of the loss for a single point without seeing the whole distribution?"

The answer to that question used to be "with calculus." But nowadays, people do it with "autograd" or "autodiff", which basically means "we let the computer figure it out." There isn't autograd functionality in numpy, but there is in a closely related library called [Jax, which is being developed by Google](https://jax.readthedocs.io/en/latest/#). Jax has a module called `numpy` which is designed to operate exactly the same way as `numpy`.

```{python}
import jax.numpy as jnp
from jax import grad
```

I'm going to rewrite the inches to centimeter functions over again, this time making sure to use jax functions to ensure everything runs smoothly.

```{python}
def inch_to_cm_jax(multiplier, inches):
  """
    a function that converts inches to cm
  """
  cm = jnp.dot(inches, multiplier)
  return(cm)

def cm_loss_jax(multiplier, inches, cm):
  """
    estimate the mismatch between the
  """
  est = inch_to_cm_jax(multiplier, inches)
  diff = est - cm
  sq_err = jnp.power(diff, 2)
  mean_sq_err = jnp.mean(sq_err)
  return(mean_sq_err)


```

Then we pass the new loss function to a jax function called `grad()` to create a new gradient function.

```{python}
cm_loss_grad_jax = grad(cm_loss_jax, argnums=0)
```

Where `cm_loss_jax()` will give use the mean-squared error for a specific multiplier, `cm_loss_grad_jax()` will give us the *slope* for that multiplier, automatically.

```{python}
print(multiplier_guess)
```

```{python}
# This is the mean-squared-error
print(cm_loss_jax(multiplier_guess, belt_in, belt_cm))
```

```{python}
# This is the slope
print(cm_loss_grad_jax(multiplier_guess, belt_in, belt_cm))
```

## Learning Rates and "Epochs"

Now we can write a for-loop to iteratively update out multiplier guess, changing it just a little bit proportional to the gradient. There are two "hyper parameters" we need to choose here.

1.  The "learning rate". We can't go adding the gradient *itself* to the multiplier. The gradient right now is in the thousands, and we're trying to nudge 1.7 to 2.54. So, we pick a "learning rate", which is just a very small decimal to multiply the gradient by before we add it to the parameter. I'll say let's start at 1/100,000
2.  The number of "epochs." We need to decide how many for loops we're going to go through before we decide to call it and check on how the learning has gone. I'll say let's go for 1000.

```{python}
learning_rate = 1/100_000
epochs = 1000
```

```{python}
# I want to be able to plot everything after, so I'm going to create collectors.
epoch_list    = []
param_list    = []
loss_list     = []
gradient_list = []
```

```{python}
multiplier_guess = 0.
for i in range(epochs):
  # append the current epoch
  epoch_list.append(i)
  # append the current guess
  param_list.append(multiplier_guess)
  
  loss = cm_loss_jax(multiplier_guess, belt_in, belt_cm)
  loss_list.append(loss)
  gradient = cm_loss_grad_jax(multiplier_guess, belt_in, belt_cm)
  gradient_list.append(gradient)
  
  multiplier_guess += -(gradient * learning_rate)

print(f"The final guess was {multiplier_guess:.3f}")
```

```{python}
#| echo: false
param_arr = np.array(param_list)
iter_arr = np.array(epoch_list)
loss_arr = np.array(loss_list)
gradient_arr = np.array(gradient_list)
```

```{r}
#| echo: false
train_df <- tibble(iter = py$iter_arr,
                   param = py$param_arr,
                   loss = py$loss_arr,
                   grad = py$gradient_arr) %>%
  pivot_longer(param:grad, names_to = "metric", values_to = "value")
```

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4
train_df %>%
  group_by(metric) %>%
  slice(1:100) %>%
  ggplot(aes(iter, value))+
    geom_point()+
    facet_wrap(~metric, scales = "free_y")
```

## This will all work with more parameters

```{python}
import pandas as pd
from palmerpenguins import load_penguins
```

```{python}
penguins = load_penguins()
```

```{python}
bill_length = np.array(penguins.dropna()["bill_length_mm"])
body_mass = np.array(penguins.dropna()["body_mass_g"])
```

```{python}
bill_length_z = (bill_length - bill_length.mean())/bill_length.std()
body_mass_z = (body_mass - body_mass.mean())/body_mass.std()
```

```{python}
bill_length_X = np.stack([np.ones(bill_length_z.size), bill_length_z], axis = 1)
bill_length_X[0:10, ]
```

```{python}
param_guess = np.array([0., 0.])
mass_guess = np.dot(bill_length_X, param_guess)
```

```{python}
np.mean(np.power(body_mass_z - mass_guess, 2))
```

```{python}
def fit_mass(params, X):
  """
    Given some values and parameters
    guess the outcome 
  """
  est = jnp.dot(X, params)
  return(est)

def fit_loss(params, X, actual):
  """
    Return the loss of the params
  """
  est = fit_mass(params, X)
  err = est - actual
  sq_err = jnp.power(err, 2)
  mse = jnp.mean(sq_err)
  return(mse)

fit_grad = grad(fit_loss, argnums=0)
```

```{python}
fit_loss(param_guess, bill_length_X, body_mass_z)
```

```{python}
fit_grad(param_guess, bill_length_X, body_mass_z)
```

```{python}
epoch_list    = []
param_list    = []
loss_list     = []
gradient_list = []

# I'm cheating starting out so close, just so this'll run quickly
param_guess = np.array([6., 4.])
learning_rate = 0.01
#vt = np.array([0., 0.])

for i in range(1000):
  # append the current epoch
  epoch_list.append(i)
  param_list.append(param_guess)
  loss = fit_loss(param_guess, bill_length_X, body_mass_z)
  loss_list.append(loss)
  gradient = fit_grad(param_guess,  bill_length_X, body_mass_z)
  gradient_list.append(gradient)
 
  param_guess += -(gradient * learning_rate)
 
print(f"Final param guess was {param_guess}")
```

```{python}
#| echo: false
param_arr = np.array(param_list)
gradient_arr = np.array(gradient_list)
loss_arr = np.array(loss_list)
```

```{r}
#| echo: false
data.frame(py$param_arr) %>%
  set_names(c("intercept", "slope")) %>%
  mutate(epoch = 1:n(),
         loss = py$loss_arr)->param_df
```

```{r}
#| echo: false
library(palmerpenguins)
library(gganimate)
penguins %>%
  drop_na()%>%
  mutate(bill_length_z = scale(bill_length_mm),
         bill_depth_z = scale(bill_depth_mm),
         body_mass_z = scale(body_mass_g))->penguins
```

```{r}
param_df %>%
  slice(1:500) %>%
  ggplot(aes(group = epoch)) +
    geom_point(data = penguins, 
               aes(x = bill_length_z, y = body_mass_z),
               group = NA)+
    geom_abline(aes(slope = slope, intercept = intercept, color = loss)) +
    theme(text = element_text(family = "serif", size = 16))+
    transition_states(states = epoch)

```

## Even more complex

```{python}
body_mass = jnp.array(penguins.dropna()["body_mass_g"])
bill_length = jnp.array(penguins.dropna()["bill_length_mm"])
bill_depth = jnp.array(penguins.dropna()["bill_depth_mm"])
flipper_length = jnp.array(penguins.dropna()["flipper_length_mm"])
```

```{python}
body_mass_z = (body_mass-body_mass.mean())/body_mass.std()
bill_length_z = (bill_length-bill_length.mean())/bill_length.std()
bill_depth_z = (bill_depth-bill_depth.mean())/bill_depth.std()
flipper_length_z = (flipper_length-flipper_length.mean())/flipper_length.std()
```

```{python}
data = jnp.vstack([bill_length_z, bill_depth_z, flipper_length_z])
```

```{python}
from jax import random
from jax.nn import relu

key = random.PRNGKey(42)
subkeys = random.split(key, 3)

layer_1 = random.uniform(key = subkeys[0], shape = (3,5))
layer_2 = random.uniform(key = subkeys[1], shape = (5,5))
layer_3 = random.uniform(key = subkeys[1], shape = (5,1))
```

```{python}
#jnp.dot(relu(jnp.dot(data.T, layer_1)), layer_2).squeeze()
```

```{python}
def estimate_mass(layers, data):
  l0_out = jnp.dot(data.T, layers[0])
  #l0_act = relu(l0_out)
  l1_out = jnp.dot(l0_out, layers[1])
  #l1_act = relu(l1_out)
  estimate = jnp.dot(l1_out, layers[2])
  #estimate = jnp.dot(jnp.dot(jnp.dot(data.T, layers[0]), layers[1]), layers[2])
  return(estimate)

def mass_loss(layers, data, body_mass_z):
  estimate = estimate_mass(layers, data)
  err = estimate.squeeze() - body_mass_z
  sq_err = jnp.power(err, 2)
  mean_sq_err = sq_err.mean()
  return(mean_sq_err)

mass_grad = grad(mass_loss, argnums = 0)
```

```{python}
est_list = []
loss_list = []

key = random.PRNGKey(42)
subkeys = random.split(key, 3)

layer_1 = random.uniform(key = subkeys[0], shape = (3,5))
layer_2 = random.uniform(key = subkeys[1], shape = (5,5))
layer_3 = random.uniform(key = subkeys[2], shape = (5,1))

learning_rate = 0.001
for i in range(1000):
  est = estimate_mass((layer_1, layer_2, layer_3), data)
  loss = mass_loss((layer_1, layer_2, layer_3), data, body_mass_z)
  est_list.append(est)
  loss_list.append(loss)
  
  gradient = mass_grad((layer_1, layer_2, layer_3), data, body_mass_z)
  layer_1 += -(gradient[0] * learning_rate)
  layer_2 += -(gradient[1] * learning_rate)
  layer_3 += -(gradient[2] * learning_rate)

```

```{python}
loss_arr = np.array(loss_list)
est_arr = np.array(est_list)
```

```{r}
data.frame(py$est_arr) %>%
  mutate(epoch = 1:n()) %>%
  pivot_longer(starts_with("X")) -> fits
```

```{r}
library(palmerpenguins)
library(gganimate)
penguins %>%
  drop_na() %>%
  mutate(body_mass_z = scale(body_mass_g)) %>%
  select(body_mass_z, species) %>%
  mutate(name = paste0("X", 1:333)) -> real_data
```

```{r}
fits %>%
  left_join(real_data) %>%
  filter(epoch %% 5 == 0) %>%
  ggplot(aes(body_mass_z, value))+
    geom_abline(intercept = 0, slope = 1, color = "grey")+
    geom_point(aes(color = species)) +
    theme(text = element_text(family = "serif", size = 16))+
    scale_color_bright()+
    transition_states(states = epoch)

```

## Categorical prediction

```{python}
from jax.nn import one_hot, softmax
```

```{python}
unique_species = set(penguins.dropna()["species"])
species_to_index = {s:idx for idx, s in enumerate(unique_species)}
species_index = np.array([species_to_index[s] for s in penguins.dropna()["species"]])
species_arr = jnp.array(species_index)
species_one_hot = one_hot(species_arr, 3)
```

```{python}
data = jnp.vstack([bill_length_z, bill_depth_z, flipper_length_z, body_mass_z])
data_idx = np.array(list(range(data.shape[1])))
n_train = int(np.floor(data.shape[1] * 0.8))
train_idx = np.random.choice(list(range(data.shape[1])), size = n_train)
test_idx = [x for x in list(range(data.shape[1])) if not x in train_idx]

data_train = data[:, train_idx]
data_test = data[:, test_idx]

label_train = species_one_hot[train_idx,]
label_test = species_one_hot[test_idx,]

species_index_train = species_index[train_idx]
species_index_test = species_index[test_idx]
```

```{python}
label_train.shape
```

```{python}
def estimate_species(layers, biases, data):
  l0_out = jnp.dot(data.T, layers[0]) + biases[0]
  l0_act = relu(l0_out)
  l1_out = jnp.dot(l0_out, layers[1]) + biases[1]
  l1_act = relu(l1_out)
  l2_out = jnp.dot(l1_out, layers[2]) + biases[2]
  estimate = softmax(l2_out, axis = -1)
  #estimate = jnp.dot(jnp.dot(jnp.dot(data.T, layers[0]), layers[1]), layers[2])
  return(estimate)

def estimate_loss(layers, biases, data, species_one_hot):
  est = estimate_species(layers, biases, data)
  loss = -1 * (jnp.log(est) * species_one_hot).sum()
  return(loss)

def accuracy(est, species_index):
  acc = sum(est.argmax(1) == species_index)/species_index.size
  return(acc)

estimate_grad = grad(estimate_loss, argnums = (0,1))
```

```{python}
key = random.PRNGKey(42)


layer_1 = random.uniform(key = subkeys[0], shape = (4,5))
biases_1 = random.uniform(key = subkeys[1], shape = (5,))
layer_2 = random.uniform(key = subkeys[2], shape = (5,5))
biases_2 = random.uniform(key = subkeys[3], shape = (5,))
layer_3 = random.uniform(key = subkeys[4], shape = (5,3))
biases_3 = random.uniform(key = subkeys[5], shape = (3,))

epochs = 900
learning_rate = 0.001
train_loss_list = []
test_loss_list = []

train_acc_list = []
test_acc_list = []


for i in range(epochs):
  est = estimate_species((layer_1, layer_2, layer_3),
                         (biases_1, biases_2, biases_3),
                          data_train)
  train_loss = estimate_loss((layer_1, layer_2, layer_3),
                       (biases_1, biases_2, biases_3),
                       data_train, label_train)
  train_loss_list.append(train_loss)
  train_acc = accuracy(est, species_index_train)
  train_acc_list.append(train_acc)
  
  test_est =  estimate_species((layer_1, layer_2, layer_3),
                         (biases_1, biases_2, biases_3),
                          data_test)
  test_loss = estimate_loss((layer_1, layer_2, layer_3),
                       (biases_1, biases_2, biases_3),
                       data_test, label_test)
  test_loss_list.append(test_loss)
  test_acc = accuracy(test_est, species_index_test)
  test_acc_list.append(test_acc)
  
  
  gradient = estimate_grad((layer_1, layer_2, layer_3),
                           (biases_1, biases_2, biases_3),
                            data_train, label_train)
  layer_1 += -(learning_rate * gradient[0][0])
  layer_2 += -(learning_rate * gradient[0][1])
  layer_3 += -(learning_rate * gradient[0][2])
  biases_1 += -(learning_rate * gradient[1][0])
  biases_2 += -(learning_rate * gradient[1][1])
  biases_3 += -(learning_rate * gradient[1][2])

```

```{python}
train_acc_arr = np.array(train_acc_list)
test_acc_arr = np.array(test_acc_list)
```

```{r}
plot(py$train_acc_arr)
```

```{python}
train_loss_arr = np.array(train_loss_list)
```

```{r}
py$train_loss_arr[900:1000] * 0.001
```
