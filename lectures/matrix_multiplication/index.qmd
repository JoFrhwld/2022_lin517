---
title: "Matrix Multiplication"
author: "Josef Fruehwald"
editor: visual
---

In the lecture notes on [gradient descent](../gradient_descent/01_gradient_descent.qmd), we already saw the use of matrix multiplication used to do linear regression, and I framed it as a kind of extension of dot products. Let's

```{python}
import numpy as np
```

```{python}
foot_in_tocm = np.array([[2.54 * 12], 
                         [2.54]])
foot_in_tocm                         
```

```{python}
heights_ft = np.array([[5,10],
                       [5, 11],
                       [5, 4]])
```

```{python}
heights_ft[1, ]
```

```{python}
np.dot(heights_ft[1, ], foot_in_tocm)
```

```{python}
np.dot(heights_ft, foot_in_tocm)
```

```{python}
heights_ft @ foot_in_tocm
```

```{python}
foot_in_foo = np.array([[30, 60],
                        [2.54, 5]])
```

```{python}
heights_ft @ foot_in_foo
```
